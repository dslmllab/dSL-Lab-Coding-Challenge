{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings and Text Vectorization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand vector space models** and their importance in NLP\n",
    "2. **Implement various text vectorization techniques** (BoW, TF-IDF, N-grams)\n",
    "3. **Work with pre-trained word embeddings** (Word2Vec, GloVe, FastText)\n",
    "4. **Train custom word embeddings** for domain-specific tasks\n",
    "5. **Explore contextual embeddings** and their advantages\n",
    "6. **Evaluate embedding quality** using various metrics\n",
    "7. **Apply embeddings to downstream tasks** like similarity and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text vectorization is the process of converting text into numerical representations that machine learning algorithms can work with. This is fundamental to NLP because computers can only process numbers, not raw text.\n",
    "\n",
    "### Why Vector Representations Matter:\n",
    "- **Mathematical Operations**: Enable arithmetic operations on text\n",
    "- **Similarity Computation**: Calculate semantic similarity between words/documents\n",
    "- **Machine Learning**: Serve as input features for ML models\n",
    "- **Semantic Understanding**: Capture meaning and relationships in text\n",
    "\n",
    "### Evolution of Text Representations:\n",
    "1. **Sparse Representations**: One-hot encoding, Bag of Words, TF-IDF\n",
    "2. **Dense Representations**: Word2Vec, GloVe, FastText\n",
    "3. **Contextual Representations**: BERT, GPT, RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:58:46.768624Z",
     "start_time": "2025-07-02T14:58:19.763193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.7.3)\n",
      "Requirement already satisfied: seaborn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.13.2)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-tzohdj2f/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=8.3.0,<8.4.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "       cwd: None\n",
      "  Complete output (268 lines):\n",
      "  Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "  Collecting setuptools\n",
      "    Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "  Collecting cython<3.0,>=0.25\n",
      "    Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "  Collecting cymem<2.1.0,>=2.0.2\n",
      "    Using cached cymem-2.0.11-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "  Collecting preshed<3.1.0,>=3.0.2\n",
      "    Using cached preshed-3.0.10.tar.gz (15 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "      Preparing wheel metadata: started\n",
      "      Preparing wheel metadata: finished with status 'done'\n",
      "  Collecting murmurhash<1.1.0,>=0.28.0\n",
      "    Using cached murmurhash-1.0.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "  Collecting thinc<8.4.0,>=8.3.0\n",
      "    Using cached thinc-8.3.2.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-m2irzwg9/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (73 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.10.tar.gz (15 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-j5938dz4/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-j5938dz4/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-z2_n_55h/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-z2_n_55h/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-td68fkrm/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-td68fkrm/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0, 1.2.1, 1.3.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/8a/9f/b2193b69dd112a46800182e897cda2fc9c497dfdd9352a0c5ba9252cf5f0/thinc-8.3.2.tar.gz#sha256=3e8ef69eac89a601e11d47fc9e43d26ffe7ef682dcf667c94ff35ff690549aeb (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-m2irzwg9/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "    Using cached thinc-8.3.1.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-se3e8i8_/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (73 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.10.tar.gz (15 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-u4m1llcz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-u4m1llcz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-zv8l6tfy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-zv8l6tfy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-mzf6bijz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-mzf6bijz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0, 1.2.1, 1.3.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/ea/60/b7d645d621a47d649975b53c13cdf3e66b456a24727ccd34794f1014f45c/thinc-8.3.1.tar.gz#sha256=44e747bbf93e981dfded7d432b68ef1ba75b28ef46fc51f185477970743b36e9 (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-se3e8i8_/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "    Using cached thinc-8.3.0.tar.gz (193 kB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'error'\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-isg3m_re/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"''\n",
      "         cwd: None\n",
      "    Complete output (73 lines):\n",
      "    Ignoring numpy: markers 'python_version >= \"3.9\"' don't match your environment\n",
      "    Collecting setuptools\n",
      "      Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "    Collecting cython<3.0,>=0.25\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "    Collecting murmurhash<1.1.0,>=1.0.2\n",
      "      Using cached murmurhash-1.0.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting cymem<2.1.0,>=2.0.2\n",
      "      Using cached cymem-2.0.11-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "    Collecting preshed<3.1.0,>=3.0.2\n",
      "      Using cached preshed-3.0.10.tar.gz (15 kB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'done'\n",
      "      Getting requirements to build wheel: started\n",
      "      Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing wheel metadata: started\n",
      "        Preparing wheel metadata: finished with status 'done'\n",
      "    Collecting blis<1.1.0,>=1.0.0\n",
      "      Using cached blis-1.0.2.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-w4edot0_/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/8a/69/8686634ff12188a7ac5dad16472cc66f166f6d4c34babfe8bcb4ef44ab7c/blis-1.0.2.tar.gz#sha256=68df878871a9db34efd58f648e12e06806e381991bd9e70df198c33d7b259383 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-w4edot0_/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-dendmqwy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-dendmqwy/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "      Using cached blis-1.0.0.tar.gz (3.6 MB)\n",
      "      Installing build dependencies: started\n",
      "      Installing build dependencies: finished with status 'error'\n",
      "      ERROR: Command errored out with exit status 1:\n",
      "       command: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-q8gb0qdz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0'\n",
      "           cwd: None\n",
      "      Complete output (8 lines):\n",
      "      Collecting setuptools\n",
      "        Using cached setuptools-75.3.2-py3-none-any.whl (1.3 MB)\n",
      "      Collecting cython>=0.25\n",
      "        Using cached cython-3.1.2-cp38-cp38-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "      ERROR: Could not find a version that satisfies the requirement numpy<3.0.0,>=2.0.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.21.2, 1.21.3, 1.21.4, 1.21.5, 1.21.6, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4)\n",
      "      ERROR: No matching distribution found for numpy<3.0.0,>=2.0.0\n",
      "      WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "      You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "      ----------------------------------------\n",
      "    WARNING: Discarding https://files.pythonhosted.org/packages/b6/0b/a73be025d991e8795626a830314984f7bda5de440e80b72a53bc316bb870/blis-1.0.0.tar.gz#sha256=9ea14649ff07457e4112c7b94605e4aeb4f2fd5a8bd57c296ff8fbd154966ede (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-q8gb0qdz/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25' 'numpy>=2.0.0,<3.0.0' Check the logs for full command output.\n",
      "    ERROR: Could not find a version that satisfies the requirement blis<1.1.0,>=1.0.0 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.8, 0.0.9.dev104, 0.0.10, 0.0.12, 0.0.13, 0.0.16, 0.1.0, 0.2.0.dev0, 0.2.0, 0.2.1, 0.2.2.dev0, 0.2.2, 0.2.3.dev0, 0.2.3.dev1, 0.2.3.dev2, 0.2.3.dev3, 0.2.3, 0.2.4, 0.3.1, 0.4.0.dev0, 0.4.0.dev1, 0.4.0, 0.4.1, 0.7.0, 0.7.1.dev0, 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.7.5, 0.7.6, 0.7.7, 0.7.8, 0.7.9, 0.7.10, 0.7.11, 0.9.0, 0.9.1.dev0, 0.9.1.dev1, 0.9.1, 1.0.0a1, 1.0.0, 1.0.1, 1.0.2, 1.2.0, 1.2.1, 1.3.0)\n",
      "    ERROR: No matching distribution found for blis<1.1.0,>=1.0.0\n",
      "    WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "    You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "    ----------------------------------------\n",
      "  WARNING: Discarding https://files.pythonhosted.org/packages/15/0a/250f7fa34632616bb4fc37decbc46ed09243bade708968dc869d8a4c144b/thinc-8.3.0.tar.gz#sha256=eb3bed54f5c00ec9addaaa208c51ccfa059483d73140cd515aa33373715c6e59 (from https://pypi.org/simple/thinc/) (requires-python:>=3.6). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-isg3m_re/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'murmurhash>=1.0.2,<1.1.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'blis>=1.0.0,<1.1.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\n",
      "  ERROR: Could not find a version that satisfies the requirement thinc<8.4.0,>=8.3.0 (from versions: 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.41, 1.42, 1.60, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.70, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 2.0, 3.0, 3.1, 3.2, 3.3, 3.4.1, 4.0.0, 4.1.0, 4.2.0, 5.0.0, 5.0.1, 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7, 5.0.8, 6.0.0, 6.1.0, 6.1.1, 6.1.2, 6.1.3, 6.2.0, 6.3.0, 6.4.0, 6.5.0, 6.5.2, 6.6.0, 6.7.0, 6.7.1, 6.7.2, 6.7.3, 6.8.0, 6.8.1, 6.8.2, 6.9.0, 6.10.0, 6.10.1.dev0, 6.10.1, 6.10.2.dev0, 6.10.2.dev1, 6.10.2, 6.10.3.dev0, 6.10.3.dev1, 6.10.3, 6.10.4.dev0, 6.11.0.dev2, 6.11.1.dev0, 6.11.1.dev1, 6.11.1.dev2, 6.11.1.dev3, 6.11.1.dev4, 6.11.1.dev6, 6.11.1.dev7, 6.11.1.dev10, 6.11.1.dev11, 6.11.1.dev12, 6.11.1.dev13, 6.11.1.dev15, 6.11.1.dev16, 6.11.1.dev17, 6.11.1.dev18, 6.11.1.dev19, 6.11.1.dev20, 6.11.1, 6.11.2.dev0, 6.11.2, 6.11.3.dev1, 6.11.3.dev2, 6.12.0, 6.12.1, 7.0.0.dev0, 7.0.0.dev1, 7.0.0.dev2, 7.0.0.dev3, 7.0.0.dev4, 7.0.0.dev5, 7.0.0.dev6, 7.0.0.dev8, 7.0.0, 7.0.1.dev0, 7.0.1.dev1, 7.0.1.dev2, 7.0.1, 7.0.2, 7.0.3, 7.0.4.dev0, 7.0.4, 7.0.5.dev0, 7.0.5, 7.0.6, 7.0.7, 7.0.8, 7.1.0.dev0, 7.1.0, 7.1.1, 7.2.0.dev3, 7.2.0, 7.3.0.dev0, 7.3.0, 7.3.1, 7.4.0.dev0, 7.4.0.dev1, 7.4.0.dev2, 7.4.0, 7.4.1, 7.4.2, 7.4.3, 7.4.4, 7.4.5, 7.4.6, 8.0.0.dev0, 8.0.0.dev2, 8.0.0.dev4, 8.0.0a0, 8.0.0a1, 8.0.0a2, 8.0.0a3, 8.0.0a6, 8.0.0a8, 8.0.0a9, 8.0.0a11, 8.0.0a12, 8.0.0a13, 8.0.0a14, 8.0.0a16, 8.0.0a17, 8.0.0a18, 8.0.0a19, 8.0.0a20, 8.0.0a21, 8.0.0a22, 8.0.0a23, 8.0.0a24, 8.0.0a25, 8.0.0a26, 8.0.0a27, 8.0.0a28, 8.0.0a29, 8.0.0a30, 8.0.0a31, 8.0.0a32, 8.0.0a33, 8.0.0a34, 8.0.0a35, 8.0.0a36, 8.0.0a40, 8.0.0a41, 8.0.0a42, 8.0.0a43, 8.0.0a44, 8.0.0rc0, 8.0.0rc1, 8.0.0rc2, 8.0.0rc3, 8.0.0rc4, 8.0.0rc5, 8.0.0rc6.dev0, 8.0.0rc6, 8.0.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.0.5, 8.0.6, 8.0.7, 8.0.8, 8.0.9, 8.0.10, 8.0.11, 8.0.12, 8.0.13, 8.0.14.dev0, 8.0.14, 8.0.15, 8.0.16, 8.0.17, 8.1.0.dev0, 8.1.0.dev1, 8.1.0.dev2, 8.1.0.dev3, 8.1.0, 8.1.1, 8.1.2, 8.1.3, 8.1.4, 8.1.5, 8.1.6, 8.1.7, 8.1.8, 8.1.9, 8.1.10, 8.1.11, 8.1.12, 8.2.0, 8.2.1, 8.2.2, 8.2.3, 8.2.4, 8.2.5, 8.3.0, 8.3.1, 8.3.2, 9.0.0.dev0, 9.0.0.dev1, 9.0.0.dev2, 9.0.0.dev3, 9.0.0.dev4, 9.0.0.dev5)\n",
      "  ERROR: No matching distribution found for thinc<8.4.0,>=8.3.0\n",
      "  WARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "  You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/07/53/536941af8fbb5cb10a778f0dbd2a17b0f13e7ebfc11e67b154be60508fdf/spacy-3.8.2.tar.gz#sha256=4b37ebd25ada4059b0dc9e0893e70dde5df83485329a068ef04580e70892a65d (from https://pypi.org/simple/spacy/) (requires-python:>=3.7). Command errored out with exit status 1: /Library/Frameworks/Python.framework/Versions/3.8/bin/python3 /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-standalone-pip-x698gb6k/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/7x/4b9tm2d111j5qqd94k87nqxw0000gn/T/pip-build-env-tzohdj2f/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools 'cython>=0.25,<3.0' 'cymem>=2.0.2,<2.1.0' 'preshed>=3.0.2,<3.1.0' 'murmurhash>=0.28.0,<1.1.0' 'thinc>=8.3.0,<8.4.0' 'numpy>=2.0.0,<2.1.0; python_version < '\"'\"'3.9'\"'\"'' 'numpy>=2.0.0,<2.1.0; python_version >= '\"'\"'3.9'\"'\"'' Check the logs for full command output.\u001b[0m\n",
      "\u001b[?25h  Using cached spacy-3.7.5-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.3.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp38-cp38-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1 MB 7.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.67.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.13-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (2.32.3)\n",
      "Collecting thinc<8.3.0,>=8.2.2\n",
      "  Using cached thinc-8.2.5.tar.gz (193 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (3.1.5)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.8-cp38-cp38-macosx_10_9_x86_64.whl (490 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.10-cp38-cp38-macosx_10_9_x86_64.whl\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from spacy) (58.0.4)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-7.3.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp38-cp38-macosx_10_9_x86_64.whl (192 kB)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp38-cp38-macosx_10_9_x86_64.whl (38 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.11-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Building wheels for collected packages: thinc\n",
      "  Building wheel for thinc (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thinc: filename=thinc-8.2.5-cp38-cp38-macosx_10_9_x86_64.whl size=861753 sha256=845c0ab777a4b23092827260366e7e02aea8305a6d14f8802b8cc293de39f191\n",
      "  Stored in directory: /Users/viraajimothukuri/Library/Caches/pip/wheels/ca/65/2f/a856a5c5a2338dcc54a8e8fbbd904aef94510ff7575effd769\n",
      "Successfully built thinc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: catalogue, wrapt, srsly, shellingham, murmurhash, marisa-trie, cymem, wasabi, typer, smart-open, preshed, language-data, confection, cloudpathlib, blis, weasel, thinc, spacy-loggers, spacy-legacy, langcodes, spacy, nltk, gensim\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 gensim-4.3.3 langcodes-3.4.1 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.13 nltk-3.9.1 preshed-3.0.10 shellingham-1.5.4 smart-open-7.3.0 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.16.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip3 install numpy pandas matplotlib seaborn nltk spacy scikit-learn gensim tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T14:58:46.808552Z",
     "start_time": "2025-07-02T14:58:46.785075Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for item in ['punkt', 'stopwords']:\n",
    "    nltk.download(item, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Scikit-learn for traditional methods\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Word embeddings\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk_downloads = ['punkt', 'stopwords']\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.download(item, quiet=True)\n",
    "    except:\n",
    "        print(f\"Could not download {item}\")\n",
    "\n",
    "print(\"Setup completed successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Gensim version: {gensim.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample corpus for embeddings training\n",
    "sample_corpus = [\n",
    "    \"The cat sat on the mat and looked around.\",\n",
    "    \"A dog ran quickly through the park.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Birds fly high in the blue sky.\",\n",
    "    \"Fish swim deep in the ocean water.\",\n",
    "    \"The sun shines brightly during the day.\",\n",
    "    \"Stars twinkle in the dark night sky.\",\n",
    "    \"Flowers bloom beautifully in spring.\",\n",
    "    \"Trees grow tall in the forest.\",\n",
    "    \"Rivers flow towards the sea.\",\n",
    "    \"Mountains stand majestically against the horizon.\",\n",
    "    \"Children play happily in the playground.\",\n",
    "    \"Students study diligently in the library.\",\n",
    "    \"Teachers explain concepts clearly to students.\",\n",
    "    \"Books contain knowledge and wisdom.\",\n",
    "    \"Computers process information very quickly.\",\n",
    "    \"Internet connects people around the world.\",\n",
    "    \"Mobile phones are essential communication tools.\",\n",
    "    \"Cars drive on roads and highways.\",\n",
    "    \"Planes fly through clouds in the sky.\",\n",
    "    \"Music brings joy to many people.\",\n",
    "    \"Art expresses creativity and emotions.\",\n",
    "    \"Sports promote health and fitness.\",\n",
    "    \"Food provides energy and nutrition.\",\n",
    "    \"Water is essential for all life.\"\n",
    "]\n",
    "\n",
    "# Create different categories for classification tasks\n",
    "categorized_corpus = {\n",
    "    'animals': [\n",
    "        \"The cat sat on the mat peacefully.\",\n",
    "        \"A dog ran quickly through the park.\",\n",
    "        \"Birds fly high in the blue sky.\",\n",
    "        \"Fish swim deep in the ocean water.\",\n",
    "        \"Elephants are large gray mammals.\",\n",
    "        \"Lions are fierce predators in Africa.\",\n",
    "        \"Dolphins are intelligent marine animals.\",\n",
    "        \"Bears hibernate during winter months.\"\n",
    "    ],\n",
    "    'technology': [\n",
    "        \"Computers process information very quickly.\",\n",
    "        \"Internet connects people around the world.\",\n",
    "        \"Mobile phones are essential communication tools.\",\n",
    "        \"Artificial intelligence is advancing rapidly.\",\n",
    "        \"Robots can perform complex tasks.\",\n",
    "        \"Software applications help users daily.\",\n",
    "        \"Digital cameras capture high-quality images.\",\n",
    "        \"Virtual reality creates immersive experiences.\"\n",
    "    ],\n",
    "    'nature': [\n",
    "        \"The sun shines brightly during the day.\",\n",
    "        \"Stars twinkle in the dark night sky.\",\n",
    "        \"Flowers bloom beautifully in spring.\",\n",
    "        \"Trees grow tall in the forest.\",\n",
    "        \"Rivers flow towards the sea.\",\n",
    "        \"Mountains stand majestically against the horizon.\",\n",
    "        \"Rain falls gently on the earth.\",\n",
    "        \"Wind blows softly through the leaves.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten categorized corpus\n",
    "all_categorized_texts = []\n",
    "all_labels = []\n",
    "for category, texts in categorized_corpus.items():\n",
    "    all_categorized_texts.extend(texts)\n",
    "    all_labels.extend([category] * len(texts))\n",
    "\n",
    "print(f\"Sample corpus: {len(sample_corpus)} documents\")\n",
    "print(f\"Categorized corpus: {len(all_categorized_texts)} documents in {len(categorized_corpus)} categories\")\n",
    "print(f\"Categories: {list(categorized_corpus.keys())}\")\n",
    "\n",
    "# Display sample texts\n",
    "print(\"\\nSample texts:\")\n",
    "for i, text in enumerate(sample_corpus[:5]):\n",
    "    print(f\"{i+1}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Vectorization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWordsVectorizer:\n",
    "    \"\"\"Custom Bag of Words implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=None, lowercase=True, stop_words=None):\n",
    "        self.max_features = max_features\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = set(stop_words) if stop_words else set()\n",
    "        self.vocabulary_ = {}\n",
    "        self.feature_names_ = []\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"Basic text preprocessing\"\"\"\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Simple tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words\n",
    "        if self.stop_words:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Remove punctuation\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary from documents\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = self._preprocess_text(doc)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Select top features if max_features is specified\n",
    "        if self.max_features:\n",
    "            most_common = word_counts.most_common(self.max_features)\n",
    "            vocabulary = [word for word, count in most_common]\n",
    "        else:\n",
    "            vocabulary = list(word_counts.keys())\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        self.vocabulary_ = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        self.feature_names_ = vocabulary\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Transform documents to BoW vectors\"\"\"\n",
    "        vectors = np.zeros((len(documents), len(self.vocabulary_)))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            tokens = self._preprocess_text(doc)\n",
    "            token_counts = Counter(tokens)\n",
    "            \n",
    "            for token, count in token_counts.items():\n",
    "                if token in self.vocabulary_:\n",
    "                    word_idx = self.vocabulary_[token]\n",
    "                    vectors[doc_idx, word_idx] = count\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(documents).transform(documents)\n",
    "\n",
    "# Test custom BoW\n",
    "print(\"Custom Bag of Words Vectorizer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "custom_bow = BagOfWordsVectorizer(max_features=20, stop_words=stopwords.words('english'))\n",
    "bow_vectors = custom_bow.fit_transform(sample_corpus[:5])\n",
    "\n",
    "print(f\"Vocabulary size: {len(custom_bow.vocabulary_)}\")\n",
    "print(f\"Vector shape: {bow_vectors.shape}\")\n",
    "print(f\"Feature names: {custom_bow.feature_names_[:10]}...\")\n",
    "\n",
    "# Display first document vector\n",
    "print(f\"\\nFirst document: '{sample_corpus[0]}'\")\n",
    "print(f\"Vector: {bow_vectors[0][:10]}...\")\n",
    "\n",
    "# Compare with sklearn's CountVectorizer\n",
    "print(\"\\nCompare with sklearn's CountVectorizer:\")\n",
    "sklearn_bow = CountVectorizer(max_features=20, stop_words='english', lowercase=True)\n",
    "sklearn_vectors = sklearn_bow.fit_transform(sample_corpus[:5]).toarray()\n",
    "\n",
    "print(f\"Sklearn vocabulary size: {len(sklearn_bow.vocabulary_)}\")\n",
    "print(f\"Sklearn vector shape: {sklearn_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFVectorizer:\n",
    "    \"\"\"Custom TF-IDF implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_features=None, lowercase=True, stop_words=None):\n",
    "        self.max_features = max_features\n",
    "        self.lowercase = lowercase\n",
    "        self.stop_words = set(stop_words) if stop_words else set()\n",
    "        self.vocabulary_ = {}\n",
    "        self.idf_ = {}\n",
    "        self.feature_names_ = []\n",
    "    \n",
    "    def _preprocess_text(self, text):\n",
    "        \"\"\"Basic text preprocessing\"\"\"\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        if self.stop_words:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Build vocabulary and calculate IDF values\"\"\"\n",
    "        # Build vocabulary\n",
    "        word_counts = Counter()\n",
    "        doc_word_sets = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = self._preprocess_text(doc)\n",
    "            doc_word_set = set(tokens)\n",
    "            doc_word_sets.append(doc_word_set)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Select vocabulary\n",
    "        if self.max_features:\n",
    "            most_common = word_counts.most_common(self.max_features)\n",
    "            vocabulary = [word for word, count in most_common]\n",
    "        else:\n",
    "            vocabulary = list(word_counts.keys())\n",
    "        \n",
    "        self.vocabulary_ = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "        self.feature_names_ = vocabulary\n",
    "        \n",
    "        # Calculate IDF values\n",
    "        num_docs = len(documents)\n",
    "        for word in vocabulary:\n",
    "            # Count documents containing the word\n",
    "            doc_freq = sum(1 for doc_words in doc_word_sets if word in doc_words)\n",
    "            # Calculate IDF\n",
    "            self.idf_[word] = np.log(num_docs / (doc_freq + 1))  # +1 for smoothing\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        \"\"\"Transform documents to TF-IDF vectors\"\"\"\n",
    "        vectors = np.zeros((len(documents), len(self.vocabulary_)))\n",
    "        \n",
    "        for doc_idx, doc in enumerate(documents):\n",
    "            tokens = self._preprocess_text(doc)\n",
    "            token_counts = Counter(tokens)\n",
    "            total_tokens = len(tokens)\n",
    "            \n",
    "            for token, count in token_counts.items():\n",
    "                if token in self.vocabulary_:\n",
    "                    word_idx = self.vocabulary_[token]\n",
    "                    tf = count / total_tokens  # Term frequency\n",
    "                    idf = self.idf_[token]     # Inverse document frequency\n",
    "                    vectors[doc_idx, word_idx] = tf * idf\n",
    "        \n",
    "        return vectors\n",
    "    \n",
    "    def fit_transform(self, documents):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(documents).transform(documents)\n",
    "\n",
    "# Test custom TF-IDF\n",
    "print(\"Custom TF-IDF Vectorizer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "custom_tfidf = TFIDFVectorizer(max_features=20, stop_words=stopwords.words('english'))\n",
    "tfidf_vectors = custom_tfidf.fit_transform(sample_corpus[:8])\n",
    "\n",
    "print(f\"Vocabulary size: {len(custom_tfidf.vocabulary_)}\")\n",
    "print(f\"Vector shape: {tfidf_vectors.shape}\")\n",
    "print(f\"Feature names: {custom_tfidf.feature_names_[:10]}\")\n",
    "\n",
    "# Show IDF values\n",
    "print(\"\\nTop IDF values:\")\n",
    "sorted_idf = sorted(custom_tfidf.idf_.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for word, idf_val in sorted_idf:\n",
    "    print(f\"{word}: {idf_val:.3f}\")\n",
    "\n",
    "# Compare with sklearn's TfidfVectorizer\n",
    "sklearn_tfidf = TfidfVectorizer(max_features=20, stop_words='english')\n",
    "sklearn_tfidf_vectors = sklearn_tfidf.fit_transform(sample_corpus[:8]).toarray()\n",
    "\n",
    "print(f\"\\nSklearn TF-IDF shape: {sklearn_tfidf_vectors.shape}\")\n",
    "\n",
    "# Visualize TF-IDF vectors\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(tfidf_vectors[:5], cmap='viridis', aspect='auto')\n",
    "plt.title('Custom TF-IDF Vectors (First 5 docs)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Documents')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sklearn_tfidf_vectors[:5], cmap='viridis', aspect='auto')\n",
    "plt.title('Sklearn TF-IDF Vectors (First 5 docs)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Documents')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 N-gram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n):\n",
    "    \"\"\"Create n-grams from text\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "    \n",
    "    ngrams = []\n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram = ' '.join(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "def analyze_ngrams(corpus, max_n=3):\n",
    "    \"\"\"Analyze n-grams in corpus\"\"\"\n",
    "    ngram_analysis = {}\n",
    "    \n",
    "    for n in range(1, max_n + 1):\n",
    "        all_ngrams = []\n",
    "        for text in corpus:\n",
    "            ngrams = create_ngrams(text, n)\n",
    "            all_ngrams.extend(ngrams)\n",
    "        \n",
    "        ngram_counts = Counter(all_ngrams)\n",
    "        ngram_analysis[n] = ngram_counts\n",
    "    \n",
    "    return ngram_analysis\n",
    "\n",
    "# Analyze n-grams in sample corpus\n",
    "print(\"N-gram Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "ngram_analysis = analyze_ngrams(sample_corpus[:10])\n",
    "\n",
    "for n, ngram_counts in ngram_analysis.items():\n",
    "    print(f\"\\n{n}-grams (Top 10):\")\n",
    "    for ngram, count in ngram_counts.most_common(10):\n",
    "        print(f\"  '{ngram}': {count}\")\n",
    "\n",
    "# Use sklearn for n-gram vectorization\n",
    "print(\"\\nN-gram Vectorization with sklearn:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Different n-gram configurations\n",
    "configs = {\n",
    "    'unigrams': (1, 1),\n",
    "    'bigrams': (2, 2),\n",
    "    'trigrams': (3, 3),\n",
    "    'uni+bigrams': (1, 2),\n",
    "    'uni+bi+trigrams': (1, 3)\n",
    "}\n",
    "\n",
    "vectorizer_results = {}\n",
    "\n",
    "for name, ngram_range in configs.items():\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=50,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    vectors = vectorizer.fit_transform(sample_corpus[:10])\n",
    "    vectorizer_results[name] = {\n",
    "        'vectorizer': vectorizer,\n",
    "        'vectors': vectors,\n",
    "        'feature_names': vectorizer.get_feature_names_out()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:15}: {vectors.shape[1]} features\")\n",
    "\n",
    "# Show sample features for different n-gram ranges\n",
    "print(\"\\nSample features:\")\n",
    "for name, results in vectorizer_results.items():\n",
    "    features = results['feature_names'][:10]\n",
    "    print(f\"{name:15}: {list(features)}\")\n",
    "\n",
    "# Visualize n-gram frequency\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for i, (n, ngram_counts) in enumerate(ngram_analysis.items(), 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    \n",
    "    top_ngrams = ngram_counts.most_common(10)\n",
    "    ngrams, counts = zip(*top_ngrams)\n",
    "    \n",
    "    plt.barh(range(len(ngrams)), counts)\n",
    "    plt.yticks(range(len(ngrams)), [ngram[:20] + '...' if len(ngram) > 20 else ngram for ngram in ngrams])\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.title(f'Top {n}-grams')\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare corpus for Word2Vec training\n",
    "def preprocess_for_word2vec(corpus):\n",
    "    \"\"\"Preprocess corpus for Word2Vec training\"\"\"\n",
    "    processed_corpus = []\n",
    "    \n",
    "    for text in corpus:\n",
    "        # Tokenize and clean\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token for token in tokens if token.isalpha() and len(token) > 1]\n",
    "        processed_corpus.append(tokens)\n",
    "    \n",
    "    return processed_corpus\n",
    "\n",
    "# Prepare training data\n",
    "training_data = preprocess_for_word2vec(sample_corpus)\n",
    "\n",
    "print(\"Word2Vec Training:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Training corpus: {len(training_data)} sentences\")\n",
    "print(f\"Sample sentence: {training_data[0]}\")\n",
    "\n",
    "# Train Word2Vec models with different architectures\n",
    "word2vec_models = {}\n",
    "\n",
    "# Skip-gram model\n",
    "sg_model = Word2Vec(\n",
    "    sentences=training_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "word2vec_models['skip-gram'] = sg_model\n",
    "\n",
    "# CBOW model\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=training_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0,  # CBOW\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "word2vec_models['cbow'] = cbow_model\n",
    "\n",
    "print(f\"\\nTrained models: {list(word2vec_models.keys())}\")\n",
    "\n",
    "# Analyze vocabulary\n",
    "for name, model in word2vec_models.items():\n",
    "    vocab_size = len(model.wv.key_to_index)\n",
    "    print(f\"{name:10}: {vocab_size} words in vocabulary\")\n",
    "\n",
    "# Test word similarities\n",
    "print(\"\\nWord Similarities (Skip-gram model):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_words = ['cat', 'dog', 'sun', 'sky', 'water']\n",
    "sg_model = word2vec_models['skip-gram']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in sg_model.wv:\n",
    "        try:\n",
    "            similar_words = sg_model.wv.most_similar(word, topn=3)\n",
    "            print(f\"{word:8}: {similar_words}\")\n",
    "        except:\n",
    "            print(f\"{word:8}: No similar words found\")\n",
    "    else:\n",
    "        print(f\"{word:8}: Not in vocabulary\")\n",
    "\n",
    "# Word arithmetic examples\n",
    "print(\"\\nWord Arithmetic Examples:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    # Example: sky - day + night ≈ stars\n",
    "    result = sg_model.wv.most_similar(\n",
    "        positive=['sky', 'night'],\n",
    "        negative=['day'],\n",
    "        topn=3\n",
    "    )\n",
    "    print(f\"sky - day + night = {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Word arithmetic failed: {e}\")\n",
    "\n",
    "# Get word vectors\n",
    "print(\"\\nWord Vectors:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "for word in ['cat', 'dog', 'sun']:\n",
    "    if word in sg_model.wv:\n",
    "        vector = sg_model.wv[word]\n",
    "        print(f\"{word}: shape {vector.shape}, sample: {vector[:5]}\")\n",
    "\n",
    "# Compare Skip-gram vs CBOW\n",
    "print(\"\\nSkip-gram vs CBOW Comparison:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "test_word = 'cat'\n",
    "if test_word in sg_model.wv and test_word in cbow_model.wv:\n",
    "    sg_vector = sg_model.wv[test_word]\n",
    "    cbow_vector = cbow_model.wv[test_word]\n",
    "    \n",
    "    # Calculate cosine similarity between the vectors\n",
    "    similarity = cosine_similarity([sg_vector], [cbow_vector])[0][0]\n",
    "    print(f\"Vector similarity for '{test_word}': {similarity:.4f}\")\n",
    "    \n",
    "    # Compare similar words\n",
    "    print(f\"\\nSimilar words to '{test_word}':\")\n",
    "    try:\n",
    "        sg_similar = sg_model.wv.most_similar(test_word, topn=3)\n",
    "        cbow_similar = cbow_model.wv.most_similar(test_word, topn=3)\n",
    "        print(f\"Skip-gram: {sg_similar}\")\n",
    "        print(f\"CBOW:      {cbow_similar}\")\n",
    "    except:\n",
    "        print(\"Could not compute similar words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText model\n",
    "print(\"FastText Training:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "fasttext_model = FastText(\n",
    "    sentences=training_data,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    min_n=3,  # Minimum character n-gram\n",
    "    max_n=6,  # Maximum character n-gram\n",
    "    sg=1,     # Skip-gram\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"FastText vocabulary size: {len(fasttext_model.wv.key_to_index)}\")\n",
    "\n",
    "# Test FastText's ability to handle out-of-vocabulary words\n",
    "print(\"\\nOut-of-Vocabulary Word Handling:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test words (some may not be in training vocabulary)\n",
    "test_oov_words = ['cats', 'dogs', 'flying', 'swimming', 'unknown_word']\n",
    "\n",
    "for word in test_oov_words:\n",
    "    # Word2Vec (Skip-gram)\n",
    "    if word in sg_model.wv:\n",
    "        w2v_status = \"In vocabulary\"\n",
    "        w2v_vector = sg_model.wv[word]\n",
    "    else:\n",
    "        w2v_status = \"OOV (Out of Vocabulary)\"\n",
    "        w2v_vector = None\n",
    "    \n",
    "    # FastText (can handle OOV through subword information)\n",
    "    try:\n",
    "        ft_vector = fasttext_model.wv[word]\n",
    "        ft_status = \"Vector generated\"\n",
    "    except:\n",
    "        ft_status = \"Failed\"\n",
    "        ft_vector = None\n",
    "    \n",
    "    print(f\"{word:15} - Word2Vec: {w2v_status:20} FastText: {ft_status}\")\n",
    "\n",
    "# Compare similar words\n",
    "print(\"\\nSimilar Words Comparison (Word2Vec vs FastText):\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "comparison_words = ['cat', 'sun', 'water']\n",
    "\n",
    "for word in comparison_words:\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    \n",
    "    # Word2Vec similar words\n",
    "    if word in sg_model.wv:\n",
    "        try:\n",
    "            w2v_similar = sg_model.wv.most_similar(word, topn=3)\n",
    "            print(f\"  Word2Vec:  {w2v_similar}\")\n",
    "        except:\n",
    "            print(f\"  Word2Vec:  No similar words\")\n",
    "    else:\n",
    "        print(f\"  Word2Vec:  Word not in vocabulary\")\n",
    "    \n",
    "    # FastText similar words\n",
    "    try:\n",
    "        ft_similar = fasttext_model.wv.most_similar(word, topn=3)\n",
    "        print(f\"  FastText:  {ft_similar}\")\n",
    "    except:\n",
    "        print(f\"  FastText:  No similar words\")\n",
    "\n",
    "# Analyze character n-grams (FastText-specific)\n",
    "print(\"\\nCharacter N-grams Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def show_char_ngrams(word, min_n=3, max_n=6):\n",
    "    \"\"\"Show character n-grams for a word\"\"\"\n",
    "    word = f\"<{word}>\"  # FastText adds boundary markers\n",
    "    ngrams = []\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        for i in range(len(word) - n + 1):\n",
    "            ngrams.append(word[i:i+n])\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "example_words = ['cat', 'running', 'beautiful']\n",
    "for word in example_words:\n",
    "    ngrams = show_char_ngrams(word)\n",
    "    print(f\"{word:10}: {ngrams[:10]}...\")\n",
    "\n",
    "# Save models for later use\n",
    "print(\"\\nSaving models...\")\n",
    "sg_model.save(\"word2vec_skipgram.model\")\n",
    "cbow_model.save(\"word2vec_cbow.model\")\n",
    "fasttext_model.save(\"fasttext.model\")\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Document Embeddings with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Doc2Vec\n",
    "def prepare_doc2vec_data(corpus, labels=None):\n",
    "    \"\"\"Prepare tagged documents for Doc2Vec\"\"\"\n",
    "    tagged_docs = []\n",
    "    \n",
    "    for i, doc in enumerate(corpus):\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        tokens = [token for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # Create tag\n",
    "        if labels:\n",
    "            tag = f\"{labels[i]}_{i}\"\n",
    "        else:\n",
    "            tag = f\"DOC_{i}\"\n",
    "        \n",
    "        tagged_doc = TaggedDocument(words=tokens, tags=[tag])\n",
    "        tagged_docs.append(tagged_doc)\n",
    "    \n",
    "    return tagged_docs\n",
    "\n",
    "# Prepare training data\n",
    "tagged_docs = prepare_doc2vec_data(all_categorized_texts, all_labels)\n",
    "\n",
    "print(\"Doc2Vec Training:\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"Tagged documents: {len(tagged_docs)}\")\n",
    "print(f\"Sample tagged doc: {tagged_docs[0]}\")\n",
    "\n",
    "# Train Doc2Vec models\n",
    "doc2vec_models = {}\n",
    "\n",
    "# PV-DM (Distributed Memory)\n",
    "dm_model = Doc2Vec(\n",
    "    documents=tagged_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    dm=1,  # PV-DM\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "doc2vec_models['PV-DM'] = dm_model\n",
    "\n",
    "# PV-DBOW (Distributed Bag of Words)\n",
    "dbow_model = Doc2Vec(\n",
    "    documents=tagged_docs,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    dm=0,  # PV-DBOW\n",
    "    epochs=100,\n",
    "    seed=42\n",
    ")\n",
    "doc2vec_models['PV-DBOW'] = dbow_model\n",
    "\n",
    "print(f\"\\nTrained Doc2Vec models: {list(doc2vec_models.keys())}\")\n",
    "\n",
    "# Test document similarity\n",
    "print(\"\\nDocument Similarity Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def find_similar_documents(model, doc_id, topn=3):\n",
    "    \"\"\"Find similar documents using Doc2Vec\"\"\"\n",
    "    try:\n",
    "        similar_docs = model.dv.most_similar(doc_id, topn=topn)\n",
    "        return similar_docs\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Test with a few documents\n",
    "test_doc_indices = [0, 8, 16]  # Different categories\n",
    "\n",
    "for model_name, model in doc2vec_models.items():\n",
    "    print(f\"\\n{model_name} Model:\")\n",
    "    \n",
    "    for idx in test_doc_indices:\n",
    "        doc_tag = tagged_docs[idx].tags[0]\n",
    "        original_text = all_categorized_texts[idx]\n",
    "        original_label = all_labels[idx]\n",
    "        \n",
    "        similar_docs = find_similar_documents(model, doc_tag, topn=2)\n",
    "        \n",
    "        print(f\"\\n  Original ({original_label}): {original_text[:50]}...\")\n",
    "        print(f\"  Similar documents:\")\n",
    "        \n",
    "        for similar_tag, similarity in similar_docs:\n",
    "            # Extract index from tag\n",
    "            similar_idx = int(similar_tag.split('_')[-1])\n",
    "            similar_text = all_categorized_texts[similar_idx]\n",
    "            similar_label = all_labels[similar_idx]\n",
    "            \n",
    "            print(f\"    {similarity:.3f} ({similar_label}): {similar_text[:40]}...\")\n",
    "\n",
    "# Infer vector for new document\n",
    "print(\"\\nInferring Vector for New Document:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "new_doc = \"The computer processes data very quickly and efficiently.\"\n",
    "new_tokens = word_tokenize(new_doc.lower())\n",
    "new_tokens = [token for token in new_tokens if token.isalpha()]\n",
    "\n",
    "print(f\"New document: {new_doc}\")\n",
    "\n",
    "for model_name, model in doc2vec_models.items():\n",
    "    # Infer vector\n",
    "    inferred_vector = model.infer_vector(new_tokens)\n",
    "    \n",
    "    # Find most similar documents\n",
    "    similar_docs = model.dv.most_similar([inferred_vector], topn=3)\n",
    "    \n",
    "    print(f\"\\n{model_name} - Most similar documents:\")\n",
    "    for doc_tag, similarity in similar_docs:\n",
    "        doc_idx = int(doc_tag.split('_')[-1])\n",
    "        doc_text = all_categorized_texts[doc_idx]\n",
    "        doc_label = all_labels[doc_idx]\n",
    "        print(f\"  {similarity:.3f} ({doc_label}): {doc_text[:50]}...\")\n",
    "\n",
    "# Get document vectors\n",
    "print(\"\\nDocument Vector Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "dm_model = doc2vec_models['PV-DM']\n",
    "\n",
    "# Get vectors for documents from different categories\n",
    "category_vectors = defaultdict(list)\n",
    "\n",
    "for i, (text, label) in enumerate(zip(all_categorized_texts, all_labels)):\n",
    "    doc_tag = tagged_docs[i].tags[0]\n",
    "    if doc_tag in dm_model.dv:\n",
    "        vector = dm_model.dv[doc_tag]\n",
    "        category_vectors[label].append(vector)\n",
    "\n",
    "# Calculate average vectors per category\n",
    "category_avg_vectors = {}\n",
    "for category, vectors in category_vectors.items():\n",
    "    avg_vector = np.mean(vectors, axis=0)\n",
    "    category_avg_vectors[category] = avg_vector\n",
    "    print(f\"{category:10}: {len(vectors)} documents, avg vector shape: {avg_vector.shape}\")\n",
    "\n",
    "# Calculate similarity between categories\n",
    "print(\"\\nCategory Similarity Matrix:\")\n",
    "categories = list(category_avg_vectors.keys())\n",
    "similarity_matrix = np.zeros((len(categories), len(categories)))\n",
    "\n",
    "for i, cat1 in enumerate(categories):\n",
    "    for j, cat2 in enumerate(categories):\n",
    "        sim = cosine_similarity([category_avg_vectors[cat1]], [category_avg_vectors[cat2]])[0][0]\n",
    "        similarity_matrix[i][j] = sim\n",
    "\n",
    "# Display similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=categories, columns=categories)\n",
    "print(similarity_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model, words, method='tsne', title=\"Word Embeddings\"):\n",
    "    \"\"\"Visualize word embeddings using dimensionality reduction\"\"\"\n",
    "    \n",
    "    # Get vectors for words that exist in vocabulary\n",
    "    vectors = []\n",
    "    valid_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "            valid_words.append(word)\n",
    "    \n",
    "    if len(vectors) < 2:\n",
    "        print(f\"Not enough valid words for visualization ({len(vectors)} found)\")\n",
    "        return\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method == 'tsne':\n",
    "        if len(vectors) < 4:\n",
    "            print(\"t-SNE requires at least 4 samples, using PCA instead\")\n",
    "            method = 'pca'\n",
    "        else:\n",
    "            reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, len(vectors)-1))\n",
    "    \n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    reduced_vectors = reducer.fit_transform(vectors)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(valid_words):\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    plt.title(f\"{title} - {method.upper()}\")\n",
    "    plt.xlabel(f\"{method.upper()} Component 1\")\n",
    "    plt.ylabel(f\"{method.upper()} Component 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Collect words from our vocabulary\n",
    "sg_model = word2vec_models['skip-gram']\n",
    "available_words = list(sg_model.wv.key_to_index.keys())\n",
    "\n",
    "# Select interesting words for visualization\n",
    "words_to_visualize = [\n",
    "    'cat', 'dog', 'animals', 'bird', 'fish',\n",
    "    'sun', 'moon', 'sky', 'star', 'day', 'night',\n",
    "    'tree', 'flower', 'water', 'river', 'mountain',\n",
    "    'computer', 'phone', 'internet', 'technology',\n",
    "    'book', 'student', 'teacher', 'library',\n",
    "    'car', 'plane', 'road'\n",
    "]\n",
    "\n",
    "# Filter words that exist in vocabulary\n",
    "valid_words = [word for word in words_to_visualize if word in available_words]\n",
    "print(f\"Visualizing {len(valid_words)} words: {valid_words}\")\n",
    "\n",
    "# Visualize Skip-gram embeddings\n",
    "if len(valid_words) >= 4:\n",
    "    print(\"\\nSkip-gram Word Embeddings:\")\n",
    "    visualize_embeddings(sg_model, valid_words, 'tsne', \"Skip-gram Word2Vec\")\n",
    "    \n",
    "    print(\"\\nFastText Word Embeddings:\")\n",
    "    visualize_embeddings(fasttext_model, valid_words, 'tsne', \"FastText\")\n",
    "else:\n",
    "    print(\"Not enough words for visualization\")\n",
    "\n",
    "# Visualize document embeddings\n",
    "def visualize_document_embeddings(model, texts, labels):\n",
    "    \"\"\"Visualize document embeddings\"\"\"\n",
    "    \n",
    "    vectors = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "        doc_tag = f\"{label}_{i}\"\n",
    "        if doc_tag in model.dv:\n",
    "            vectors.append(model.dv[doc_tag])\n",
    "            valid_labels.append(label)\n",
    "    \n",
    "    if len(vectors) < 4:\n",
    "        print(f\"Not enough documents for visualization ({len(vectors)} found)\")\n",
    "        return\n",
    "    \n",
    "    vectors = np.array(vectors)\n",
    "    \n",
    "    # Use t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(vectors)-1))\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "    \n",
    "    # Plot with different colors for different categories\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    unique_labels = list(set(valid_labels))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = np.array(valid_labels) == label\n",
    "        plt.scatter(reduced_vectors[mask, 0], reduced_vectors[mask, 1], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=100)\n",
    "    \n",
    "    plt.title(\"Document Embeddings (Doc2Vec PV-DM)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nDocument Embeddings Visualization:\")\n",
    "dm_model = doc2vec_models['PV-DM']\n",
    "visualize_document_embeddings(dm_model, all_categorized_texts, all_labels)\n",
    "\n",
    "# Create embedding similarity heatmap\n",
    "def create_similarity_heatmap(model, words):\n",
    "    \"\"\"Create similarity heatmap for words\"\"\"\n",
    "    \n",
    "    valid_words = [word for word in words if word in model.wv]\n",
    "    \n",
    "    if len(valid_words) < 2:\n",
    "        print(\"Not enough valid words for heatmap\")\n",
    "        return\n",
    "    \n",
    "    # Create similarity matrix\n",
    "    similarity_matrix = np.zeros((len(valid_words), len(valid_words)))\n",
    "    \n",
    "    for i, word1 in enumerate(valid_words):\n",
    "        for j, word2 in enumerate(valid_words):\n",
    "            if i == j:\n",
    "                similarity_matrix[i][j] = 1.0\n",
    "            else:\n",
    "                sim = model.wv.similarity(word1, word2)\n",
    "                similarity_matrix[i][j] = sim\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(similarity_matrix, \n",
    "                xticklabels=valid_words, \n",
    "                yticklabels=valid_words,\n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                fmt='.3f')\n",
    "    plt.title(\"Word Similarity Heatmap (Skip-gram)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create heatmap for selected words\n",
    "heatmap_words = ['cat', 'dog', 'sun', 'moon', 'tree', 'water', 'computer', 'book']\n",
    "heatmap_words = [word for word in heatmap_words if word in available_words][:8]  # Limit to 8 words\n",
    "\n",
    "if len(heatmap_words) >= 2:\n",
    "    print(f\"\\nCreating similarity heatmap for: {heatmap_words}\")\n",
    "    create_similarity_heatmap(sg_model, heatmap_words)\n",
    "else:\n",
    "    print(\"Not enough words for similarity heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Embedding Evaluation and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding evaluation metrics\n",
    "class EmbeddingEvaluator:\n",
    "    \"\"\"Evaluate embedding quality\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def intrinsic_evaluation(self):\n",
    "        \"\"\"Intrinsic evaluation of embeddings\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Vocabulary coverage\n",
    "        vocab_size = len(self.model.wv.key_to_index)\n",
    "        results['vocabulary_size'] = vocab_size\n",
    "        \n",
    "        # Average vector magnitude\n",
    "        all_vectors = [self.model.wv[word] for word in self.model.wv.key_to_index.keys()]\n",
    "        avg_magnitude = np.mean([np.linalg.norm(vec) for vec in all_vectors])\n",
    "        results['avg_vector_magnitude'] = avg_magnitude\n",
    "        \n",
    "        # Vector dimensionality\n",
    "        vector_dim = self.model.wv.vector_size\n",
    "        results['vector_dimensionality'] = vector_dim\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def similarity_evaluation(self, word_pairs):\n",
    "        \"\"\"Evaluate semantic similarity\"\"\"\n",
    "        similarities = []\n",
    "        valid_pairs = []\n",
    "        \n",
    "        for word1, word2 in word_pairs:\n",
    "            if word1 in self.model.wv and word2 in self.model.wv:\n",
    "                sim = self.model.wv.similarity(word1, word2)\n",
    "                similarities.append(sim)\n",
    "                valid_pairs.append((word1, word2))\n",
    "        \n",
    "        return valid_pairs, similarities\n",
    "    \n",
    "    def analogy_evaluation(self, analogies):\n",
    "        \"\"\"Evaluate word analogies (A:B :: C:D)\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        results = []\n",
    "        \n",
    "        for a, b, c, expected_d in analogies:\n",
    "            if all(word in self.model.wv for word in [a, b, c, expected_d]):\n",
    "                try:\n",
    "                    # Find word D such that A:B :: C:D\n",
    "                    predicted = self.model.wv.most_similar(\n",
    "                        positive=[b, c], negative=[a], topn=1\n",
    "                    )\n",
    "                    \n",
    "                    predicted_d = predicted[0][0]\n",
    "                    is_correct = (predicted_d == expected_d)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'analogy': f\"{a}:{b} :: {c}:{expected_d}\",\n",
    "                        'predicted': predicted_d,\n",
    "                        'correct': is_correct\n",
    "                    })\n",
    "                    \n",
    "                    if is_correct:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        return accuracy, results\n",
    "\n",
    "# Evaluate Skip-gram model\n",
    "print(\"Embedding Evaluation:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "sg_evaluator = EmbeddingEvaluator(sg_model)\n",
    "\n",
    "# Intrinsic evaluation\n",
    "intrinsic_results = sg_evaluator.intrinsic_evaluation()\n",
    "print(\"Intrinsic Evaluation:\")\n",
    "for metric, value in intrinsic_results.items():\n",
    "    print(f\"  {metric}: {value}\")\n",
    "\n",
    "# Similarity evaluation\n",
    "word_pairs = [\n",
    "    ('cat', 'dog'),\n",
    "    ('sun', 'moon'),\n",
    "    ('day', 'night'),\n",
    "    ('tree', 'forest'),\n",
    "    ('computer', 'technology'),\n",
    "    ('book', 'library'),\n",
    "    ('water', 'ocean'),\n",
    "    ('car', 'road')\n",
    "]\n",
    "\n",
    "valid_pairs, similarities = sg_evaluator.similarity_evaluation(word_pairs)\n",
    "print(f\"\\nSimilarity Evaluation ({len(valid_pairs)} pairs):\")\n",
    "for (w1, w2), sim in zip(valid_pairs, similarities):\n",
    "    print(f\"  {w1:10} - {w2:10}: {sim:.3f}\")\n",
    "\n",
    "# Simple analogies (may not work well with small corpus)\n",
    "analogies = [\n",
    "    ('day', 'sun', 'night', 'moon'),\n",
    "    ('cat', 'kitten', 'dog', 'puppy'),  # These words might not be in vocabulary\n",
    "]\n",
    "\n",
    "analogy_accuracy, analogy_results = sg_evaluator.analogy_evaluation(analogies)\n",
    "print(f\"\\nAnalogy Evaluation (Accuracy: {analogy_accuracy:.3f}):\")\n",
    "for result in analogy_results:\n",
    "    status = \"✓\" if result['correct'] else \"✗\"\n",
    "    print(f\"  {status} {result['analogy']} → {result['predicted']}\")\n",
    "\n",
    "# Application: Text Classification using embeddings\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"APPLICATION: TEXT CLASSIFICATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def document_to_vector(doc, model, method='average'):\n",
    "    \"\"\"Convert document to vector using word embeddings\"\"\"\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token in model.wv]\n",
    "    \n",
    "    if not tokens:\n",
    "        return np.zeros(model.wv.vector_size)\n",
    "    \n",
    "    if method == 'average':\n",
    "        vectors = [model.wv[token] for token in tokens]\n",
    "        return np.mean(vectors, axis=0)\n",
    "    elif method == 'sum':\n",
    "        vectors = [model.wv[token] for token in tokens]\n",
    "        return np.sum(vectors, axis=0)\n",
    "    \n",
    "    return np.zeros(model.wv.vector_size)\n",
    "\n",
    "# Prepare classification data\n",
    "X_embeddings = []\n",
    "y_labels = []\n",
    "\n",
    "for text, label in zip(all_categorized_texts, all_labels):\n",
    "    vector = document_to_vector(text, sg_model, 'average')\n",
    "    if np.any(vector):  # Only include if vector is not all zeros\n",
    "        X_embeddings.append(vector)\n",
    "        y_labels.append(label)\n",
    "\n",
    "X_embeddings = np.array(X_embeddings)\n",
    "print(f\"Classification dataset: {X_embeddings.shape[0]} documents, {X_embeddings.shape[1]} features\")\n",
    "print(f\"Classes: {set(y_labels)}\")\n",
    "\n",
    "if X_embeddings.shape[0] > 6:  # Need enough samples for train/test split\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_embeddings, y_labels, test_size=0.3, random_state=42, stratify=y_labels\n",
    "    )\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nClassification Results:\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # Detailed report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Test with new document\n",
    "    new_test_doc = \"The artificial intelligence system processes information rapidly.\"\n",
    "    new_vector = document_to_vector(new_test_doc, sg_model, 'average')\n",
    "    \n",
    "    if np.any(new_vector):\n",
    "        predicted_class = classifier.predict([new_vector])[0]\n",
    "        prediction_proba = classifier.predict_proba([new_vector])[0]\n",
    "        \n",
    "        print(f\"\\nNew document classification:\")\n",
    "        print(f\"Document: {new_test_doc}\")\n",
    "        print(f\"Predicted class: {predicted_class}\")\n",
    "        print(f\"Class probabilities:\")\n",
    "        for class_name, prob in zip(classifier.classes_, prediction_proba):\n",
    "            print(f\"  {class_name}: {prob:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough samples for classification\")\n",
    "\n",
    "# Compare different vectorization methods\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VECTORIZATION METHOD COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_methods = {\n",
    "    'TF-IDF': TfidfVectorizer(max_features=100, stop_words='english'),\n",
    "    'Word2Vec_avg': lambda texts: np.array([document_to_vector(text, sg_model, 'average') for text in texts]),\n",
    "    'FastText_avg': lambda texts: np.array([document_to_vector(text, fasttext_model, 'average') for text in texts])\n",
    "}\n",
    "\n",
    "method_results = {}\n",
    "\n",
    "for method_name, vectorizer in comparison_methods.items():\n",
    "    try:\n",
    "        if method_name == 'TF-IDF':\n",
    "            X = vectorizer.fit_transform(all_categorized_texts).toarray()\n",
    "        else:\n",
    "            X = vectorizer(all_categorized_texts)\n",
    "        \n",
    "        # Filter out zero vectors\n",
    "        non_zero_mask = np.any(X != 0, axis=1)\n",
    "        X_filtered = X[non_zero_mask]\n",
    "        y_filtered = [all_labels[i] for i, mask in enumerate(non_zero_mask) if mask]\n",
    "        \n",
    "        if len(X_filtered) > 6:\n",
    "            # Train and evaluate\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_filtered, y_filtered, test_size=0.3, random_state=42\n",
    "            )\n",
    "            \n",
    "            clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            method_results[method_name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'samples': len(X_filtered),\n",
    "                'features': X_filtered.shape[1]\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {method_name}: {e}\")\n",
    "\n",
    "# Display comparison results\n",
    "if method_results:\n",
    "    print(\"Method Comparison Results:\")\n",
    "    print(f\"{'Method':<15} {'Accuracy':<10} {'Samples':<10} {'Features':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for method, results in method_results.items():\n",
    "        print(f\"{method:<15} {results['accuracy']:<10.3f} {results['samples']:<10} {results['features']:<10}\")\n",
    "else:\n",
    "    print(\"No comparison results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Challenge Section\n",
    "\n",
    "Complete these challenges to master word embeddings and text vectorization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Custom Similarity Metrics\n",
    "\n",
    "Implement different similarity metrics for comparing embeddings.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement Manhattan distance\n",
    "2. Implement Euclidean distance  \n",
    "3. Implement Jaccard similarity for binary vectors\n",
    "4. Compare different metrics on the same word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement custom similarity metrics\n",
    "class SimilarityMetrics:\n",
    "    @staticmethod\n",
    "    def manhattan_distance(vec1, vec2):\n",
    "        \"\"\"\n",
    "        Calculate Manhattan (L1) distance between two vectors\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def euclidean_distance(vec1, vec2):\n",
    "        \"\"\"\n",
    "        Calculate Euclidean (L2) distance between two vectors\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def jaccard_similarity(vec1, vec2, threshold=0.0):\n",
    "        \"\"\"\n",
    "        Calculate Jaccard similarity for binary/sparse vectors\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_metrics(vec1, vec2):\n",
    "        \"\"\"\n",
    "        Compare all similarity metrics\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test your implementations\n",
    "print(\"Challenge L1: Implement custom similarity metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Embedding Arithmetic Explorer\n",
    "\n",
    "Create an interactive tool for exploring word arithmetic with embeddings.\n",
    "\n",
    "**Tasks:**\n",
    "1. Build function to perform word arithmetic (A - B + C)\n",
    "2. Create batch arithmetic operations\n",
    "3. Visualize arithmetic results\n",
    "4. Test famous examples (king - man + woman ≈ queen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement embedding arithmetic explorer\n",
    "class EmbeddingArithmetic:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def word_arithmetic(self, positive_words, negative_words, topn=5):\n",
    "        \"\"\"\n",
    "        Perform word arithmetic: positive_words - negative_words\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def batch_arithmetic(self, arithmetic_examples):\n",
    "        \"\"\"\n",
    "        Perform multiple arithmetic operations\n",
    "        Input: list of (positive_words, negative_words, expected_result)\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def visualize_arithmetic(self, positive_words, negative_words, result_words):\n",
    "        \"\"\"\n",
    "        Visualize word arithmetic in 2D space\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# Test famous examples\n",
    "# arithmetic_explorer = EmbeddingArithmetic(sg_model)\n",
    "print(\"Challenge L2: Implement embedding arithmetic explorer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Embedding Quality Dashboard\n",
    "\n",
    "Create a comprehensive dashboard for evaluating embedding quality.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement vocabulary coverage analysis\n",
    "2. Create embedding density/sparsity metrics\n",
    "3. Build similarity distribution analysis\n",
    "4. Generate comprehensive quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement embedding quality dashboard\n",
    "class EmbeddingQualityDashboard:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def vocabulary_coverage_analysis(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Analyze what percentage of test corpus words are covered\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def embedding_density_metrics(self):\n",
    "        \"\"\"\n",
    "        Calculate sparsity and density of embeddings\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def similarity_distribution_analysis(self, sample_size=1000):\n",
    "        \"\"\"\n",
    "        Analyze distribution of similarities between random word pairs\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def generate_quality_report(self, test_corpus=None):\n",
    "        \"\"\"\n",
    "        Generate comprehensive quality report with visualizations\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge L3: Implement embedding quality dashboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Custom Word2Vec Implementation\n",
    "\n",
    "Implement a simplified version of Word2Vec from scratch.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement Skip-gram with negative sampling\n",
    "2. Create vocabulary building with frequency filtering\n",
    "3. Implement training loop with gradient descent\n",
    "4. Compare results with Gensim's Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement custom Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, corpus, window_size=5, negative_samples=5):\n",
    "        \"\"\"\n",
    "        Create dataset for Word2Vec training\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        \"\"\"\n",
    "        Implement Skip-gram model architecture\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, center_word, context_words, negative_words):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class CustomWord2Vec:\n",
    "    def __init__(self, embedding_dim=100, window_size=5, negative_samples=5):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        \n",
    "    def train(self, corpus, epochs=100, lr=0.001):\n",
    "        \"\"\"\n",
    "        Train the Word2Vec model\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        \"\"\"\n",
    "        Get vector for a word\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def most_similar(self, word, topn=5):\n",
    "        \"\"\"\n",
    "        Find most similar words\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge M1: Implement custom Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Multi-language Embedding Alignment\n",
    "\n",
    "Align embeddings across different languages.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create bilingual word pairs dataset\n",
    "2. Implement Procrustes alignment\n",
    "3. Evaluate cross-lingual similarity\n",
    "4. Build translation using aligned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-language embedding alignment\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "class CrossLingualEmbeddings:\n",
    "    def __init__(self, source_embeddings, target_embeddings):\n",
    "        self.source_embeddings = source_embeddings\n",
    "        self.target_embeddings = target_embeddings\n",
    "        self.alignment_matrix = None\n",
    "    \n",
    "    def create_bilingual_pairs(self, word_pairs):\n",
    "        \"\"\"\n",
    "        Create training pairs for alignment\n",
    "        word_pairs: list of (source_word, target_word) tuples\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def procrustes_alignment(self, source_vectors, target_vectors):\n",
    "        \"\"\"\n",
    "        Learn alignment matrix using Procrustes analysis\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def align_embeddings(self, bilingual_pairs):\n",
    "        \"\"\"\n",
    "        Align source embeddings to target space\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def translate_word(self, source_word, topn=5):\n",
    "        \"\"\"\n",
    "        Translate word using aligned embeddings\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def evaluate_alignment(self, test_pairs):\n",
    "        \"\"\"\n",
    "        Evaluate alignment quality on test pairs\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge M2: Implement multi-language embedding alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Dynamic Embedding Updates\n",
    "\n",
    "Implement online learning for embeddings with new vocabulary.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement incremental vocabulary expansion\n",
    "2. Create online embedding updates\n",
    "3. Handle concept drift in embeddings\n",
    "4. Evaluate stability vs. adaptability trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement dynamic embedding updates\n",
    "class DynamicEmbeddings:\n",
    "    def __init__(self, base_model, update_rate=0.01, stability_factor=0.9):\n",
    "        self.base_model = base_model\n",
    "        self.update_rate = update_rate\n",
    "        self.stability_factor = stability_factor\n",
    "        self.embedding_history = {}\n",
    "    \n",
    "    def add_new_vocabulary(self, new_words, context_sentences):\n",
    "        \"\"\"\n",
    "        Add new words to vocabulary incrementally\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def online_update(self, new_sentences, batch_size=32):\n",
    "        \"\"\"\n",
    "        Update embeddings with new sentences online\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def detect_concept_drift(self, words_to_monitor, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Detect if word meanings have drifted significantly\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def adaptive_update_strategy(self, word, new_vector):\n",
    "        \"\"\"\n",
    "        Adaptively update embeddings based on confidence and drift\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def evaluate_stability(self, test_words, num_updates=10):\n",
    "        \"\"\"\n",
    "        Evaluate how stable embeddings are over updates\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge M3: Implement dynamic embedding updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 7: Contextual Embeddings from Scratch\n",
    "\n",
    "Implement a simplified contextual embedding model.\n",
    "\n",
    "**Tasks:**\n",
    "1. Build bidirectional LSTM for context\n",
    "2. Implement attention mechanism\n",
    "3. Create context-dependent word representations\n",
    "4. Compare with static embeddings on polysemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement contextual embeddings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContextualEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
    "        super(ContextualEmbedding, self).__init__()\n",
    "        \"\"\"\n",
    "        Implement contextual embedding model\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass to get contextual embeddings\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \"\"\"\n",
    "        Implement attention mechanism\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class ContextualEmbeddingTrainer:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def train(self, corpus, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train contextual embedding model\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def get_contextual_embedding(self, sentence, word_index):\n",
    "        \"\"\"\n",
    "        Get contextual embedding for specific word in sentence\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def evaluate_polysemy(self, polysemous_words, contexts):\n",
    "        \"\"\"\n",
    "        Evaluate how well model handles polysemous words\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge H1: Implement contextual embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 8: Embedding Space Analysis and Manipulation\n",
    "\n",
    "Deep analysis and manipulation of embedding spaces.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement embedding space clustering\n",
    "2. Create semantic subspace identification\n",
    "3. Build embedding space transformations\n",
    "4. Develop embedding space quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement embedding space analysis\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "class EmbeddingSpaceAnalyzer:\n",
    "    def __init__(self, embeddings, vocab):\n",
    "        self.embeddings = embeddings\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def cluster_embedding_space(self, method='kmeans', n_clusters=10):\n",
    "        \"\"\"\n",
    "        Cluster words in embedding space\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def identify_semantic_subspaces(self, semantic_categories):\n",
    "        \"\"\"\n",
    "        Identify subspaces corresponding to semantic categories\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def analyze_embedding_geometry(self):\n",
    "        \"\"\"\n",
    "        Analyze geometric properties of embedding space\n",
    "        - Dimensionality\n",
    "        - Isotropy\n",
    "        - Local density\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def find_semantic_directions(self, word_pairs):\n",
    "        \"\"\"\n",
    "        Find directions in space corresponding to semantic relations\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class EmbeddingSpaceManipulator:\n",
    "    def __init__(self, embeddings, vocab):\n",
    "        self.embeddings = embeddings\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def debias_embeddings(self, bias_direction, words_to_debias):\n",
    "        \"\"\"\n",
    "        Remove bias from embeddings along specified direction\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def retrofit_embeddings(self, semantic_constraints):\n",
    "        \"\"\"\n",
    "        Retrofit embeddings using semantic constraints\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def compress_embeddings(self, target_dim, method='pca'):\n",
    "        \"\"\"\n",
    "        Compress embeddings to lower dimensions\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def enhance_embedding_isotropy(self):\n",
    "        \"\"\"\n",
    "        Make embedding space more isotropic\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge H2: Implement embedding space analysis and manipulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 9: Hybrid Embedding Architecture\n",
    "\n",
    "Create a hybrid system combining multiple embedding approaches.\n",
    "\n",
    "**Tasks:**\n",
    "1. Combine static and contextual embeddings\n",
    "2. Integrate character and word-level information\n",
    "3. Implement adaptive embedding selection\n",
    "4. Build multi-granularity embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement hybrid embedding architecture\n",
    "class HybridEmbeddingSystem:\n",
    "    def __init__(self):\n",
    "        self.static_embeddings = None\n",
    "        self.contextual_model = None\n",
    "        self.char_embeddings = None\n",
    "        self.fusion_weights = None\n",
    "    \n",
    "    def initialize_components(self, vocab_size, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initialize all embedding components\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def adaptive_embedding_selection(self, word, context, selection_criteria):\n",
    "        \"\"\"\n",
    "        Adaptively select which embedding to use based on criteria\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def multi_granularity_fusion(self, word, context):\n",
    "        \"\"\"\n",
    "        Combine character, subword, word, and context embeddings\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def dynamic_fusion_weights(self, word_properties, context_properties):\n",
    "        \"\"\"\n",
    "        Dynamically determine fusion weights based on properties\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def train_fusion_network(self, training_data, downstream_task):\n",
    "        \"\"\"\n",
    "        Train fusion network for specific downstream task\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def evaluate_hybrid_system(self, test_data, baselines):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation against baseline methods\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class MetaEmbeddingLearner:\n",
    "    def __init__(self, embedding_sources):\n",
    "        self.embedding_sources = embedding_sources\n",
    "    \n",
    "    def learn_optimal_combination(self, validation_data, task_type):\n",
    "        \"\"\"\n",
    "        Learn optimal way to combine different embedding sources\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def transfer_combination_strategy(self, source_task, target_task):\n",
    "        \"\"\"\n",
    "        Transfer learned combination strategy to new task\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge H3: Implement hybrid embedding architecture\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
