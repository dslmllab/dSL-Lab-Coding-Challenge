{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation: Creating Shakespeare-Style Text\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand RNN architecture** and its applications in sequence modeling\n",
    "2. **Build character-level text generation models** using PyTorch\n",
    "3. **Implement different RNN variants** (vanilla RNN, LSTM, GRU)\n",
    "4. **Apply temperature sampling** for controlling text generation diversity\n",
    "5. **Fine-tune models** for different writing styles and domains\n",
    "6. **Evaluate text generation quality** using various metrics\n",
    "7. **Understand the challenges** and limitations of neural text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to RNN Text Generation\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)** are particularly well-suited for sequential data like text because they can maintain memory of previous inputs. In text generation, we train the network to predict the next character (or word) given the previous characters in the sequence.\n",
    "\n",
    "### Why Shakespeare?\n",
    "- **Rich linguistic patterns**: Shakespeare's works contain complex vocabulary and grammar\n",
    "- **Distinctive style**: Easily recognizable writing patterns\n",
    "- **Sufficient data**: Large corpus available for training\n",
    "- **Cultural significance**: Widely known and appreciated\n",
    "\n",
    "### Key Concepts:\n",
    "- **Character-level modeling**: Predicting one character at a time\n",
    "- **Hidden states**: Memory mechanism in RNNs\n",
    "- **Temperature sampling**: Controlling randomness in generation\n",
    "- **Sequence-to-sequence learning**: Learning patterns in sequential data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn nltk torch tqdm scikit-learn",
    "",
    "# Enable ipywidgets for Jupyter",
    "!jupyter nbextension enable --py widgetsnbextension",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import unidecode\n",
    "import requests\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style and random seeds for reproducibility\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_shakespeare_data():\n",
    "    \"\"\"Download Shakespeare text data\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        with open('shakespeare.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(\"Successfully downloaded Shakespeare data\")\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "        # Fallback: create sample data\n",
    "        sample_text = \"\"\"\n",
    "        HAMLET:\n",
    "        To be, or not to be, that is the question:\n",
    "        Whether 'tis nobler in the mind to suffer\n",
    "        The slings and arrows of outrageous fortune,\n",
    "        Or to take arms against a sea of troubles,\n",
    "        And by opposing end them.\n",
    "        \n",
    "        MACBETH:\n",
    "        Tomorrow, and tomorrow, and tomorrow,\n",
    "        Creeps in this petty pace from day to day,\n",
    "        To the last syllable of recorded time;\n",
    "        And all our yesterdays have lighted fools\n",
    "        The way to dusty death.\n",
    "        \"\"\"\n",
    "        # Repeat sample text to make it longer\n",
    "        sample_text = sample_text * 100\n",
    "        with open('shakespeare.txt', 'w') as f:\n",
    "            f.write(sample_text)\n",
    "        return sample_text\n",
    "\n",
    "# Download and load the data\n",
    "text_data = download_shakespeare_data()\n",
    "\n",
    "# Read the file and clean it\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Convert unicode to ASCII\n",
    "text = unidecode.unidecode(raw_text)\n",
    "\n",
    "print(f\"Text length: {len(text):,} characters\")\n",
    "print(f\"First 500 characters:\\n{text[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the text\n",
    "def analyze_text(text):\n",
    "    \"\"\"Analyze the text data\"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    char_counts = Counter(text)\n",
    "    \n",
    "    print(f\"Total characters: {len(text):,}\")\n",
    "    print(f\"Unique characters: {len(chars)}\")\n",
    "    print(f\"Characters: {''.join(chars)}\")\n",
    "    \n",
    "    # Character frequency analysis\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Most common characters\n",
    "    plt.subplot(2, 2, 1)\n",
    "    common_chars = char_counts.most_common(20)\n",
    "    chars_list, counts_list = zip(*common_chars)\n",
    "    plt.bar(range(len(chars_list)), counts_list)\n",
    "    plt.xticks(range(len(chars_list)), [repr(c) for c in chars_list], rotation=45)\n",
    "    plt.title('Top 20 Most Common Characters')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Character distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist([char_counts[c] for c in chars], bins=50, alpha=0.7)\n",
    "    plt.title('Character Frequency Distribution')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Number of Characters')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Line length analysis\n",
    "    plt.subplot(2, 2, 3)\n",
    "    lines = text.split('\\n')\n",
    "    line_lengths = [len(line) for line in lines if line.strip()]\n",
    "    plt.hist(line_lengths, bins=50, alpha=0.7)\n",
    "    plt.title('Line Length Distribution')\n",
    "    plt.xlabel('Characters per Line')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Word analysis\n",
    "    plt.subplot(2, 2, 4)\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    word_lengths = [len(word) for word in words]\n",
    "    plt.hist(word_lengths, bins=range(1, max(word_lengths)+1), alpha=0.7)\n",
    "    plt.title('Word Length Distribution')\n",
    "    plt.xlabel('Characters per Word')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return chars, char_counts\n",
    "\n",
    "chars, char_counts = analyze_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"Dataset class for character-level text data\"\"\"\n",
    "    \n",
    "    def __init__(self, text, sequence_length=100):\n",
    "        self.text = text\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Create character mappings\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.chars)}\n",
    "        \n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.text_length = len(text)\n",
    "        \n",
    "        print(f\"Dataset created:\")\n",
    "        print(f\"  - Text length: {self.text_length:,} characters\")\n",
    "        print(f\"  - Vocabulary size: {self.vocab_size} characters\")\n",
    "        print(f\"  - Sequence length: {self.sequence_length}\")\n",
    "        print(f\"  - Total sequences: {self.text_length - self.sequence_length:,}\")\n",
    "    \n",
    "    def char_to_tensor(self, char):\n",
    "        \"\"\"Convert character to tensor\"\"\"\n",
    "        return torch.tensor([self.char_to_idx[char]], dtype=torch.long).to(device)\n",
    "    \n",
    "    def string_to_tensor(self, string):\n",
    "        \"\"\"Convert string to tensor\"\"\"\n",
    "        tensor = torch.zeros(len(string), dtype=torch.long)\n",
    "        for i, char in enumerate(string):\n",
    "            tensor[i] = self.char_to_idx[char]\n",
    "        return tensor.to(device)\n",
    "    \n",
    "    def get_random_batch(self, batch_size=1):\n",
    "        \"\"\"Get random training batch\"\"\"\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            start_idx = random.randint(0, self.text_length - self.sequence_length - 1)\n",
    "            end_idx = start_idx + self.sequence_length + 1\n",
    "            \n",
    "            chunk = self.text[start_idx:end_idx]\n",
    "            input_seq = self.string_to_tensor(chunk[:-1])\n",
    "            target_seq = self.string_to_tensor(chunk[1:])\n",
    "            \n",
    "            inputs.append(input_seq)\n",
    "            targets.append(target_seq)\n",
    "        \n",
    "        # Stack tensors\n",
    "        inputs = torch.stack(inputs)\n",
    "        targets = torch.stack(targets)\n",
    "        \n",
    "        return inputs, targets\n",
    "    \n",
    "    def tensor_to_string(self, tensor):\n",
    "        \"\"\"Convert tensor back to string\"\"\"\n",
    "        if tensor.dim() > 1:\n",
    "            tensor = tensor.squeeze()\n",
    "        return ''.join([self.idx_to_char[idx.item()] for idx in tensor])\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextDataset(text, sequence_length=100)\n",
    "\n",
    "# Test the dataset\n",
    "sample_input, sample_target = dataset.get_random_batch(batch_size=1)\n",
    "print(f\"\\nSample input shape: {sample_input.shape}\")\n",
    "print(f\"Sample target shape: {sample_target.shape}\")\n",
    "print(f\"\\nSample text: '{dataset.tensor_to_string(sample_input[0][:50])}'\")\n",
    "print(f\"Target text:  '{dataset.tensor_to_string(sample_target[0][:50])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN Model Architectures\n",
    "\n",
    "We'll implement multiple RNN architectures to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \"\"\"Character-level RNN for text generation\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, \n",
    "                 rnn_type='GRU', dropout=0.2):\n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # RNN layer\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                              batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers,\n",
    "                             batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        else:  # Vanilla RNN\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers,\n",
    "                             batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Output layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # RNN\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Apply dropout and linear layer\n",
    "        rnn_out = self.dropout(rnn_out)\n",
    "        \n",
    "        # Reshape for linear layer\n",
    "        rnn_out = rnn_out.reshape(-1, self.hidden_dim)\n",
    "        output = self.fc(rnn_out)\n",
    "        \n",
    "        # Reshape back to (batch_size, seq_len, vocab_size)\n",
    "        output = output.view(batch_size, seq_len, self.vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "            return (h0, c0)\n",
    "        else:\n",
    "            return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "# Test model creation\n",
    "model = CharRNN(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    rnn_type='GRU',\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created: {model.rnn_type}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input, test_target = dataset.get_random_batch(batch_size=2)\n",
    "test_output, test_hidden = model(test_input)\n",
    "print(f\"\\nTest shapes:\")\n",
    "print(f\"Input: {test_input.shape}\")\n",
    "print(f\"Output: {test_output.shape}\")\n",
    "print(f\"Hidden: {test_hidden.shape if not isinstance(test_hidden, tuple) else test_hidden[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, num_epochs=1000, learning_rate=0.002, \n",
    "                batch_size=32, print_every=100, save_every=500):\n",
    "    \"\"\"Train the RNN model\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.8)\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Get batch\n",
    "        inputs, targets = dataset.get_random_batch(batch_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output.reshape(-1, dataset.vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % print_every == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_loss = sum(losses[-print_every:]) / print_every\n",
    "            print(f'Epoch [{epoch}/{num_epochs}] - Loss: {avg_loss:.4f} - Time: {elapsed:.1f}s')\n",
    "            \n",
    "            # Generate sample text\n",
    "            sample_text = generate_text(model, dataset, prime_str=\"To be\", \n",
    "                                      predict_len=100, temperature=0.8)\n",
    "            print(f'Sample: \"{sample_text[:80]}...\"\\n')\n",
    "        \n",
    "        # Save model\n",
    "        if epoch % save_every == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "                'losses': losses\n",
    "            }, f'checkpoint_epoch_{epoch}.pth')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_text(model, dataset, prime_str='A', predict_len=200, temperature=0.8):\n",
    "    \"\"\"Generate text using trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Initialize\n",
    "        hidden = model.init_hidden(1)\n",
    "        prime_input = dataset.string_to_tensor(prime_str).unsqueeze(0)\n",
    "        predicted = prime_str\n",
    "        \n",
    "        # Use priming string to build up hidden state\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = model(prime_input[:, p:p+1], hidden)\n",
    "        \n",
    "        # Start with last character of prime string\n",
    "        inp = prime_input[:, -1:]\n",
    "        \n",
    "        # Generate characters\n",
    "        for p in range(predict_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            \n",
    "            # Apply temperature\n",
    "            output_dist = output[0, -1].div(temperature).exp()\n",
    "            \n",
    "            # Sample from distribution\n",
    "            top_i = torch.multinomial(output_dist, 1)[0]\n",
    "            \n",
    "            # Add predicted character to string\n",
    "            predicted_char = dataset.idx_to_char[top_i.item()]\n",
    "            predicted += predicted_char\n",
    "            \n",
    "            # Use as next input\n",
    "            inp = torch.tensor([[top_i]], dtype=torch.long).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    return predicted\n",
    "\n",
    "def evaluate_model(model, dataset, num_samples=100):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            inputs, targets = dataset.get_random_batch(batch_size=1)\n",
    "            hidden = model.init_hidden(1)\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            \n",
    "            loss = criterion(output.reshape(-1, dataset.vocab_size), targets.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / num_samples\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    model.train()\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "print(\"Training functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = CharRNN(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    rnn_type='GRU',\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"Training {model.rnn_type} model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Train the model\n",
    "losses = train_model(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    num_epochs=1000,\n",
    "    learning_rate=0.002,\n",
    "    batch_size=32,\n",
    "    print_every=100,\n",
    "    save_every=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Smoothed loss curve\n",
    "plt.subplot(2, 2, 2)\n",
    "window_size = 50\n",
    "smoothed_losses = []\n",
    "for i in range(len(losses)):\n",
    "    start = max(0, i - window_size + 1)\n",
    "    smoothed_losses.append(sum(losses[start:i+1]) / (i - start + 1))\n",
    "\n",
    "plt.plot(smoothed_losses)\n",
    "plt.title('Smoothed Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Loss distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(losses[-500:], bins=50, alpha=0.7)\n",
    "plt.title('Loss Distribution (Last 500 epochs)')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Learning progress\n",
    "plt.subplot(2, 2, 4)\n",
    "epochs = list(range(100, len(losses)+1, 100))\n",
    "epoch_losses = [sum(losses[i-100:i])/100 for i in epochs]\n",
    "plt.plot(epochs, epoch_losses, 'o-')\n",
    "plt.title('Learning Progress (100-epoch averages)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate final model\n",
    "final_loss, perplexity = evaluate_model(model, dataset)\n",
    "print(f\"\\nFinal evaluation:\")\n",
    "print(f\"Average loss: {final_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Text Generation and Temperature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_temperatures(model, dataset, prime_str=\"To be\", predict_len=200):\n",
    "    \"\"\"Compare text generation with different temperatures\"\"\"\n",
    "    temperatures = [0.2, 0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "    \n",
    "    print(f\"Comparing temperatures for prompt: '{prime_str}'\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(model, dataset, prime_str, predict_len, temp)\n",
    "        print(f\"Temperature {temp}:\")\n",
    "        print(f\"{generated}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Compare different temperatures\n",
    "compare_temperatures(model, dataset, \"HAMLET:\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_generation_diversity(model, dataset, prime_str=\"To be\", num_samples=10, temperature=0.8):\n",
    "    \"\"\"Analyze diversity in generated text\"\"\"\n",
    "    generations = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        text = generate_text(model, dataset, prime_str, 100, temperature)\n",
    "        generations.append(text)\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    all_chars = ''.join(generations)\n",
    "    char_counts = Counter(all_chars)\n",
    "    \n",
    "    # Unique characters\n",
    "    unique_chars = len(set(all_chars))\n",
    "    \n",
    "    # Calculate bigram diversity\n",
    "    bigrams = [all_chars[i:i+2] for i in range(len(all_chars)-1)]\n",
    "    unique_bigrams = len(set(bigrams))\n",
    "    \n",
    "    print(f\"Generation Diversity Analysis (Temperature: {temperature})\")\n",
    "    print(f\"Number of samples: {num_samples}\")\n",
    "    print(f\"Total characters generated: {len(all_chars)}\")\n",
    "    print(f\"Unique characters: {unique_chars}/{dataset.vocab_size} ({100*unique_chars/dataset.vocab_size:.1f}%)\")\n",
    "    print(f\"Unique bigrams: {unique_bigrams}\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(f\"\\nSample generations:\")\n",
    "    for i, gen in enumerate(generations[:3]):\n",
    "        print(f\"{i+1}. {gen[:80]}...\")\n",
    "    \n",
    "    return generations, char_counts\n",
    "\n",
    "# Analyze generation diversity\n",
    "generations, char_counts = analyze_generation_diversity(model, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rnn_architectures(dataset, architectures=['RNN', 'GRU', 'LSTM'], \n",
    "                              num_epochs=500, hidden_dim=128):\n",
    "    \"\"\"Compare different RNN architectures\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for arch in architectures:\n",
    "        print(f\"\\nTraining {arch} model...\")\n",
    "        \n",
    "        # Create model\n",
    "        model = CharRNN(\n",
    "            vocab_size=dataset.vocab_size,\n",
    "            embedding_dim=64,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=1,\n",
    "            rnn_type=arch,\n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        losses = train_model(\n",
    "            model=model,\n",
    "            dataset=dataset,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=0.002,\n",
    "            batch_size=16,\n",
    "            print_every=100\n",
    "        )\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        final_loss, perplexity = evaluate_model(model, dataset)\n",
    "        \n",
    "        # Generate sample\n",
    "        sample_text = generate_text(model, dataset, \"To be\", 100, 0.8)\n",
    "        \n",
    "        results[arch] = {\n",
    "            'model': model,\n",
    "            'losses': losses,\n",
    "            'final_loss': final_loss,\n",
    "            'perplexity': perplexity,\n",
    "            'training_time': training_time,\n",
    "            'sample_text': sample_text,\n",
    "            'num_params': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ARCHITECTURE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    comparison_df = []\n",
    "    for arch, result in results.items():\n",
    "        comparison_df.append({\n",
    "            'Architecture': arch,\n",
    "            'Parameters': f\"{result['num_params']:,}\",\n",
    "            'Final Loss': f\"{result['final_loss']:.4f}\",\n",
    "            'Perplexity': f\"{result['perplexity']:.2f}\",\n",
    "            'Training Time': f\"{result['training_time']:.1f}s\"\n",
    "        })\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(comparison_df)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    plt.subplot(2, 3, 1)\n",
    "    for arch, result in results.items():\n",
    "        plt.plot(result['losses'], label=arch)\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Final metrics\n",
    "    architectures_list = list(results.keys())\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    final_losses = [results[arch]['final_loss'] for arch in architectures_list]\n",
    "    plt.bar(architectures_list, final_losses)\n",
    "    plt.title('Final Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    perplexities = [results[arch]['perplexity'] for arch in architectures_list]\n",
    "    plt.bar(architectures_list, perplexities)\n",
    "    plt.title('Perplexity')\n",
    "    plt.ylabel('Perplexity')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    training_times = [results[arch]['training_time'] for arch in architectures_list]\n",
    "    plt.bar(architectures_list, training_times)\n",
    "    plt.title('Training Time')\n",
    "    plt.ylabel('Seconds')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    num_params = [results[arch]['num_params'] for arch in architectures_list]\n",
    "    plt.bar(architectures_list, num_params)\n",
    "    plt.title('Number of Parameters')\n",
    "    plt.ylabel('Parameters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show sample texts\n",
    "    print(\"\\nSample generated texts:\")\n",
    "    for arch, result in results.items():\n",
    "        print(f\"\\n{arch}: {result['sample_text'][:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Note: This will take some time to run\n",
    "# comparison_results = compare_rnn_architectures(dataset, num_epochs=300)\n",
    "print(\"Architecture comparison function defined. Uncomment to run comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Text Generation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(model, dataset, prime_str='A', predict_len=200, top_p=0.9):\n",
    "    \"\"\"Generate text using nucleus (top-p) sampling\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1)\n",
    "        prime_input = dataset.string_to_tensor(prime_str).unsqueeze(0)\n",
    "        predicted = prime_str\n",
    "        \n",
    "        # Build up hidden state with prime string\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = model(prime_input[:, p:p+1], hidden)\n",
    "        \n",
    "        inp = prime_input[:, -1:]\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = F.softmax(output[0, -1], dim=0)\n",
    "            \n",
    "            # Sort probabilities\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            \n",
    "            # Find cumulative probabilities\n",
    "            cumsum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "            \n",
    "            # Find cutoff index\n",
    "            cutoff_idx = torch.where(cumsum_probs > top_p)[0]\n",
    "            if len(cutoff_idx) > 0:\n",
    "                cutoff_idx = cutoff_idx[0].item()\n",
    "            else:\n",
    "                cutoff_idx = len(sorted_probs) - 1\n",
    "            \n",
    "            # Keep only top-p probabilities\n",
    "            top_indices = sorted_indices[:cutoff_idx + 1]\n",
    "            top_probs = sorted_probs[:cutoff_idx + 1]\n",
    "            \n",
    "            # Renormalize\n",
    "            top_probs = top_probs / top_probs.sum()\n",
    "            \n",
    "            # Sample from top-p distribution\n",
    "            sampled_idx = torch.multinomial(top_probs, 1)[0]\n",
    "            selected_idx = top_indices[sampled_idx]\n",
    "            \n",
    "            # Add predicted character\n",
    "            predicted_char = dataset.idx_to_char[selected_idx.item()]\n",
    "            predicted += predicted_char\n",
    "            \n",
    "            inp = torch.tensor([[selected_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    return predicted\n",
    "\n",
    "def top_k_sampling(model, dataset, prime_str='A', predict_len=200, top_k=40):\n",
    "    \"\"\"Generate text using top-k sampling\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1)\n",
    "        prime_input = dataset.string_to_tensor(prime_str).unsqueeze(0)\n",
    "        predicted = prime_str\n",
    "        \n",
    "        # Build up hidden state with prime string\n",
    "        for p in range(len(prime_str) - 1):\n",
    "            _, hidden = model(prime_input[:, p:p+1], hidden)\n",
    "        \n",
    "        inp = prime_input[:, -1:]\n",
    "        \n",
    "        for p in range(predict_len):\n",
    "            output, hidden = model(inp, hidden)\n",
    "            \n",
    "            # Get top-k values and indices\n",
    "            top_k_values, top_k_indices = torch.topk(output[0, -1], top_k)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            top_k_probs = F.softmax(top_k_values, dim=0)\n",
    "            \n",
    "            # Sample from top-k distribution\n",
    "            sampled_idx = torch.multinomial(top_k_probs, 1)[0]\n",
    "            selected_idx = top_k_indices[sampled_idx]\n",
    "            \n",
    "            # Add predicted character\n",
    "            predicted_char = dataset.idx_to_char[selected_idx.item()]\n",
    "            predicted += predicted_char\n",
    "            \n",
    "            inp = torch.tensor([[selected_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    model.train()\n",
    "    return predicted\n",
    "\n",
    "# Compare different sampling methods\n",
    "print(\"Comparing different sampling methods:\")\n",
    "print(\"\\n1. Temperature sampling (0.8):\")\n",
    "temp_text = generate_text(model, dataset, \"HAMLET:\", 150, 0.8)\n",
    "print(temp_text)\n",
    "\n",
    "print(\"\\n2. Top-k sampling (k=40):\")\n",
    "topk_text = top_k_sampling(model, dataset, \"HAMLET:\", 150, 40)\n",
    "print(topk_text)\n",
    "\n",
    "print(\"\\n3. Nucleus sampling (p=0.9):\")\n",
    "nucleus_text = nucleus_sampling(model, dataset, \"HAMLET:\", 150, 0.9)\n",
    "print(nucleus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Challenge Section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Word-Level Text Generation\n",
    "\n",
    "Implement a word-level RNN instead of character-level.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a word-level dataset class\n",
    "2. Handle vocabulary with unknown word tokens\n",
    "3. Implement word-level RNN model\n",
    "4. Compare word-level vs character-level generation\n",
    "5. Analyze the trade-offs (vocabulary size, coherence, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement word-level text generation\n",
    "class WordDataset:\n",
    "    def __init__(self, text, sequence_length=50, min_word_freq=2):\n",
    "        \"\"\"\n",
    "        Create word-level dataset\n",
    "        - Handle tokenization\n",
    "        - Create vocabulary with UNK tokens\n",
    "        - Implement word-to-index mapping\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text into words\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def build_vocabulary(self, words, min_freq):\n",
    "        \"\"\"\n",
    "        Build vocabulary with frequency filtering\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
    "        super(WordRNN, self).__init__()\n",
    "        # TODO: Implement word-level RNN\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# TODO: Compare word-level vs character-level performance\n",
    "def compare_word_vs_char_generation():\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Advanced RNN Architectures\n",
    "\n",
    "Implement and compare advanced RNN architectures.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement bidirectional RNN for better context understanding\n",
    "2. Create attention mechanism for RNN\n",
    "3. Implement residual connections in deep RNNs\n",
    "4. Add layer normalization\n",
    "5. Compare performance of different architectural improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement advanced RNN architectures\n",
    "class BidirectionalRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
    "        super(BidirectionalRNN, self).__init__()\n",
    "        # TODO: Implement bidirectional RNN\n",
    "        # Note: For generation, you'll need to use only forward direction\n",
    "        pass\n",
    "\n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        # TODO: Implement attention mechanism\n",
    "        pass\n",
    "    \n",
    "    def attention(self, hidden_states, current_hidden):\n",
    "        \"\"\"\n",
    "        Implement attention mechanism\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class ResidualRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=4):\n",
    "        super(ResidualRNN, self).__init__()\n",
    "        # TODO: Implement RNN with residual connections\n",
    "        pass\n",
    "\n",
    "class LayerNormRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2):\n",
    "        super(LayerNormRNN, self).__init__()\n",
    "        # TODO: Implement RNN with layer normalization\n",
    "        pass\n",
    "\n",
    "# TODO: Compare architectural improvements\n",
    "def compare_advanced_architectures():\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Multi-Style Text Generation\n",
    "\n",
    "Create a model that can generate text in different styles.\n",
    "\n",
    "**Tasks:**\n",
    "1. Collect texts from different authors/genres\n",
    "2. Implement style-conditional generation\n",
    "3. Create style embeddings\n",
    "4. Implement style transfer between different writing styles\n",
    "5. Evaluate style consistency and quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-style text generation\n",
    "class MultiStyleDataset:\n",
    "    def __init__(self, texts_by_style, sequence_length=100):\n",
    "        \"\"\"\n",
    "        Dataset that handles multiple text styles\n",
    "        texts_by_style: dict mapping style_name -> text_content\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def get_style_batch(self, style, batch_size=1):\n",
    "        \"\"\"\n",
    "        Get batch of sequences from specific style\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "class StyleConditionalRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                 num_styles, style_embedding_dim=50, num_layers=2):\n",
    "        super(StyleConditionalRNN, self).__init__()\n",
    "        # TODO: Implement style-conditional RNN\n",
    "        self.style_embedding = nn.Embedding(num_styles, style_embedding_dim)\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, style_ids, hidden=None):\n",
    "        # TODO: Incorporate style information\n",
    "        pass\n",
    "\n",
    "def collect_multi_style_data():\n",
    "    \"\"\"\n",
    "    Collect texts from different authors/styles\n",
    "    \"\"\"\n",
    "    # TODO: Implement data collection from multiple sources\n",
    "    # Consider: Shakespeare, Modern poetry, News articles, etc.\n",
    "    pass\n",
    "\n",
    "def generate_with_style(model, dataset, style, prime_str, length=200):\n",
    "    \"\"\"\n",
    "    Generate text in specific style\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def evaluate_style_consistency(model, dataset, style, num_samples=10):\n",
    "    \"\"\"\n",
    "    Evaluate how consistently the model generates in given style\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Interactive Text Generation\n",
    "\n",
    "Build an interactive text generation system with real-time feedback.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create interactive interface for text generation\n",
    "2. Implement real-time parameter adjustment (temperature, top-k, etc.)\n",
    "3. Add text completion and suggestion features\n",
    "4. Implement user feedback mechanism to improve generation\n",
    "5. Create visualization of model attention/decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement interactive text generation system\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class InteractiveTextGenerator:\n",
    "    def __init__(self, model, dataset):\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.generation_history = []\n",
    "        \n",
    "    def create_interface(self):\n",
    "        \"\"\"\n",
    "        Create interactive widgets for text generation\n",
    "        \"\"\"\n",
    "        # TODO: Create widgets for:\n",
    "        # - Text input box for prime string\n",
    "        # - Sliders for temperature, length, top-k\n",
    "        # - Buttons for generate, save, clear\n",
    "        # - Text area for output\n",
    "        pass\n",
    "    \n",
    "    def generate_interactive(self, prime_str, length, temperature, top_k):\n",
    "        \"\"\"\n",
    "        Generate text with interactive parameters\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def visualize_attention(self, text, attention_weights):\n",
    "        \"\"\"\n",
    "        Visualize attention weights for generated text\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def collect_feedback(self, generated_text, user_rating):\n",
    "        \"\"\"\n",
    "        Collect user feedback for improving generation\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "# TODO: Implement text completion system\n",
    "def implement_text_completion(model, dataset):\n",
    "    \"\"\"\n",
    "    Implement auto-completion suggestions\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Create interactive generator\n",
    "# interactive_gen = InteractiveTextGenerator(model, dataset)\n",
    "# interactive_gen.create_interface()\n",
    "print(\"Interactive text generator class defined. Uncomment to create interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Text Generation Evaluation Metrics\n",
    "\n",
    "Implement comprehensive evaluation metrics for generated text quality.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement BLEU score for text similarity\n",
    "2. Calculate perplexity and other language model metrics\n",
    "3. Implement semantic similarity measures\n",
    "4. Create readability and coherence metrics\n",
    "5. Develop style consistency measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement text evaluation metrics\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class TextEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def calculate_bleu_score(self, reference_text, generated_text, n=4):\n",
    "        \"\"\"\n",
    "        Calculate BLEU score for generated text\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def calculate_perplexity(self, model, dataset, text):\n",
    "        \"\"\"\n",
    "        Calculate perplexity of generated text\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def calculate_diversity_metrics(self, generated_texts):\n",
    "        \"\"\"\n",
    "        Calculate diversity metrics:\n",
    "        - Distinct-1, Distinct-2 (unique unigrams/bigrams)\n",
    "        - Entropy\n",
    "        - Self-BLEU (diversity between generated samples)\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def calculate_coherence_score(self, text):\n",
    "        \"\"\"\n",
    "        Calculate text coherence using various heuristics\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def calculate_readability_metrics(self, text):\n",
    "        \"\"\"\n",
    "        Calculate readability scores:\n",
    "        - Flesch Reading Ease\n",
    "        - Flesch-Kincaid Grade Level\n",
    "        - Average sentence length\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def evaluate_style_consistency(self, generated_texts, target_style):\n",
    "        \"\"\"\n",
    "        Evaluate consistency with target writing style\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def comprehensive_evaluation(self, model, dataset, generated_texts, reference_texts=None):\n",
    "        \"\"\"\n",
    "        Perform comprehensive evaluation of generated texts\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Calculate all metrics\n",
    "        # Your implementation here\n",
    "        \n",
    "        return results\n",
    "\n",
    "# TODO: Implement human evaluation interface\n",
    "def create_human_evaluation_interface(generated_texts):\n",
    "    \"\"\"\n",
    "    Create interface for human evaluation of generated texts\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Example usage\n",
    "evaluator = TextEvaluator()\n",
    "print(\"Text evaluator created. Implement methods to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: Transfer Learning and Fine-tuning\n",
    "\n",
    "Implement transfer learning for adapting pre-trained models to new domains.\n",
    "\n",
    "**Tasks:**\n",
    "1. Pre-train model on large general corpus\n",
    "2. Fine-tune on specific domain (poetry, technical writing, etc.)\n",
    "3. Implement domain adaptation techniques\n",
    "4. Compare transfer learning vs training from scratch\n",
    "5. Analyze what the model learns during fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement transfer learning for text generation\n",
    "class TransferLearningManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pretrain_on_general_corpus(self, model, general_dataset, epochs=1000):\n",
    "        \"\"\"\n",
    "        Pre-train model on large general text corpus\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def fine_tune_on_domain(self, pretrained_model, domain_dataset, \n",
    "                           epochs=200, freeze_layers=None):\n",
    "        \"\"\"\n",
    "        Fine-tune pre-trained model on specific domain\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        # Consider:\n",
    "        # - Freezing certain layers\n",
    "        # - Using lower learning rate\n",
    "        # - Gradual unfreezing\n",
    "        pass\n",
    "    \n",
    "    def domain_adaptation(self, source_model, target_dataset):\n",
    "        \"\"\"\n",
    "        Implement domain adaptation techniques\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def analyze_layer_representations(self, model, texts):\n",
    "        \"\"\"\n",
    "        Analyze what different layers learn during fine-tuning\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def compare_transfer_vs_scratch(self, domain_dataset):\n",
    "        \"\"\"\n",
    "        Compare transfer learning vs training from scratch\n",
    "        \"\"\"\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "def collect_domain_specific_data(domain='poetry'):\n",
    "    \"\"\"\n",
    "    Collect domain-specific text data\n",
    "    \"\"\"\n",
    "    # TODO: Implement data collection for specific domains\n",
    "    # Examples: poetry, technical documentation, news, etc.\n",
    "    pass\n",
    "\n",
    "# Create transfer learning manager\n",
    "transfer_manager = TransferLearningManager()\n",
    "print(\"Transfer learning manager created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 7: Analysis Questions\n",
    "\n",
    "Answer these questions based on your experiments:\n",
    "\n",
    "1. **Architecture Comparison**: How do different RNN architectures (RNN, LSTM, GRU) compare in terms of training speed, memory usage, and generation quality?\n",
    "\n",
    "2. **Character vs Word Level**: What are the trade-offs between character-level and word-level text generation? When would you choose one over the other?\n",
    "\n",
    "3. **Temperature Effects**: How does temperature affect the creativity vs coherence trade-off in text generation?\n",
    "\n",
    "4. **Sequence Length**: How does the training sequence length affect the model's ability to capture long-term dependencies?\n",
    "\n",
    "5. **Overfitting**: What signs of overfitting do you observe in text generation models, and how can they be mitigated?\n",
    "\n",
    "6. **Evaluation Challenges**: What are the main challenges in evaluating generated text quality, and how can they be addressed?\n",
    "\n",
    "7. **Real-world Applications**: What are the practical limitations of RNN-based text generation in real-world applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers:\n",
    "\n",
    "**1. Architecture Comparison:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**2. Character vs Word Level:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**3. Temperature Effects:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**4. Sequence Length:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**5. Overfitting:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**6. Evaluation Challenges:**\n",
    "<!-- Your analysis here -->\n",
    "\n",
    "**7. Real-world Applications:**\n",
    "<!-- Your analysis here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Modern Transformer Comparison\n",
    "\n",
    "Compare your RNN models with modern transformer-based approaches.\n",
    "\n",
    "**Tasks:**\n",
    "1. Implement a simple transformer for text generation\n",
    "2. Use pre-trained models (GPT-2, etc.) for comparison\n",
    "3. Analyze differences in generated text quality\n",
    "4. Compare computational requirements\n",
    "5. Discuss when RNNs might still be preferred over transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement transformer comparison\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers, max_seq_len=1000):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        # TODO: Implement simple transformer for text generation\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "\n",
    "def compare_rnn_vs_transformer(rnn_model, transformer_model, dataset):\n",
    "    \"\"\"\n",
    "    Compare RNN vs Transformer performance\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # Compare:\n",
    "    # - Training time\n",
    "    # - Memory usage\n",
    "    # - Generation quality\n",
    "    # - Long-term dependencies\n",
    "    pass\n",
    "\n",
    "def use_pretrained_gpt2(prompt, max_length=200):\n",
    "    \"\"\"\n",
    "    Use pre-trained GPT-2 for text generation comparison\n",
    "    \"\"\"\n",
    "    # TODO: Use transformers library to load GPT-2\n",
    "    # from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    pass\n",
    "\n",
    "print(\"Transformer comparison framework defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}