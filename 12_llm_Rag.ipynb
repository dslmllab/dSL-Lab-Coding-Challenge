{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dslmllab/dSL-Lab-Coding-Challenge/blob/main/12_llm_Rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBUSolhCuwp6"
      },
      "source": [
        "# Large Language Models (LLMs) Tutorial with Challenges\n",
        "\n",
        "## Table of Contents\n",
        "1. Introduction to LLMs\n",
        "2. Key Concepts and Architecture\n",
        "3. Working with Pre-trained Models\n",
        "4. Fine-tuning LLMs\n",
        "5. Prompt Engineering\n",
        "6. LLM Applications\n",
        "7. Challenges and Exercises\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7gshmUmuwp6"
      },
      "source": [
        "## 1. Introduction to LLMs\n",
        "\n",
        "Large Language Models (LLMs) are neural networks trained on vast amounts of text data to understand and generate human-like text. They have revolutionized NLP by achieving state-of-the-art performance on various tasks.\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Scale**: Billions of parameters (GPT-3: 175B, LLaMA: 7B-70B)\n",
        "- **Pre-training**: Trained on massive text corpora\n",
        "- **Transfer Learning**: Can be fine-tuned for specific tasks\n",
        "- **Few-shot Learning**: Can adapt to new tasks with minimal examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "stN6U9_huwp6",
        "outputId": "e764e571-cf44-4ca3-8d52-df2657c473bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch transformers numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KRtR1_TYuwp7",
        "outputId": "8c9a2863-089d-48b4-d4f9-6b1bd2abf718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    pipeline,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f36KCg6uwp7"
      },
      "source": [
        "## 2. Key Concepts and Architecture\n",
        "\n",
        "### Transformer Architecture\n",
        "LLMs are based on the Transformer architecture, which uses self-attention mechanisms to process sequential data.\n",
        "\n",
        "### Key Components:\n",
        "1. **Self-Attention**: Allows the model to focus on different parts of the input\n",
        "2. **Positional Encoding**: Provides position information to the model\n",
        "3. **Feed-Forward Networks**: Process the attention outputs\n",
        "4. **Layer Normalization**: Stabilizes training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WmfpiZS2uwp7",
        "outputId": "4bb82fc9-4cb5-449b-bdfe-22704e44bca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (    AutoTokenizer,     AutoModelForCausalLM,    pipeline,    GPT2LMHeadModel,    GPT2Tokenizer)\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if GPU is available\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Note: If you get import errors, install required packages:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngRRSZjguwp7"
      },
      "source": [
        "## 3. Working with Pre-trained Models\n",
        "\n",
        "Let's load and use a pre-trained language model from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iqLteNeFuwp7",
        "outputId": "db77de01-a211-4195-eddc-d10b6d7ac922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: gpt2\n",
            "Number of parameters: 124,439,808\n"
          ]
        }
      ],
      "source": [
        "# Load a small pre-trained model (GPT-2)\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B3ySV0sPuwp7",
        "outputId": "58b1fb7b-f329-4601-d6d6-c9433e297c60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The future of artificial intelligence is\n",
            "Generated: The future of artificial intelligence is still uncertain, but the ability to do anything we want to with it is certainly in the making. It's possible that this is the key to a truly advanced machine. We can't be sure, but it's always good to have something that can\n"
          ]
        }
      ],
      "source": [
        "# Text generation function\n",
        "def generate_text(prompt, max_length=100, temperature=0.8, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Generate text using the loaded model\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        max_length: Maximum length of generated text (including prompt)\n",
        "        temperature: Controls randomness (0.0 = deterministic, 1.0 = random)\n",
        "        top_p: Nucleus sampling parameter\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Calculate prompt length\n",
        "    prompt_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Ensure max_length is greater than prompt length\n",
        "    effective_max_length = max(max_length, prompt_length + 50)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,  # Use max_new_tokens instead of max_length\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode and return\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Test generation\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "generated = generate_text(prompt, max_length=100)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IilSu1uOuwp7"
      },
      "source": [
        "## 4. Fine-tuning LLMs\n",
        "\n",
        "Fine-tuning allows us to adapt pre-trained models to specific tasks or domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pNFAKEl7uwp8",
        "outputId": "717e2c9a-f9d3-47db-e7b7-67ff2f45ff20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 4\n",
            "Batch example shape: torch.Size([2, 128])\n"
          ]
        }
      ],
      "source": [
        "# Example: Preparing data for fine-tuning\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "# Sample training data\n",
        "training_texts = [\n",
        "    \"Machine learning is transforming industries.\",\n",
        "    \"Natural language processing enables computers to understand human language.\",\n",
        "    \"Deep learning models can learn complex patterns from data.\",\n",
        "    \"Transformers have revolutionized NLP tasks.\"\n",
        "]\n",
        "\n",
        "# Create dataset\n",
        "dataset = TextDataset(training_texts, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Batch example shape: {next(iter(dataloader))['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7aXzXsJKuwp8",
        "outputId": "7e8114c5-3318-4443-aa07-316f594042f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1 epoch...\n",
            "Average loss: 7.6350\n"
          ]
        }
      ],
      "source": [
        "# Simple fine-tuning loop (demonstration)\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def train_step(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Move to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train for one epoch (demonstration)\n",
        "print(\"Training for 1 epoch...\")\n",
        "avg_loss = train_step(model, dataloader, optimizer, device)\n",
        "print(f\"Average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96K036wwuwp8"
      },
      "source": [
        "## 5. Prompt Engineering\n",
        "\n",
        "Prompt engineering is the art of crafting inputs to get desired outputs from LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Fq6Sj5iAuwp8",
        "outputId": "7d238c49-74b1-4bfb-fb08-e64b410e23e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-shot prompt:\n",
            "Classify the sentiment of the following text\n",
            "\n",
            "Input: This movie was fantastic!\n",
            "Output: Positive\n",
            "\n",
            "Input: I really didn't enjoy the food.\n",
            "Output: Negative\n",
            "\n",
            "Input: The weather is okay today.\n",
            "Output: Neutral\n",
            "\n",
            "Input: The service was excellent and the staff were friendly.\n",
            "Output:\n",
            "\n",
            "Model output:\n",
            "Classify the sentiment of the following text\n",
            "\n",
            "Input: This movie was fantastic!\n",
            "Output: Positive\n",
            "\n",
            "Input: I really didn't enjoy the food.\n",
            "Output: Negative\n",
            "\n",
            "Input: The weather is okay today.\n",
            "Output: Neutral\n",
            "\n",
            "Input: The service was excellent and the staff were friendly.\n",
            "Output: Negative\n",
            "\n",
            "Input: The weather is okay today.\n",
            "Output: Positive\n",
            "\n",
            "Output: Negative\n",
            "\n",
            "\n",
            "Input: The weather is okay today.\n",
            "\n",
            "Output: Negative\n",
            "Output: Negative\n",
            "\n",
            "Output: Positive\n",
            "\n",
            "Output: Negative\n"
          ]
        }
      ],
      "source": [
        "# Prompt engineering examples\n",
        "class PromptTemplates:\n",
        "    @staticmethod\n",
        "    def zero_shot(task, input_text):\n",
        "        return f\"{task}: {input_text}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def few_shot(task, examples, input_text):\n",
        "        prompt = f\"{task}\\n\\n\"\n",
        "        for ex in examples:\n",
        "            prompt += f\"Input: {ex['input']}\\nOutput: {ex['output']}\\n\\n\"\n",
        "        prompt += f\"Input: {input_text}\\nOutput:\"\n",
        "        return prompt\n",
        "\n",
        "    @staticmethod\n",
        "    def chain_of_thought(question):\n",
        "        return f\"{question}\\n\\nLet's think step by step:\"\n",
        "\n",
        "# Example: Sentiment analysis with few-shot learning\n",
        "sentiment_examples = [\n",
        "    {\"input\": \"This movie was fantastic!\", \"output\": \"Positive\"},\n",
        "    {\"input\": \"I really didn't enjoy the food.\", \"output\": \"Negative\"},\n",
        "    {\"input\": \"The weather is okay today.\", \"output\": \"Neutral\"}\n",
        "]\n",
        "\n",
        "test_text = \"The service was excellent and the staff were friendly.\"\n",
        "prompt = PromptTemplates.few_shot(\n",
        "    \"Classify the sentiment of the following text\",\n",
        "    sentiment_examples,\n",
        "    test_text\n",
        ")\n",
        "\n",
        "print(\"Few-shot prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\nModel output:\")\n",
        "# Use max_new_tokens for better control\n",
        "print(generate_text(prompt, temperature=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UotxhAFZuwp8"
      },
      "source": [
        "## 6. LLM Applications\n",
        "\n",
        "Let's explore some practical applications of LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0VlmCBwVuwp8",
        "outputId": "c027dd2a-3844-4584-f7c0-dedd8d568c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "Artificial intelligence has made significant strides in recent years, \n",
            "particularly in the field of natural language processing. Large language \n",
            "models like GPT, BERT, and T5 have demonstrated remarkable capabilities \n",
            "in understanding and generating human-like text. These models are trained \n",
            "on vast amounts of data and can perform various tasks such as translation, \n",
            "summarization, and question answering without task-specific training.\n",
            "\n",
            "\n",
            "Summary:\n",
            "Summarize the following text in one sentence:\n",
            "\n",
            "\n",
            "Artificial intelligence has made significant strides in recent years, \n",
            "particularly in the field of natural language processing. Large language \n",
            "models like GPT, BERT, and T5 have demonstrated remarkable capabilities \n",
            "in understanding and generating human-like text. These models are trained \n",
            "on vast amounts of data and can perform various tasks such as translation, \n",
            "summarization, and question answering without task-specific training.\n",
            "\n",
            "\n",
            "Summary:\n",
            "\n",
            "\n",
            "This is a unique method of training. This method is \n",
            "the only method for creating a human-like \n"
          ]
        }
      ],
      "source": [
        "# Application 1: Text Summarization\n",
        "def summarize_text(text, model, tokenizer, max_summary_length=50):\n",
        "    prompt = f\"Summarize the following text in one sentence:\\n\\n{text}\\n\\nSummary:\"\n",
        "    return generate_text(prompt, max_length=len(prompt.split()) + max_summary_length)\n",
        "\n",
        "# Example\n",
        "long_text = \"\"\"\n",
        "Artificial intelligence has made significant strides in recent years,\n",
        "particularly in the field of natural language processing. Large language\n",
        "models like GPT, BERT, and T5 have demonstrated remarkable capabilities\n",
        "in understanding and generating human-like text. These models are trained\n",
        "on vast amounts of data and can perform various tasks such as translation,\n",
        "summarization, and question answering without task-specific training.\n",
        "\"\"\"\n",
        "\n",
        "summary = summarize_text(long_text, model, tokenizer)\n",
        "print(\"Original text:\")\n",
        "print(long_text)\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Yr_Ra68Huwp8",
        "outputId": "cc565799-d141-4909-d8f5-4849943d52ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated code:\n",
            "# Python function that calculates the factorial of a number recursively\n",
            "def sum(x,y) (x,y,y) = sum(x,y)\n",
            "The function is called with the following arguments:\n",
            "The first argument is the number of the number of arguments to the function. The second argument is\n"
          ]
        }
      ],
      "source": [
        "# Application 2: Code Generation\n",
        "def generate_code(description):\n",
        "    prompt = f\"# Python function that {description}\\ndef\"\n",
        "    return generate_text(prompt, max_length=150, temperature=0.2)\n",
        "\n",
        "# Example\n",
        "code_description = \"calculates the factorial of a number recursively\"\n",
        "generated_code = generate_code(code_description)\n",
        "print(\"Generated code:\")\n",
        "print(generated_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ragEN7u2uwp8",
        "outputId": "aec4b2d8-af8b-4b33-b4e7-0781e2593b7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: When was the Transformer architecture introduced?\n",
            "Answer: Context: The Transformer architecture was introduced in the paper 'Attention is All You Need' \n",
            "by Vaswani et al. in 2017. It revolutionized NLP by replacing recurrent layers with \n",
            "self-attention mechanisms, allowing for better parallelization and capturing long-range dependencies.\n",
            "\n",
            "Question: When was the Transformer architecture introduced?\n",
            "\n",
            "Answer: The Transformer architecture was introduced in the paper 'The Transformer Architecture'\n",
            "\n",
            "by Vaswani et al. in 2017. It revolutionized NLP by replacing recurrent layers with \n",
            "\n",
            "self-attention mechanisms.\n",
            "\n",
            "Question:\n"
          ]
        }
      ],
      "source": [
        "# Application 3: Question Answering\n",
        "def answer_question(context, question):\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return generate_text(prompt, max_length=len(prompt.split()) + 50, temperature=0.3)\n",
        "\n",
        "# Example\n",
        "context = \"\"\"The Transformer architecture was introduced in the paper 'Attention is All You Need'\n",
        "by Vaswani et al. in 2017. It revolutionized NLP by replacing recurrent layers with\n",
        "self-attention mechanisms, allowing for better parallelization and capturing long-range dependencies.\"\"\"\n",
        "\n",
        "question = \"When was the Transformer architecture introduced?\"\n",
        "answer = answer_question(context, question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCsEr1bSuwp8"
      },
      "source": [
        "## 7. Challenges and Exercises\n",
        "\n",
        "Now it's time to test your understanding with these challenges!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYziIzfUuwp8"
      },
      "source": [
        "### Challenge 1: Implement Temperature Sampling\n",
        "\n",
        "Implement a function that demonstrates how temperature affects text generation. Generate the same prompt with different temperature values and compare the outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RLBQxainuwp8"
      },
      "outputs": [],
      "source": [
        "# Challenge 1: Your code here\n",
        "def compare_temperatures(prompt, temperatures=[0.1, 0.5, 1.0, 1.5]):\n",
        "    \"\"\"\n",
        "    Generate text with different temperature values and compare outputs\n",
        "\n",
        "    TODO:\n",
        "    1. For each temperature value, generate text\n",
        "    2. Display the results side by side\n",
        "    3. Analyze how temperature affects creativity/randomness\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Test your function\n",
        "# compare_temperatures(\"The meaning of life is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS5YVHV-uwp8"
      },
      "source": [
        "### Challenge 2: Build a Custom Few-Shot Classifier\n",
        "\n",
        "Create a few-shot classifier for a custom task (e.g., classifying programming languages from code snippets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_MGqRRN4uwp8"
      },
      "outputs": [],
      "source": [
        "# Challenge 2: Your code here\n",
        "class FewShotClassifier:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.examples = []\n",
        "\n",
        "    def add_example(self, input_text, label):\n",
        "        \"\"\"\n",
        "        Add a training example for few-shot learning\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def classify(self, input_text):\n",
        "        \"\"\"\n",
        "        Classify the input text using few-shot learning\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "# Test your classifier\n",
        "# classifier = FewShotClassifier(model, tokenizer)\n",
        "# Add examples and test classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvd9imEuwp8"
      },
      "source": [
        "### Challenge 3: Implement Beam Search\n",
        "\n",
        "Implement beam search decoding for text generation and compare it with greedy decoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ERPUEBzRuwp8"
      },
      "outputs": [],
      "source": [
        "# Challenge 3: Your code here\n",
        "def beam_search_generate(model, tokenizer, prompt, beam_width=3, max_length=50):\n",
        "    \"\"\"\n",
        "    Implement beam search for text generation\n",
        "\n",
        "    TODO:\n",
        "    1. Tokenize the prompt\n",
        "    2. Maintain top-k sequences at each step\n",
        "    3. Expand each sequence and keep top-k overall\n",
        "    4. Return the best sequence\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Compare with greedy decoding\n",
        "# prompt = \"The future of technology\"\n",
        "# beam_output = beam_search_generate(model, tokenizer, prompt)\n",
        "# greedy_output = generate_text(prompt, temperature=0.0)  # Greedy when temp=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyP-lOlAuwp8"
      },
      "source": [
        "### Challenge 4: Prompt Optimization\n",
        "\n",
        "Design and test different prompt templates for a specific task (e.g., translation, style transfer) and evaluate which works best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rOPo5Bzwuwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 4: Your code here\n",
        "def evaluate_prompts(task_description, test_cases, prompt_templates):\n",
        "    \"\"\"\n",
        "    Evaluate different prompt templates for a task\n",
        "\n",
        "    TODO:\n",
        "    1. Design at least 3 different prompt templates\n",
        "    2. Test each template on the test cases\n",
        "    3. Implement a scoring mechanism\n",
        "    4. Return the best-performing template\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Example task: Style transfer (formal to casual)\n",
        "# test_cases = [\n",
        "#     \"I would like to request your assistance with this matter.\",\n",
        "#     \"Please find the attached document for your review.\"\n",
        "# ]\n",
        "# prompt_templates = [\n",
        "#     # Template 1, Template 2, Template 3...\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlpKrODouwp9"
      },
      "source": [
        "### Challenge 5: Build a Simple RAG System\n",
        "\n",
        "Implement a basic Retrieval-Augmented Generation (RAG) system that retrieves relevant context before generating answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "NpH6W-xvuwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 5: Your code here\n",
        "class SimpleRAG:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.knowledge_base = []\n",
        "\n",
        "    def add_document(self, document):\n",
        "        \"\"\"\n",
        "        Add a document to the knowledge base\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def retrieve(self, query, k=3):\n",
        "        \"\"\"\n",
        "        Retrieve top-k relevant documents for the query\n",
        "\n",
        "        TODO:\n",
        "        1. Implement a simple similarity metric (e.g., word overlap)\n",
        "        2. Return top-k most relevant documents\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def generate_answer(self, query):\n",
        "        \"\"\"\n",
        "        Generate an answer using retrieved context\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "# Test your RAG system\n",
        "# rag = SimpleRAG(model, tokenizer)\n",
        "# Add some documents and test question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDaksuRuwp9"
      },
      "source": [
        "### Challenge 6: Analyze Model Biases\n",
        "\n",
        "Design experiments to test for potential biases in the language model and propose mitigation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZXTN3n55uwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 6: Your code here\n",
        "def analyze_bias(model, tokenizer, bias_type='gender'):\n",
        "    \"\"\"\n",
        "    Analyze potential biases in model outputs\n",
        "\n",
        "    TODO:\n",
        "    1. Design test prompts that might reveal biases\n",
        "    2. Generate responses and analyze patterns\n",
        "    3. Quantify bias if possible\n",
        "    4. Suggest mitigation strategies\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Example test cases for gender bias\n",
        "# test_prompts = [\n",
        "#     \"The nurse said\",\n",
        "#     \"The engineer said\",\n",
        "#     \"The CEO decided to\",\n",
        "#     \"The secretary was\"\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTrLihy0uwp9"
      },
      "source": [
        "### Challenge 7: Implement Perplexity Calculation\n",
        "\n",
        "Calculate the perplexity of the model on a given text corpus to evaluate model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ranf50bSuwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 7: Your code here\n",
        "def calculate_perplexity(model, tokenizer, text_corpus):\n",
        "    \"\"\"\n",
        "    Calculate perplexity of the model on a text corpus\n",
        "\n",
        "    Perplexity = exp(average negative log-likelihood)\n",
        "\n",
        "    TODO:\n",
        "    1. Tokenize the text corpus\n",
        "    2. Calculate log probabilities for each token\n",
        "    3. Compute average negative log-likelihood\n",
        "    4. Return perplexity\n",
        "    \"\"\"\n",
        "    # Your implementation here\n",
        "    pass\n",
        "\n",
        "# Test on sample texts\n",
        "# sample_texts = [\n",
        "#     \"The quick brown fox jumps over the lazy dog.\",\n",
        "#     \"Machine learning is a subset of artificial intelligence.\",\n",
        "#     \"asdfjkl qwerty zxcvbn\"  # Random text for comparison\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmiQUK2uwp9"
      },
      "source": [
        "### Challenge 8: Create a Dialogue System\n",
        "\n",
        "Build a simple dialogue system that maintains context across multiple turns of conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "O64ske6suwp9"
      },
      "outputs": [],
      "source": [
        "# Challenge 8: Your code here\n",
        "class DialogueSystem:\n",
        "    def __init__(self, model, tokenizer, max_history=5):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_history = max_history\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def add_turn(self, speaker, text):\n",
        "        \"\"\"\n",
        "        Add a conversation turn to history\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def generate_response(self, user_input):\n",
        "        \"\"\"\n",
        "        Generate a response considering conversation history\n",
        "\n",
        "        TODO:\n",
        "        1. Format conversation history as context\n",
        "        2. Create appropriate prompt\n",
        "        3. Generate response\n",
        "        4. Update conversation history\n",
        "        \"\"\"\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def reset_conversation(self):\n",
        "        \"\"\"Reset conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "\n",
        "# Test the dialogue system\n",
        "# dialogue = DialogueSystem(model, tokenizer)\n",
        "# Simulate a multi-turn conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IccvyVFEuwp9"
      },
      "source": [
        "## Bonus Challenges\n",
        "\n",
        "### Advanced Challenge 1: Implement LoRA (Low-Rank Adaptation)\n",
        "Research and implement a simple version of LoRA for efficient fine-tuning.\n",
        "\n",
        "### Advanced Challenge 2: Build a Token Prediction Visualizer\n",
        "Create a visualization tool that shows the top-k predicted tokens at each generation step.\n",
        "\n",
        "### Advanced Challenge 3: Implement Constrained Generation\n",
        "Build a system that generates text with constraints (e.g., must include certain words, follow a specific pattern)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV08oyCMuwp9"
      },
      "source": [
        "## Resources for Further Learning\n",
        "\n",
        "1. **Papers to Read:**\n",
        "   - \"Attention Is All You Need\" (Vaswani et al., 2017)\n",
        "   - \"Language Models are Few-Shot Learners\" (GPT-3 paper)\n",
        "   - \"BERT: Pre-training of Deep Bidirectional Transformers\"\n",
        "\n",
        "2. **Useful Libraries:**\n",
        "   - Hugging Face Transformers\n",
        "   - LangChain for LLM applications\n",
        "   - OpenAI API for GPT models\n",
        "\n",
        "3. **Online Resources:**\n",
        "   - Hugging Face Course\n",
        "   - Fast.ai Practical Deep Learning\n",
        "   - The Illustrated Transformer\n",
        "\n",
        "4. **Practice Platforms:**\n",
        "   - Kaggle NLP competitions\n",
        "   - Hugging Face Model Hub\n",
        "   - Papers with Code"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}