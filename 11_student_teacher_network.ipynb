{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-Teacher Network for MNIST Classification\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "* Train a teacher network to classify the MNIST dataset\n",
    "* Understand knowledge distillation and how the student network learns from the teacher\n",
    "* Implement and compare different student architectures\n",
    "* Analyze the performance trade-offs between model size and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Knowledge Distillation\n",
    "\n",
    "Knowledge distillation is a technique where a smaller \"student\" model learns to mimic a larger \"teacher\" model. The student learns not just from the hard labels (correct classes) but also from the soft probabilities output by the teacher.\n",
    "\n",
    "### Why Knowledge Distillation?\n",
    "- **Model Compression**: Deploy smaller models on edge devices\n",
    "- **Faster Inference**: Reduced computation time\n",
    "- **Knowledge Transfer**: Student learns richer representations from teacher's outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn torch tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T02:14:25.418324Z",
     "start_time": "2025-07-02T02:14:25.379303Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Knowledge distillation hyperparameters\n",
    "temperature = 3.0  # Temperature for softening probability distributions\n",
    "alpha = 0.7       # Weight for distillation loss vs hard target loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Preparation\n",
    "\n",
    "MNIST dataset contains 60,000 training images and 10,000 test images of handwritten digits (0-9). Each image is 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare MNIST dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architectures\n",
    "\n",
    "### Teacher Network\n",
    "A larger, more complex network with multiple convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Teacher, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(7*7*32, 300)\n",
    "        self.fc2 = nn.Linear(300, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Calculate teacher model parameters\n",
    "teacher_model = Teacher()\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"Teacher Network Parameters: {teacher_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Network\n",
    "A smaller, simpler network that will learn from the teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc1 = nn.Linear(14*14*16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "\n",
    "# Calculate student model parameters\n",
    "student_model = Student()\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"Student Network Parameters: {student_params:,}\")\n",
    "print(f\"Compression Ratio: {teacher_params/student_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, model_name=\"Model\"):\n",
    "    \"\"\"Standard training function for any model\"\"\"\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'{model_name} - Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def test_model(model, test_loader, model_name=\"Model\"):\n",
    "    \"\"\"Test function for any model\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{model_name} - Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize teacher\n",
    "teacher = Teacher().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train teacher\n",
    "print(\"Training Teacher Network...\")\n",
    "teacher_losses, teacher_accs = train_model(teacher, train_loader, criterion, optimizer, num_epochs, \"Teacher\")\n",
    "\n",
    "# Test teacher\n",
    "teacher_accuracy = test_model(teacher, test_loader, \"Teacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Knowledge Distillation Training\n",
    "\n",
    "Now we'll train the student using knowledge distillation. The student learns from:\n",
    "1. **Hard targets**: The true labels (standard cross-entropy loss)\n",
    "2. **Soft targets**: The teacher's output probabilities (KL divergence loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_distillation(student, teacher, train_loader, num_epochs, temperature=3.0, alpha=0.7):\n",
    "    \"\"\"Train student using knowledge distillation\"\"\"\n",
    "    student.train()\n",
    "    teacher.eval()  # Teacher in evaluation mode\n",
    "    \n",
    "    # Freeze teacher parameters\n",
    "    for param in teacher.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    criterion_hard = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get teacher outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(images)\n",
    "            \n",
    "            # Get student outputs\n",
    "            student_outputs = student(images)\n",
    "            \n",
    "            # Calculate losses\n",
    "            # 1. Hard target loss (standard cross entropy)\n",
    "            loss_hard = criterion_hard(student_outputs, labels)\n",
    "            \n",
    "            # 2. Soft target loss (KL divergence)\n",
    "            T = temperature\n",
    "            loss_soft = nn.KLDivLoss(reduction='batchmean')(\n",
    "                F.log_softmax(student_outputs / T, dim=1),\n",
    "                F.softmax(teacher_outputs / T, dim=1)\n",
    "            ) * (T * T)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = alpha * loss_hard + (1 - alpha) * loss_soft\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(student_outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'acc': 100.*correct/total})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f'Student (Distilled) - Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize student for distillation\n",
    "student_distilled = Student().to(device)\n",
    "\n",
    "# Train student with knowledge distillation\n",
    "print(\"\\nTraining Student Network with Knowledge Distillation...\")\n",
    "distilled_losses, distilled_accs = train_student_distillation(\n",
    "    student_distilled, teacher, train_loader, num_epochs, temperature, alpha\n",
    ")\n",
    "\n",
    "# Test distilled student\n",
    "distilled_accuracy = test_model(student_distilled, test_loader, \"Student (Distilled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Student Without Distillation (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize student for baseline training\n",
    "student_baseline = Student().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(student_baseline.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train student without distillation\n",
    "print(\"\\nTraining Student Network WITHOUT Knowledge Distillation (Baseline)...\")\n",
    "baseline_losses, baseline_accs = train_model(\n",
    "    student_baseline, train_loader, criterion, optimizer, num_epochs, \"Student (Baseline)\"\n",
    ")\n",
    "\n",
    "# Test baseline student\n",
    "baseline_accuracy = test_model(student_baseline, test_loader, \"Student (Baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(teacher_losses, label='Teacher', linewidth=2)\n",
    "plt.plot(distilled_losses, label='Student (Distilled)', linewidth=2)\n",
    "plt.plot(baseline_losses, label='Student (Baseline)', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(teacher_accs, label='Teacher', linewidth=2)\n",
    "plt.plot(distilled_accs, label='Student (Distilled)', linewidth=2)\n",
    "plt.plot(baseline_accs, label='Student (Baseline)', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Bar plot of final test accuracies\n",
    "plt.subplot(1, 3, 3)\n",
    "models = ['Teacher', 'Student\\n(Distilled)', 'Student\\n(Baseline)']\n",
    "accuracies = [teacher_accuracy, distilled_accuracy, baseline_accuracy]\n",
    "colors = ['blue', 'green', 'orange']\n",
    "bars = plt.bar(models, accuracies, color=colors)\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Final Test Accuracy Comparison')\n",
    "plt.ylim(90, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{acc:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Teacher Parameters: {teacher_params:,}\")\n",
    "print(f\"Student Parameters: {student_params:,}\")\n",
    "print(f\"Compression Ratio: {teacher_params/student_params:.2f}x\")\n",
    "print(f\"\\nTest Accuracies:\")\n",
    "print(f\"  Teacher: {teacher_accuracy:.2f}%\")\n",
    "print(f\"  Student (Distilled): {distilled_accuracy:.2f}%\")\n",
    "print(f\"  Student (Baseline): {baseline_accuracy:.2f}%\")\n",
    "print(f\"\\nImprovement from Distillation: {distilled_accuracy - baseline_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(teacher, student_distilled, student_baseline, test_loader, num_samples=10):\n",
    "    \"\"\"Visualize predictions from all three models\"\"\"\n",
    "    teacher.eval()\n",
    "    student_distilled.eval()\n",
    "    student_baseline.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images = images[:num_samples].to(device)\n",
    "    labels = labels[:num_samples]\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(images)\n",
    "        distilled_outputs = student_distilled(images)\n",
    "        baseline_outputs = student_baseline(images)\n",
    "    \n",
    "    _, teacher_preds = torch.max(teacher_outputs, 1)\n",
    "    _, distilled_preds = torch.max(distilled_outputs, 1)\n",
    "    _, baseline_preds = torch.max(baseline_outputs, 1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            img = images[i].cpu().squeeze()\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            \n",
    "            # Create title with predictions\n",
    "            true_label = labels[i].item()\n",
    "            t_pred = teacher_preds[i].item()\n",
    "            d_pred = distilled_preds[i].item()\n",
    "            b_pred = baseline_preds[i].item()\n",
    "            \n",
    "            title = f'True: {true_label}\\n'\n",
    "            title += f'T: {t_pred} '\n",
    "            title += '✓' if t_pred == true_label else '✗'\n",
    "            title += f' | D: {d_pred} '\n",
    "            title += '✓' if d_pred == true_label else '✗'\n",
    "            title += f' | B: {b_pred} '\n",
    "            title += '✓' if b_pred == true_label else '✗'\n",
    "            \n",
    "            ax.set_title(title, fontsize=10)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions (T=Teacher, D=Distilled, B=Baseline)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(teacher, student_distilled, student_baseline, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Challenge Section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Implement a Different Student Architecture\n",
    "\n",
    "Design a new student architecture that:\n",
    "- Has even fewer parameters than the current student\n",
    "- Uses a different approach (e.g., only fully connected layers, or different conv architecture)\n",
    "- Achieves at least 95% accuracy with distillation\n",
    "\n",
    "**Hint**: You might want to experiment with:\n",
    "- Different kernel sizes\n",
    "- Depthwise separable convolutions\n",
    "- Different activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStudent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyStudent, self).__init__()\n",
    "        # TODO: Implement your tiny student architecture here\n",
    "        # Aim for < 20,000 parameters\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        pass\n",
    "\n",
    "# TODO: Train your TinyStudent with distillation and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Temperature Analysis\n",
    "\n",
    "Investigate how different temperature values affect the student's learning:\n",
    "1. Train students with temperatures T = [1, 3, 5, 10, 20]\n",
    "2. Plot the relationship between temperature and final accuracy\n",
    "3. Explain your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [1, 3, 5, 10, 20]\n",
    "temperature_results = []\n",
    "\n",
    "# TODO: For each temperature:\n",
    "# 1. Create a new student model\n",
    "# 2. Train it with that temperature\n",
    "# 3. Test it and store the accuracy\n",
    "\n",
    "# TODO: Create a plot showing temperature vs accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Multi-Teacher Distillation\n",
    "\n",
    "Implement a system where a student learns from multiple teachers:\n",
    "1. Train 3 different teacher architectures\n",
    "2. Combine their knowledge to train a single student\n",
    "3. Compare with single-teacher distillation\n",
    "\n",
    "**Hint**: You can average the soft targets from multiple teachers or use a weighted combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define 3 different teacher architectures\n",
    "class Teacher1(nn.Module):\n",
    "    # Different architecture\n",
    "    pass\n",
    "\n",
    "class Teacher2(nn.Module):\n",
    "    # Different architecture\n",
    "    pass\n",
    "\n",
    "class Teacher3(nn.Module):\n",
    "    # Different architecture\n",
    "    pass\n",
    "\n",
    "# TODO: Implement multi-teacher distillation training function\n",
    "def train_student_multi_teacher(student, teachers, train_loader, num_epochs):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Distillation for Other Datasets\n",
    "\n",
    "Apply knowledge distillation to a more complex dataset:\n",
    "1. Use CIFAR-10 dataset (color images, 10 classes)\n",
    "2. Design appropriate teacher and student architectures\n",
    "3. Compare the effectiveness of distillation on CIFAR-10 vs MNIST\n",
    "\n",
    "**Questions to answer**:\n",
    "- Is the improvement from distillation more or less pronounced on CIFAR-10?\n",
    "- What architectural choices work best for color images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load CIFAR-10 dataset\n",
    "# TODO: Design teacher and student architectures for CIFAR-10\n",
    "# TODO: Train and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Analysis Questions\n",
    "\n",
    "Answer the following questions based on your experiments:\n",
    "\n",
    "1. **Why does knowledge distillation work?** Explain in your own words why learning from soft targets helps the student network.\n",
    "\n",
    "2. **When might distillation fail?** Can you think of scenarios where knowledge distillation might not help or even hurt performance?\n",
    "\n",
    "3. **Real-world applications**: List 3 real-world scenarios where knowledge distillation would be particularly useful.\n",
    "\n",
    "4. **Optimal α value**: Based on your experiments, what seems to be the optimal balance between hard and soft targets? Does this depend on the dataset or architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here:\n",
    "# 1. Why does knowledge distillation work?\n",
    "# Answer: \n",
    "\n",
    "# 2. When might distillation fail?\n",
    "# Answer: \n",
    "\n",
    "# 3. Real-world applications:\n",
    "# Answer: \n",
    "\n",
    "# 4. Optimal α value:\n",
    "# Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Implement Progressive Distillation\n",
    "\n",
    "Implement a progressive distillation approach where:\n",
    "1. Train a large teacher\n",
    "2. Train a medium-sized student from the teacher\n",
    "3. Train a tiny student from the medium student\n",
    "4. Compare this \"chain\" approach with direct distillation from teacher to tiny student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement progressive distillation\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}