{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification and Document Categorization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the fundamentals of text classification\n",
    "2. Implement various feature extraction techniques\n",
    "3. Build and compare different classification models\n",
    "4. Handle multi-class and multi-label classification\n",
    "5. Work with imbalanced datasets\n",
    "6. Implement hierarchical text classification\n",
    "7. Build neural network classifiers for text\n",
    "8. Deploy text classification systems in production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Text Classification\n",
    "\n",
    "Text Classification is the task of assigning predefined categories or labels to text documents. It's one of the most fundamental and widely-used NLP tasks with applications across various domains.\n",
    "\n",
    "### Types of Text Classification:\n",
    "\n",
    "1. **Binary Classification**: Two classes (spam/not spam, positive/negative)\n",
    "2. **Multi-class Classification**: Multiple mutually exclusive classes (topic classification)\n",
    "3. **Multi-label Classification**: Multiple non-exclusive labels (tag assignment)\n",
    "4. **Hierarchical Classification**: Classes organized in a hierarchy\n",
    "\n",
    "### Common Applications:\n",
    "\n",
    "- **Spam Detection**: Email spam filtering\n",
    "- **Topic Classification**: News article categorization\n",
    "- **Intent Classification**: Chatbot intent recognition\n",
    "- **Document Classification**: Legal document categorization\n",
    "- **Product Categorization**: E-commerce product classification\n",
    "- **Content Moderation**: Social media content filtering\n",
    "\n",
    "### Typical Pipeline:\n",
    "\n",
    "1. **Data Collection & Preprocessing**: Clean and prepare text data\n",
    "2. **Feature Extraction**: Convert text to numerical features\n",
    "3. **Model Training**: Train classification algorithms\n",
    "4. **Evaluation**: Assess model performance\n",
    "5. **Deployment**: Deploy model for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn nltk scikit-learn torch tqdm textstat imbalanced-learn",
    "",
    "# Download required NLTK data",
    "import nltk",
    "for item in ['punkt', 'stopwords', 'wordnet', 'omw-1.4']:",
    "    nltk.download(item, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# Deep Learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing for Classification\n",
    "\n",
    "Comprehensive text preprocessing pipeline specifically designed for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, remove_stopwords=True, lemmatize=True, min_length=2):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.min_length = min_length\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        if lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_process(self, text):\n",
    "        \"\"\"Tokenize and apply processing steps\"\"\"\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove short tokens\n",
    "        tokens = [token for token in tokens if len(token) >= self.min_length]\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if self.remove_stopwords:\n",
    "            tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        if self.lemmatize:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Clean text\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize and process\n",
    "        tokens = self.tokenize_and_process(cleaned_text)\n",
    "        \n",
    "        # Join back to string\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess_corpus(self, texts):\n",
    "        \"\"\"Preprocess a list of texts\"\"\"\n",
    "        return [self.preprocess(text) for text in texts]\n",
    "\n",
    "# Test the preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "sample_texts = [\n",
    "    \"This is a GREAT product! I love it so much!!! üòç\",\n",
    "    \"Visit our website at https://example.com for more details.\",\n",
    "    \"Contact us at support@company.com or call (555) 123-4567.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "print(\"Text Preprocessing Examples:\")\n",
    "print(\"=\" * 50)\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    print(f\"Original {i}: {text}\")\n",
    "    print(f\"Processed {i}: {processed}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction Techniques\n",
    "\n",
    "Implementing various methods to convert text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.vectorizers = {\n",
    "            'count': CountVectorizer(max_features=5000, ngram_range=(1, 2)),\n",
    "            'tfidf': TfidfVectorizer(max_features=5000, ngram_range=(1, 2)),\n",
    "            'binary': CountVectorizer(max_features=5000, binary=True),\n",
    "            'char_ngram': TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=5000)\n",
    "        }\n",
    "        self.fitted_vectorizers = {}\n",
    "    \n",
    "    def extract_statistical_features(self, texts):\n",
    "        \"\"\"Extract statistical features from texts\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            chars = list(text)\n",
    "            \n",
    "            feature_dict = {\n",
    "                'word_count': len(words),\n",
    "                'char_count': len(chars),\n",
    "                'avg_word_length': np.mean([len(word) for word in words]) if words else 0,\n",
    "                'sentence_count': len(re.split(r'[.!?]+', text)),\n",
    "                'exclamation_count': text.count('!'),\n",
    "                'question_count': text.count('?'),\n",
    "                'uppercase_ratio': sum(1 for c in chars if c.isupper()) / len(chars) if chars else 0,\n",
    "                'digit_ratio': sum(1 for c in chars if c.isdigit()) / len(chars) if chars else 0,\n",
    "                'unique_word_ratio': len(set(words)) / len(words) if words else 0\n",
    "            }\n",
    "            \n",
    "            features.append(list(feature_dict.values()))\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def fit_transform(self, texts, method='tfidf'):\n",
    "        \"\"\"Fit vectorizer and transform texts\"\"\"\n",
    "        if method not in self.vectorizers:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        vectorizer = self.vectorizers[method]\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        self.fitted_vectorizers[method] = vectorizer\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def transform(self, texts, method='tfidf'):\n",
    "        \"\"\"Transform texts using fitted vectorizer\"\"\"\n",
    "        if method not in self.fitted_vectorizers:\n",
    "            raise ValueError(f\"Vectorizer for method '{method}' not fitted yet\")\n",
    "        \n",
    "        vectorizer = self.fitted_vectorizers[method]\n",
    "        return vectorizer.transform(texts)\n",
    "    \n",
    "    def get_feature_names(self, method='tfidf'):\n",
    "        \"\"\"Get feature names from fitted vectorizer\"\"\"\n",
    "        if method not in self.fitted_vectorizers:\n",
    "            return None\n",
    "        \n",
    "        vectorizer = self.fitted_vectorizers[method]\n",
    "        if hasattr(vectorizer, 'get_feature_names_out'):\n",
    "            return vectorizer.get_feature_names_out()\n",
    "        return None\n",
    "    \n",
    "    def compare_vectorization_methods(self, texts, labels=None):\n",
    "        \"\"\"Compare different vectorization methods\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for method in self.vectorizers.keys():\n",
    "            print(f\"Extracting features using {method}...\")\n",
    "            \n",
    "            # Fit and transform\n",
    "            X = self.fit_transform(texts, method)\n",
    "            \n",
    "            results[method] = {\n",
    "                'shape': X.shape,\n",
    "                'sparsity': 1.0 - (X.nnz / (X.shape[0] * X.shape[1])),\n",
    "                'features': X\n",
    "            }\n",
    "            \n",
    "            print(f\"  Shape: {X.shape}\")\n",
    "            print(f\"  Sparsity: {results[method]['sparsity']:.3f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create sample dataset for demonstration\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample text classification dataset\"\"\"\n",
    "    \n",
    "    # Technology articles\n",
    "    tech_texts = [\n",
    "        \"Artificial intelligence and machine learning are revolutionizing industries.\",\n",
    "        \"The new smartphone features advanced camera technology and 5G connectivity.\",\n",
    "        \"Cloud computing enables scalable and flexible IT infrastructure.\",\n",
    "        \"Blockchain technology offers secure and transparent transactions.\",\n",
    "        \"Internet of Things devices are connecting everyday objects to the web.\",\n",
    "        \"Quantum computing promises to solve complex computational problems.\",\n",
    "        \"Cybersecurity measures are essential for protecting digital assets.\",\n",
    "        \"Virtual reality creates immersive digital experiences.\"\n",
    "    ]\n",
    "    \n",
    "    # Sports articles\n",
    "    sports_texts = [\n",
    "        \"The basketball team won the championship with an incredible performance.\",\n",
    "        \"Soccer players train rigorously to improve their skills and fitness.\",\n",
    "        \"Tennis tournaments attract millions of viewers worldwide.\",\n",
    "        \"Olympic athletes dedicate years to perfecting their techniques.\",\n",
    "        \"Baseball season brings excitement to fans across the country.\",\n",
    "        \"Swimming competitions showcase athletic prowess and endurance.\",\n",
    "        \"Football teams strategize to outplay their opponents.\",\n",
    "        \"Marathon runners push their limits in endurance challenges.\"\n",
    "    ]\n",
    "    \n",
    "    # Health articles\n",
    "    health_texts = [\n",
    "        \"Regular exercise and balanced nutrition promote overall wellness.\",\n",
    "        \"Medical research leads to breakthrough treatments for diseases.\",\n",
    "        \"Mental health awareness is crucial for psychological wellbeing.\",\n",
    "        \"Preventive healthcare helps detect issues before they become serious.\",\n",
    "        \"Vaccination programs protect communities from infectious diseases.\",\n",
    "        \"Healthy lifestyle choices reduce the risk of chronic conditions.\",\n",
    "        \"Healthcare professionals provide essential medical services.\",\n",
    "        \"Pharmaceutical innovations improve treatment outcomes.\"\n",
    "    ]\n",
    "    \n",
    "    # Combine all texts and labels\n",
    "    all_texts = tech_texts + sports_texts + health_texts\n",
    "    all_labels = ['technology'] * len(tech_texts) + ['sports'] * len(sports_texts) + ['health'] * len(health_texts)\n",
    "    \n",
    "    return all_texts, all_labels\n",
    "\n",
    "# Create dataset and preprocess\n",
    "texts, labels = create_sample_dataset()\n",
    "processed_texts = preprocessor.preprocess_corpus(texts)\n",
    "\n",
    "print(f\"Dataset created with {len(texts)} samples\")\n",
    "print(f\"Categories: {set(labels)}\")\n",
    "print(f\"Category distribution: {Counter(labels)}\")\n",
    "\n",
    "# Extract features using different methods\n",
    "feature_extractor = FeatureExtractor()\n",
    "print(\"\\nComparing vectorization methods:\")\n",
    "print(\"=\" * 40)\n",
    "vectorization_results = feature_extractor.compare_vectorization_methods(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classical Machine Learning Classifiers\n",
    "\n",
    "Implementing and comparing various traditional ML algorithms for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel:\n",
    "    def __init__(self):\n",
    "        self.models = {\n",
    "            'naive_bayes': MultinomialNB(),\n",
    "            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'svm_linear': LinearSVC(random_state=42, max_iter=1000),\n",
    "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'gradient_boosting': GradientBoostingClassifier(random_state=42)\n",
    "        }\n",
    "        self.trained_models = {}\n",
    "        self.vectorizer = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "    \n",
    "    def prepare_data(self, texts, labels, test_size=0.2, vectorization_method='tfidf'):\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Encode labels\n",
    "        encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "            texts, encoded_labels, test_size=test_size, random_state=42, stratify=encoded_labels\n",
    "        )\n",
    "        \n",
    "        # Vectorize text\n",
    "        if vectorization_method == 'tfidf':\n",
    "            self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        elif vectorization_method == 'count':\n",
    "            self.vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported vectorization method\")\n",
    "        \n",
    "        X_train = self.vectorizer.fit_transform(X_train_text)\n",
    "        X_test = self.vectorizer.transform(X_test_text)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, X_train_text, X_test_text\n",
    "    \n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"Train all models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            self.trained_models[name] = model\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='accuracy')\n",
    "            \n",
    "            results[name] = {\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "            print(f\"  CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"Evaluate trained models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.trained_models.items():\n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict(self, texts, model_name='logistic_regression'):\n",
    "        \"\"\"Predict labels for new texts\"\"\"\n",
    "        if model_name not in self.trained_models:\n",
    "            raise ValueError(f\"Model '{model_name}' not trained\")\n",
    "        \n",
    "        # Vectorize texts\n",
    "        X = self.vectorizer.transform(texts)\n",
    "        \n",
    "        # Predict\n",
    "        model = self.trained_models[model_name]\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        # Decode labels\n",
    "        predicted_labels = self.label_encoder.inverse_transform(predictions)\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probabilities = model.predict_proba(X)\n",
    "            return predicted_labels, probabilities\n",
    "        \n",
    "        return predicted_labels, None\n",
    "    \n",
    "    def get_feature_importance(self, model_name='logistic_regression', top_n=10):\n",
    "        \"\"\"Get most important features for classification\"\"\"\n",
    "        if model_name not in self.trained_models:\n",
    "            return None\n",
    "        \n",
    "        model = self.trained_models[model_name]\n",
    "        feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        if hasattr(model, 'coef_'):\n",
    "            # For linear models\n",
    "            if len(model.classes_) == 2:\n",
    "                # Binary classification\n",
    "                coef = model.coef_[0]\n",
    "                top_positive = np.argsort(coef)[-top_n:]\n",
    "                top_negative = np.argsort(coef)[:top_n]\n",
    "                \n",
    "                return {\n",
    "                    'positive': [(feature_names[i], coef[i]) for i in reversed(top_positive)],\n",
    "                    'negative': [(feature_names[i], coef[i]) for i in top_negative]\n",
    "                }\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                results = {}\n",
    "                for i, class_name in enumerate(self.label_encoder.classes_):\n",
    "                    coef = model.coef_[i]\n",
    "                    top_indices = np.argsort(coef)[-top_n:]\n",
    "                    results[class_name] = [(feature_names[j], coef[j]) for j in reversed(top_indices)]\n",
    "                return results\n",
    "        \n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            # For tree-based models\n",
    "            importances = model.feature_importances_\n",
    "            top_indices = np.argsort(importances)[-top_n:]\n",
    "            return [(feature_names[i], importances[i]) for i in reversed(top_indices)]\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Initialize and train models\n",
    "classifier = TextClassificationModel()\n",
    "\n",
    "print(\"Preparing data for classification...\")\n",
    "X_train, X_test, y_train, y_test, X_train_text, X_test_text = classifier.prepare_data(\n",
    "    processed_texts, labels, test_size=0.3\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "print(\"\\nTraining models...\")\n",
    "print(\"=\" * 30)\n",
    "training_results = classifier.train_models(X_train, y_train)\n",
    "\n",
    "print(\"\\nEvaluating models...\")\n",
    "print(\"=\" * 30)\n",
    "evaluation_results = classifier.evaluate_models(X_test, y_test)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    print(f\"{model_name:<20} {results['accuracy']:<10.3f} {results['precision']:<10.3f} \"\n",
    "          f\"{results['recall']:<10.3f} {results['f1']:<10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Analysis and Interpretation\n",
    "\n",
    "Analyzing model performance and understanding what features drive classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance\n",
    "def plot_model_comparison(evaluation_results):\n",
    "    \"\"\"Plot model performance comparison\"\"\"\n",
    "    models = list(evaluation_results.keys())\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        scores = [evaluation_results[model][metric] for model in models]\n",
    "        \n",
    "        bars = axes[i].bar(models, scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'lightpink'])\n",
    "        axes[i].set_title(f'{metric.capitalize()} Comparison')\n",
    "        axes[i].set_ylabel(metric.capitalize())\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, score in zip(bars, scores):\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion Matrix'):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "plot_model_comparison(evaluation_results)\n",
    "\n",
    "# Plot confusion matrix for best model\n",
    "best_model = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['f1'])\n",
    "print(f\"\\nBest performing model: {best_model}\")\n",
    "print(f\"F1-Score: {evaluation_results[best_model]['f1']:.3f}\")\n",
    "\n",
    "y_pred_best = evaluation_results[best_model]['predictions']\n",
    "class_labels = classifier.label_encoder.classes_\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_best, class_labels, \n",
    "                     f'Confusion Matrix - {best_model}')\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nTop Features for Classification:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "feature_importance = classifier.get_feature_importance(best_model, top_n=5)\n",
    "if feature_importance:\n",
    "    if isinstance(feature_importance, dict) and 'positive' not in feature_importance:\n",
    "        # Multi-class classification\n",
    "        for class_name, features in feature_importance.items():\n",
    "            print(f\"\\n{class_name.upper()} class:\")\n",
    "            for feature, weight in features:\n",
    "                print(f\"  {feature}: {weight:.3f}\")\n",
    "    else:\n",
    "        # Binary classification or tree-based\n",
    "        print(feature_importance)\n",
    "\n",
    "# Test predictions on new examples\n",
    "test_examples = [\n",
    "    \"Machine learning algorithms process large datasets efficiently.\",\n",
    "    \"The football team scored a winning goal in overtime.\",\n",
    "    \"Regular checkups help maintain good health and prevent diseases.\"\n",
    "]\n",
    "\n",
    "print(\"\\nPredictions on new examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "processed_examples = preprocessor.preprocess_corpus(test_examples)\n",
    "predictions, probabilities = classifier.predict(processed_examples, best_model)\n",
    "\n",
    "for i, (original, prediction) in enumerate(zip(test_examples, predictions)):\n",
    "    print(f\"Text: {original}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    if probabilities is not None:\n",
    "        prob_dict = dict(zip(class_labels, probabilities[i]))\n",
    "        print(f\"Probabilities: {prob_dict}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-label Classification\n",
    "\n",
    "Handling cases where documents can belong to multiple categories simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTextClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "        self.label_binarizer = MultiLabelBinarizer()\n",
    "        self.models = {\n",
    "            'binary_relevance': OneVsRestClassifier(LogisticRegression(random_state=42)),\n",
    "            'classifier_chains': MultiOutputClassifier(LogisticRegression(random_state=42))\n",
    "        }\n",
    "        self.trained_models = {}\n",
    "    \n",
    "    def create_multilabel_dataset(self):\n",
    "        \"\"\"Create a sample multi-label dataset\"\"\"\n",
    "        texts = [\n",
    "            \"Artificial intelligence in healthcare improves diagnostic accuracy\",\n",
    "            \"Sports technology enhances athlete performance monitoring\",\n",
    "            \"Machine learning algorithms predict health outcomes\",\n",
    "            \"Wearable devices track fitness and health metrics\",\n",
    "            \"AI-powered sports analytics revolutionize team strategies\",\n",
    "            \"Telemedicine platforms provide remote healthcare services\",\n",
    "            \"Smart sports equipment collects performance data\",\n",
    "            \"Blockchain technology secures medical records\",\n",
    "            \"Virtual reality training simulates sports scenarios\",\n",
    "            \"IoT devices monitor patient health in real-time\",\n",
    "            \"Computer vision analyzes sports movements\",\n",
    "            \"Digital health apps promote wellness and fitness\"\n",
    "        ]\n",
    "        \n",
    "        # Multi-label assignments (documents can have multiple labels)\n",
    "        labels = [\n",
    "            ['technology', 'health'],\n",
    "            ['technology', 'sports'],\n",
    "            ['technology', 'health'],\n",
    "            ['technology', 'health', 'sports'],\n",
    "            ['technology', 'sports'],\n",
    "            ['technology', 'health'],\n",
    "            ['technology', 'sports'],\n",
    "            ['technology', 'health'],\n",
    "            ['technology', 'sports'],\n",
    "            ['technology', 'health'],\n",
    "            ['technology', 'sports'],\n",
    "            ['technology', 'health', 'sports']\n",
    "        ]\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    def prepare_multilabel_data(self, texts, labels, test_size=0.3):\n",
    "        \"\"\"Prepare multi-label data for training\"\"\"\n",
    "        # Preprocess texts\n",
    "        processed_texts = preprocessor.preprocess_corpus(texts)\n",
    "        \n",
    "        # Binarize labels\n",
    "        y_binary = self.label_binarizer.fit_transform(labels)\n",
    "        \n",
    "        # Split data\n",
    "        X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "            processed_texts, y_binary, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Vectorize\n",
    "        X_train = self.vectorizer.fit_transform(X_train_text)\n",
    "        X_test = self.vectorizer.transform(X_test_text)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, X_train_text, X_test_text\n",
    "    \n",
    "    def train_multilabel_models(self, X_train, y_train):\n",
    "        \"\"\"Train multi-label classification models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name} multi-label classifier...\")\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            self.trained_models[name] = model\n",
    "            \n",
    "            # Evaluate on training data (for demonstration)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_train, y_pred_train)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_train, y_pred_train, average='micro'\n",
    "            )\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            }\n",
    "            \n",
    "            print(f\"  Training Accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_multilabel_models(self, X_test, y_test):\n",
    "        \"\"\"Evaluate multi-label models\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in self.trained_models.items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_test, y_pred, average='micro'\n",
    "            )\n",
    "            \n",
    "            # Per-label metrics\n",
    "            label_names = self.label_binarizer.classes_\n",
    "            per_label_metrics = {}\n",
    "            \n",
    "            for i, label in enumerate(label_names):\n",
    "                label_precision, label_recall, label_f1, _ = precision_recall_fscore_support(\n",
    "                    y_test[:, i], y_pred[:, i], average='binary'\n",
    "                )\n",
    "                per_label_metrics[label] = {\n",
    "                    'precision': label_precision,\n",
    "                    'recall': label_recall,\n",
    "                    'f1': label_f1\n",
    "                }\n",
    "            \n",
    "            results[name] = {\n",
    "                'overall_accuracy': accuracy,\n",
    "                'overall_precision': precision,\n",
    "                'overall_recall': recall,\n",
    "                'overall_f1': f1,\n",
    "                'per_label': per_label_metrics,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_multilabel(self, texts, model_name='binary_relevance'):\n",
    "        \"\"\"Predict multiple labels for texts\"\"\"\n",
    "        if model_name not in self.trained_models:\n",
    "            raise ValueError(f\"Model '{model_name}' not trained\")\n",
    "        \n",
    "        # Preprocess and vectorize\n",
    "        processed_texts = preprocessor.preprocess_corpus(texts)\n",
    "        X = self.vectorizer.transform(processed_texts)\n",
    "        \n",
    "        # Predict\n",
    "        model = self.trained_models[model_name]\n",
    "        y_pred_binary = model.predict(X)\n",
    "        \n",
    "        # Convert back to label format\n",
    "        predicted_labels = self.label_binarizer.inverse_transform(y_pred_binary)\n",
    "        \n",
    "        return predicted_labels, y_pred_binary\n",
    "\n",
    "# Initialize multi-label classifier\n",
    "multilabel_classifier = MultiLabelTextClassifier()\n",
    "\n",
    "# Create and prepare multi-label dataset\n",
    "ml_texts, ml_labels = multilabel_classifier.create_multilabel_dataset()\n",
    "\n",
    "print(\"Multi-label Dataset:\")\n",
    "print(f\"Number of samples: {len(ml_texts)}\")\n",
    "print(f\"Available labels: {set().union(*ml_labels)}\")\n",
    "\n",
    "# Count label combinations\n",
    "label_combinations = Counter([tuple(sorted(labels)) for labels in ml_labels])\n",
    "print(f\"Label combinations: {dict(label_combinations)}\")\n",
    "\n",
    "# Prepare data\n",
    "X_train_ml, X_test_ml, y_train_ml, y_test_ml, _, _ = multilabel_classifier.prepare_multilabel_data(\n",
    "    ml_texts, ml_labels, test_size=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train_ml.shape}\")\n",
    "print(f\"Test set shape: {X_test_ml.shape}\")\n",
    "print(f\"Label matrix shape: {y_train_ml.shape}\")\n",
    "\n",
    "# Train models\n",
    "print(\"\\nTraining multi-label models...\")\n",
    "print(\"=\" * 40)\n",
    "ml_training_results = multilabel_classifier.train_multilabel_models(X_train_ml, y_train_ml)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\nEvaluating multi-label models...\")\n",
    "print(\"=\" * 40)\n",
    "ml_evaluation_results = multilabel_classifier.evaluate_multilabel_models(X_test_ml, y_test_ml)\n",
    "\n",
    "# Display results\n",
    "for model_name, results in ml_evaluation_results.items():\n",
    "    print(f\"\\n{model_name.upper()} Results:\")\n",
    "    print(f\"  Overall Accuracy: {results['overall_accuracy']:.3f}\")\n",
    "    print(f\"  Overall F1-Score: {results['overall_f1']:.3f}\")\n",
    "    \n",
    "    print(\"  Per-label metrics:\")\n",
    "    for label, metrics in results['per_label'].items():\n",
    "        print(f\"    {label}: F1={metrics['f1']:.3f}, Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}\")\n",
    "\n",
    "# Test predictions\n",
    "test_ml_examples = [\n",
    "    \"AI algorithms analyze athletic performance data\",\n",
    "    \"Wearable fitness trackers monitor heart rate during exercise\",\n",
    "    \"Blockchain ensures secure storage of patient medical records\"\n",
    "]\n",
    "\n",
    "print(\"\\nMulti-label Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ml_predictions, _ = multilabel_classifier.predict_multilabel(test_ml_examples)\n",
    "\n",
    "for text, labels in zip(test_ml_examples, ml_predictions):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted labels: {list(labels) if labels else 'None'}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Network Text Classification\n",
    "\n",
    "Implementing deep learning approaches for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices\n",
    "        tokens = text.split()\n",
    "        indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(indices) < self.max_length:\n",
    "            indices.extend([self.vocab['<PAD>']] * (self.max_length - len(indices)))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "        \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, filter_sizes, num_classes, dropout=0.5):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embedding_dim)\n",
    "        embedded = embedded.transpose(1, 2)  # (batch_size, embedding_dim, seq_length)\n",
    "        \n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = torch.relu(conv(embedded))  # (batch_size, num_filters, conv_length)\n",
    "            pooled = torch.max_pool1d(conv_out, conv_out.size(2))  # (batch_size, num_filters, 1)\n",
    "            conv_outputs.append(pooled.squeeze(2))  # (batch_size, num_filters)\n",
    "        \n",
    "        concatenated = torch.cat(conv_outputs, dim=1)  # (batch_size, len(filter_sizes) * num_filters)\n",
    "        dropped = self.dropout(concatenated)\n",
    "        output = self.fc(dropped)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class NeuralTextClassifier:\n",
    "    def __init__(self, embedding_dim=128, num_filters=100, filter_sizes=[3, 4, 5]):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.vocab = None\n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def build_vocabulary(self, texts, min_freq=2):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            word_freq.update(words)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "        idx = 2\n",
    "        for word, freq in word_freq.items():\n",
    "            if freq >= min_freq:\n",
    "                vocab[word] = idx\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        return vocab\n",
    "    \n",
    "    def prepare_neural_data(self, texts, labels, test_size=0.2, max_length=50):\n",
    "        \"\"\"Prepare data for neural network training\"\"\"\n",
    "        # Preprocess texts\n",
    "        processed_texts = preprocessor.preprocess_corpus(texts)\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.build_vocabulary(processed_texts)\n",
    "        \n",
    "        # Encode labels\n",
    "        encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            processed_texts, encoded_labels, test_size=test_size, random_state=42, stratify=encoded_labels\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = TextDataset(X_train, y_train, self.vocab, max_length)\n",
    "        test_dataset = TextDataset(X_test, y_test, self.vocab, max_length)\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def train_neural_model(self, train_dataset, test_dataset, epochs=10, batch_size=16, lr=0.001):\n",
    "        \"\"\"Train neural network model\"\"\"\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        vocab_size = len(self.vocab)\n",
    "        num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "        self.model = TextCNN(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            num_filters=self.num_filters,\n",
    "            filter_sizes=self.filter_sizes,\n",
    "            num_classes=num_classes\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_texts, batch_labels in train_loader:\n",
    "                batch_texts = batch_texts.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_texts)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            train_losses.append(avg_loss)\n",
    "            train_accuracies.append(accuracy)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        \n",
    "        return train_losses, train_accuracies\n",
    "    \n",
    "    def evaluate_neural_model(self, test_dataset):\n",
    "        \"\"\"Evaluate neural network model\"\"\"\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, batch_labels in test_loader:\n",
    "                batch_texts = batch_texts.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(batch_texts)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_predictions, average='weighted'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'predictions': all_predictions,\n",
    "            'true_labels': all_labels\n",
    "        }\n",
    "    \n",
    "    def predict_neural(self, texts):\n",
    "        \"\"\"Predict labels for new texts using neural model\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet\")\n",
    "        \n",
    "        # Preprocess texts\n",
    "        processed_texts = preprocessor.preprocess_corpus(texts)\n",
    "        \n",
    "        # Create dataset\n",
    "        dummy_labels = [0] * len(texts)  # Dummy labels for dataset creation\n",
    "        dataset = TextDataset(processed_texts, dummy_labels, self.vocab)\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "        \n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_texts, _ in dataloader:\n",
    "                batch_texts = batch_texts.to(self.device)\n",
    "                outputs = self.model(batch_texts)\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                probabilities.extend(probs.cpu().numpy())\n",
    "        \n",
    "        # Decode labels\n",
    "        predicted_labels = self.label_encoder.inverse_transform(predictions)\n",
    "        \n",
    "        return predicted_labels, probabilities\n",
    "\n",
    "# Train neural network classifier (using smaller dataset due to demo constraints)\n",
    "neural_classifier = NeuralTextClassifier()\n",
    "\n",
    "print(\"Preparing data for neural network...\")\n",
    "train_dataset_nn, test_dataset_nn = neural_classifier.prepare_neural_data(\n",
    "    texts, labels, test_size=0.3, max_length=30\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {len(neural_classifier.vocab)}\")\n",
    "print(f\"Training dataset size: {len(train_dataset_nn)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_nn)}\")\n",
    "\n",
    "print(\"\\nTraining neural network...\")\n",
    "print(\"=\" * 30)\n",
    "train_losses, train_accuracies = neural_classifier.train_neural_model(\n",
    "    train_dataset_nn, test_dataset_nn, epochs=5, batch_size=8\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating neural network...\")\n",
    "nn_results = neural_classifier.evaluate_neural_model(test_dataset_nn)\n",
    "\n",
    "print(f\"Test Accuracy: {nn_results['accuracy']:.3f}\")\n",
    "print(f\"Test F1-Score: {nn_results['f1']:.3f}\")\n",
    "\n",
    "# Test neural network predictions\n",
    "nn_test_examples = [\n",
    "    \"Deep learning models process complex data patterns\",\n",
    "    \"Athletes compete in Olympic swimming championships\",\n",
    "    \"Medical research develops new cancer treatments\"\n",
    "]\n",
    "\n",
    "print(\"\\nNeural Network Predictions:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "nn_predictions, nn_probabilities = neural_classifier.predict_neural(nn_test_examples)\n",
    "\n",
    "for i, (text, prediction) in enumerate(zip(nn_test_examples, nn_predictions)):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    prob_dict = dict(zip(neural_classifier.label_encoder.classes_, nn_probabilities[i]))\n",
    "    print(f\"Probabilities: {prob_dict}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Text Classification Challenges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 1: Feature Engineering Enhancement\n",
    "Enhance the feature extraction process with additional linguistic and statistical features.\n",
    "\n",
    "**Requirements:**\n",
    "- Add part-of-speech (POS) tag features\n",
    "- Include readability metrics (Flesch-Kincaid, etc.)\n",
    "- Implement domain-specific feature extraction\n",
    "\n",
    "**Success Criteria:**\n",
    "- Implement at least 10 new features\n",
    "- Improve classification accuracy by 5%+\n",
    "- Create feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 1\n",
    "import textstat\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class EnhancedFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize enhanced feature extraction\n",
    "        pass\n",
    "    \n",
    "    def extract_pos_features(self, texts):\n",
    "        # TODO: Extract part-of-speech features\n",
    "        pass\n",
    "    \n",
    "    def extract_readability_features(self, texts):\n",
    "        # TODO: Extract readability metrics\n",
    "        pass\n",
    "    \n",
    "    def extract_domain_features(self, texts, domain='general'):\n",
    "        # TODO: Extract domain-specific features\n",
    "        pass\n",
    "    \n",
    "    def combine_all_features(self, texts):\n",
    "        # TODO: Combine all feature types\n",
    "        pass\n",
    "\n",
    "# Test your enhanced feature extractor\n",
    "enhanced_extractor = EnhancedFeatureExtractor()\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Imbalanced Dataset Handling\n",
    "Implement techniques to handle imbalanced text classification datasets.\n",
    "\n",
    "**Requirements:**\n",
    "- Create artificially imbalanced dataset\n",
    "- Implement SMOTE for text data\n",
    "- Use cost-sensitive learning approaches\n",
    "- Compare different resampling techniques\n",
    "\n",
    "**Success Criteria:**\n",
    "- Handle datasets with 10:1 class imbalance\n",
    "- Improve F1-score for minority class by 20%+\n",
    "- Implement at least 3 different balancing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class ImbalancedTextClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize imbalanced text classifier\n",
    "        self.balancing_methods = {}\n",
    "    \n",
    "    def create_imbalanced_dataset(self, texts, labels, imbalance_ratio=0.1):\n",
    "        # TODO: Create artificially imbalanced dataset\n",
    "        pass\n",
    "    \n",
    "    def apply_smote(self, X, y):\n",
    "        # TODO: Apply SMOTE for text data\n",
    "        pass\n",
    "    \n",
    "    def apply_cost_sensitive_learning(self, X, y):\n",
    "        # TODO: Implement cost-sensitive learning\n",
    "        pass\n",
    "    \n",
    "    def compare_balancing_techniques(self, X, y):\n",
    "        # TODO: Compare different techniques\n",
    "        pass\n",
    "\n",
    "# TODO: Test imbalanced dataset handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "### Challenge 3: Hierarchical Text Classification\n",
    "Build a hierarchical text classification system that can handle nested categories.\n",
    "\n",
    "**Requirements:**\n",
    "- Create hierarchical label structure\n",
    "- Implement top-down classification approach\n",
    "- Handle different levels of the hierarchy\n",
    "- Evaluate performance at each level\n",
    "\n",
    "**Success Criteria:**\n",
    "- Support at least 3 levels of hierarchy\n",
    "- Achieve 80%+ accuracy at each level\n",
    "- Implement both flat and hierarchical evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 3\n",
    "class HierarchicalTextClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize hierarchical classifier\n",
    "        self.hierarchy = {}\n",
    "        self.level_classifiers = {}\n",
    "    \n",
    "    def build_hierarchy(self, labels):\n",
    "        # TODO: Build hierarchical structure from labels\n",
    "        # Example: \"technology/ai/machine_learning\"\n",
    "        pass\n",
    "    \n",
    "    def train_level_classifiers(self, texts, hierarchical_labels):\n",
    "        # TODO: Train classifier for each level\n",
    "        pass\n",
    "    \n",
    "    def predict_hierarchical(self, texts):\n",
    "        # TODO: Predict using top-down approach\n",
    "        pass\n",
    "    \n",
    "    def evaluate_hierarchical(self, texts, true_labels):\n",
    "        # TODO: Evaluate at each hierarchy level\n",
    "        pass\n",
    "\n",
    "# Create hierarchical dataset\n",
    "hierarchical_labels = [\n",
    "    \"technology/ai/machine_learning\",\n",
    "    \"technology/ai/computer_vision\",\n",
    "    \"technology/software/web_development\",\n",
    "    \"sports/team_sports/football\",\n",
    "    \"sports/individual_sports/tennis\",\n",
    "    \"health/nutrition/diet\",\n",
    "    \"health/fitness/exercise\"\n",
    "]\n",
    "\n",
    "# TODO: Implement and test hierarchical classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Domain Adaptation\n",
    "Build a system that can adapt text classifiers from one domain to another with minimal labeled data.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement domain adaptation techniques\n",
    "- Use transfer learning approaches\n",
    "- Handle domain shift in text data\n",
    "- Evaluate adaptation performance\n",
    "\n",
    "**Success Criteria:**\n",
    "- Achieve 70%+ accuracy on target domain with 20% training data\n",
    "- Implement at least 2 adaptation techniques\n",
    "- Show improvement over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 4\n",
    "class DomainAdaptationClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize domain adaptation classifier\n",
    "        self.source_model = None\n",
    "        self.target_model = None\n",
    "    \n",
    "    def train_source_domain(self, source_texts, source_labels):\n",
    "        # TODO: Train model on source domain\n",
    "        pass\n",
    "    \n",
    "    def adapt_to_target_domain(self, target_texts, target_labels, adaptation_method='fine_tuning'):\n",
    "        # TODO: Adapt model to target domain\n",
    "        # Methods: fine_tuning, feature_adaptation, etc.\n",
    "        pass\n",
    "    \n",
    "    def measure_domain_shift(self, source_texts, target_texts):\n",
    "        # TODO: Measure domain shift between source and target\n",
    "        pass\n",
    "    \n",
    "    def evaluate_adaptation(self, target_test_texts, target_test_labels):\n",
    "        # TODO: Evaluate adaptation performance\n",
    "        pass\n",
    "\n",
    "# TODO: Create source and target domain datasets and test adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 5: Few-Shot Text Classification\n",
    "Implement a few-shot learning system that can classify text with very few examples per class.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement prototypical networks for text\n",
    "- Use meta-learning approaches (MAML)\n",
    "- Handle new classes with 1-5 examples\n",
    "- Compare with traditional approaches\n",
    "\n",
    "**Success Criteria:**\n",
    "- Achieve 60%+ accuracy with 5 examples per class\n",
    "- Support adding new classes without retraining\n",
    "- Outperform traditional methods in few-shot scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 5\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # TODO: Define prototypical network architecture\n",
    "        pass\n",
    "    \n",
    "    def forward(self, support_set, query_set):\n",
    "        # TODO: Implement prototypical network forward pass\n",
    "        pass\n",
    "\n",
    "class FewShotTextClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize few-shot classifier\n",
    "        self.model = None\n",
    "        self.support_prototypes = {}\n",
    "    \n",
    "    def create_episode(self, texts, labels, n_way=3, k_shot=5, q_query=10):\n",
    "        # TODO: Create training/testing episodes\n",
    "        pass\n",
    "    \n",
    "    def train_few_shot(self, training_data, episodes=1000):\n",
    "        # TODO: Train few-shot learning model\n",
    "        pass\n",
    "    \n",
    "    def classify_few_shot(self, support_texts, support_labels, query_texts):\n",
    "        # TODO: Classify using few-shot approach\n",
    "        pass\n",
    "    \n",
    "    def add_new_class(self, class_name, example_texts):\n",
    "        # TODO: Add new class with few examples\n",
    "        pass\n",
    "\n",
    "# TODO: Implement and test few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Adversarial Robustness\n",
    "Build a text classification system that is robust against adversarial attacks.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement adversarial attack methods (word substitution, etc.)\n",
    "- Develop defense mechanisms\n",
    "- Evaluate robustness metrics\n",
    "- Compare different defense strategies\n",
    "\n",
    "**Success Criteria:**\n",
    "- Maintain 80%+ accuracy under adversarial attacks\n",
    "- Implement at least 3 attack methods\n",
    "- Develop effective defense strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 6\n",
    "class AdversarialTextAttacks:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize adversarial attack methods\n",
    "        pass\n",
    "    \n",
    "    def synonym_substitution_attack(self, text, model, target_class=None):\n",
    "        # TODO: Attack using synonym substitution\n",
    "        pass\n",
    "    \n",
    "    def word_insertion_attack(self, text, model):\n",
    "        # TODO: Attack using word insertion\n",
    "        pass\n",
    "    \n",
    "    def character_level_attack(self, text, model):\n",
    "        # TODO: Attack using character-level perturbations\n",
    "        pass\n",
    "\n",
    "class RobustTextClassifier:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize robust classifier\n",
    "        self.base_model = None\n",
    "        self.defense_mechanisms = []\n",
    "    \n",
    "    def adversarial_training(self, texts, labels, attack_method):\n",
    "        # TODO: Train with adversarial examples\n",
    "        pass\n",
    "    \n",
    "    def input_preprocessing_defense(self, text):\n",
    "        # TODO: Defend through input preprocessing\n",
    "        pass\n",
    "    \n",
    "    def ensemble_defense(self, texts):\n",
    "        # TODO: Defend using model ensemble\n",
    "        pass\n",
    "    \n",
    "    def evaluate_robustness(self, clean_texts, adversarial_texts, labels):\n",
    "        # TODO: Evaluate robustness metrics\n",
    "        pass\n",
    "\n",
    "# TODO: Implement and test adversarial robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÅ **Bonus Challenge: Production-Ready Text Classification System**\n",
    "\n",
    "Build a complete, production-ready text classification system with all the bells and whistles.\n",
    "\n",
    "### Requirements:\n",
    "1. **Scalable Architecture**: Handle millions of documents\n",
    "2. **Real-time Inference**: < 100ms response time\n",
    "3. **Model Monitoring**: Track performance degradation\n",
    "4. **A/B Testing**: Compare model versions\n",
    "5. **Auto-retraining**: Retrain on new data\n",
    "6. **Multi-model Ensemble**: Combine multiple approaches\n",
    "7. **Explainable Predictions**: Provide reasoning\n",
    "8. **API Documentation**: Complete REST API\n",
    "9. **Data Pipeline**: ETL for training data\n",
    "10. **Model Versioning**: Track model evolution\n",
    "\n",
    "### Success Criteria:\n",
    "- Handle 10,000+ requests per minute\n",
    "- 99.9% uptime\n",
    "- Automated CI/CD pipeline\n",
    "- Comprehensive monitoring dashboard\n",
    "- Docker containerization\n",
    "- Kubernetes deployment ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Bonus Challenge\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import redis\n",
    "import asyncio\n",
    "\n",
    "class ProductionTextClassificationSystem:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize production system\n",
    "        self.app = Flask(__name__)\n",
    "        self.models = {}  # Model registry\n",
    "        self.model_versions = {}\n",
    "        self.performance_metrics = {}\n",
    "        self.cache = None  # Redis cache\n",
    "        self.setup_logging()\n",
    "        self.setup_routes()\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        # TODO: Set up comprehensive logging\n",
    "        pass\n",
    "    \n",
    "    def setup_routes(self):\n",
    "        # TODO: Define API endpoints\n",
    "        @self.app.route('/classify', methods=['POST'])\n",
    "        def classify_text():\n",
    "            # TODO: Text classification endpoint\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/batch_classify', methods=['POST'])\n",
    "        def batch_classify():\n",
    "            # TODO: Batch classification endpoint\n",
    "            pass\n",
    "        \n",
    "        @self.app.route('/model_performance', methods=['GET'])\n",
    "        def get_model_performance():\n",
    "            # TODO: Model performance metrics\n",
    "            pass\n",
    "    \n",
    "    def load_model(self, model_path, version):\n",
    "        # TODO: Load and register model\n",
    "        pass\n",
    "    \n",
    "    def ensemble_predict(self, text):\n",
    "        # TODO: Ensemble prediction from multiple models\n",
    "        pass\n",
    "    \n",
    "    def monitor_performance(self, predictions, true_labels=None):\n",
    "        # TODO: Monitor model performance\n",
    "        pass\n",
    "    \n",
    "    def auto_retrain(self, new_data_threshold=1000):\n",
    "        # TODO: Automatic retraining logic\n",
    "        pass\n",
    "    \n",
    "    def explain_prediction(self, text, prediction):\n",
    "        # TODO: Provide prediction explanation\n",
    "        pass\n",
    "\n",
    "# Docker configuration example\n",
    "dockerfile_content = \"\"\"\n",
    "# TODO: Write Dockerfile for containerization\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "EXPOSE 5000\n",
    "\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# Kubernetes deployment example\n",
    "k8s_deployment = \"\"\"\n",
    "# TODO: Write Kubernetes deployment YAML\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: text-classifier\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: text-classifier\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: text-classifier\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: text-classifier\n",
    "        image: text-classifier:latest\n",
    "        ports:\n",
    "        - containerPort: 5000\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Complete the production system implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}