{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "At the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the fundamentals of Named Entity Recognition\n",
    "2. Implement rule-based NER approaches\n",
    "3. Build machine learning models for NER\n",
    "4. Use pre-trained models for entity extraction\n",
    "5. Evaluate NER system performance\n",
    "6. Handle multi-class entity classification\n",
    "7. Build custom entity recognition systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as:\n",
    "\n",
    "- **PERSON**: Names of people\n",
    "- **ORGANIZATION**: Companies, agencies, institutions\n",
    "- **LOCATION**: Countries, cities, addresses\n",
    "- **DATE**: Absolute or relative dates or periods\n",
    "- **TIME**: Times smaller than a day\n",
    "- **MONEY**: Monetary values\n",
    "- **PERCENT**: Percentage values\n",
    "- **FACILITY**: Buildings, airports, highways, bridges\n",
    "\n",
    "### Why is NER Important?\n",
    "\n",
    "1. **Information Extraction**: Extract structured information from unstructured text\n",
    "2. **Search Enhancement**: Improve search results by understanding entity types\n",
    "3. **Knowledge Graphs**: Build relationships between entities\n",
    "4. **Question Answering**: Identify entities relevant to questions\n",
    "5. **Content Classification**: Categorize documents based on entities\n",
    "\n",
    "### Approaches to NER\n",
    "\n",
    "1. **Rule-based**: Using patterns, dictionaries, and linguistic rules\n",
    "2. **Statistical**: Using machine learning models (CRF, SVM)\n",
    "3. **Deep Learning**: Using neural networks (BiLSTM-CRF, BERT)\n",
    "4. **Hybrid**: Combining multiple approaches"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn nltk spacy scikit-learn sklearn-crfsuite tqdm",
    "",
    "# Download spaCy English model if not present",
    "import spacy",
    "try:",
    "    spacy.load(\"en_core_web_sm\")",
    "except OSError:",
    "    !python -m spacy download en_core_web_sm",
    "",
    "# Download required NLTK data",
    "import nltk",
    "for item in ['punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker', 'words', 'conll2002']:",
    "    nltk.download(item, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('conll2002', quiet=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rule-Based Named Entity Recognition\n",
    "\n",
    "Rule-based NER uses patterns, regular expressions, and dictionaries to identify entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedNER:\n",
    "    def __init__(self):\n",
    "        # Pattern definitions\n",
    "        self.patterns = {\n",
    "            'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'PHONE': r'\\b(?:\\+?1[-.]?)?\\(?[0-9]{3}\\)?[-.]?[0-9]{3}[-.]?[0-9]{4}\\b',\n",
    "            'DATE': r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b',\n",
    "            'TIME': r'\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?:\\s?[AaPp][Mm])?\\b',\n",
    "            'MONEY': r'\\$\\s?\\d+(?:,\\d{3})*(?:\\.\\d{2})?\\b',\n",
    "            'PERCENT': r'\\b\\d+(?:\\.\\d+)?%\\b',\n",
    "            'URL': r'https?://(?:[-\\w.])+(?:[:\\d]+)?(?:/(?:[\\w/_.])*(?:\\?(?:[\\w&=%.])*)?(?:#(?:[\\w.])*)?)?'\n",
    "        }\n",
    "        \n",
    "        # Entity dictionaries\n",
    "        self.person_titles = {'mr', 'mrs', 'ms', 'dr', 'prof', 'sir', 'madam'}\n",
    "        self.organizations = {'google', 'microsoft', 'apple', 'amazon', 'facebook', 'netflix'}\n",
    "        self.locations = {'new york', 'london', 'paris', 'tokyo', 'beijing', 'delhi'}\n",
    "    \n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        entities = {}\n",
    "        \n",
    "        # Pattern-based extraction\n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                entities[entity_type] = matches\n",
    "        \n",
    "        # Dictionary-based extraction\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        # Person detection (simple heuristic)\n",
    "        persons = []\n",
    "        for i, word in enumerate(words):\n",
    "            if word in self.person_titles and i + 1 < len(words):\n",
    "                persons.append(f\"{word} {words[i+1]}\")\n",
    "        if persons:\n",
    "            entities['PERSON'] = persons\n",
    "        \n",
    "        # Organization detection\n",
    "        orgs = [word for word in words if word in self.organizations]\n",
    "        if orgs:\n",
    "            entities['ORGANIZATION'] = orgs\n",
    "        \n",
    "        # Location detection\n",
    "        locs = [word for word in words if word in self.locations]\n",
    "        if locs:\n",
    "            entities['LOCATION'] = locs\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def annotate_text(self, text: str) -> str:\n",
    "        \"\"\"Annotate text with entity tags\"\"\"\n",
    "        annotated = text\n",
    "        entities = self.extract_entities(text)\n",
    "        \n",
    "        for entity_type, entity_list in entities.items():\n",
    "            for entity in entity_list:\n",
    "                annotated = annotated.replace(entity, f\"[{entity}]_{entity_type}\")\n",
    "        \n",
    "        return annotated\n",
    "\n",
    "# Test the rule-based NER\n",
    "ner = RuleBasedNER()\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Dr. Smith from Google will meet with Ms. Johnson at 3:30 PM on 12/25/2023 in New York.\n",
    "Please contact him at john.smith@gmail.com or call (555) 123-4567.\n",
    "The project budget is $50,000 with a 15% contingency.\n",
    "Visit our website at https://www.example.com for more details.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nExtracted entities:\")\n",
    "entities = ner.extract_entities(sample_text)\n",
    "for ent_type, ent_list in entities.items():\n",
    "    print(f\"{ent_type}: {ent_list}\")\n",
    "\n",
    "print(\"\\nAnnotated text:\")\n",
    "print(ner.annotate_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature-Based NER with Machine Learning\n",
    "\n",
    "We'll build features for each word and use machine learning to classify entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBasedNER:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.model = LogisticRegression(max_iter=1000)\n",
    "        self.label_to_id = {}\n",
    "        self.id_to_label = {}\n",
    "    \n",
    "    def extract_features(self, tokens: List[str], pos_tags: List[str], index: int) -> Dict[str, Any]:\n",
    "        \"\"\"Extract features for word at given index\"\"\"\n",
    "        word = tokens[index]\n",
    "        pos = pos_tags[index]\n",
    "        \n",
    "        features = {\n",
    "            # Word features\n",
    "            'word': word.lower(),\n",
    "            'word_length': len(word),\n",
    "            'is_capitalized': word[0].isupper(),\n",
    "            'is_all_caps': word.isupper(),\n",
    "            'is_title_case': word.istitle(),\n",
    "            'is_numeric': word.isdigit(),\n",
    "            'has_digit': any(c.isdigit() for c in word),\n",
    "            'has_hyphen': '-' in word,\n",
    "            'has_dot': '.' in word,\n",
    "            \n",
    "            # POS features\n",
    "            'pos': pos,\n",
    "            'is_noun': pos.startswith('N'),\n",
    "            'is_proper_noun': pos == 'NNP',\n",
    "            \n",
    "            # Shape features\n",
    "            'word_shape': self.get_word_shape(word),\n",
    "            \n",
    "            # Prefix/Suffix features\n",
    "            'prefix_2': word[:2].lower() if len(word) >= 2 else '',\n",
    "            'prefix_3': word[:3].lower() if len(word) >= 3 else '',\n",
    "            'suffix_2': word[-2:].lower() if len(word) >= 2 else '',\n",
    "            'suffix_3': word[-3:].lower() if len(word) >= 3 else '',\n",
    "        }\n",
    "        \n",
    "        # Context features\n",
    "        if index > 0:\n",
    "            features['prev_word'] = tokens[index-1].lower()\n",
    "            features['prev_pos'] = pos_tags[index-1]\n",
    "        else:\n",
    "            features['prev_word'] = 'BOS'\n",
    "            features['prev_pos'] = 'BOS'\n",
    "        \n",
    "        if index < len(tokens) - 1:\n",
    "            features['next_word'] = tokens[index+1].lower()\n",
    "            features['next_pos'] = pos_tags[index+1]\n",
    "        else:\n",
    "            features['next_word'] = 'EOS'\n",
    "            features['next_pos'] = 'EOS'\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_word_shape(self, word: str) -> str:\n",
    "        \"\"\"Get word shape (X=uppercase, x=lowercase, d=digit, p=punctuation)\"\"\"\n",
    "        shape = ''\n",
    "        for char in word:\n",
    "            if char.isupper():\n",
    "                shape += 'X'\n",
    "            elif char.islower():\n",
    "                shape += 'x'\n",
    "            elif char.isdigit():\n",
    "                shape += 'd'\n",
    "            else:\n",
    "                shape += 'p'\n",
    "        return shape\n",
    "    \n",
    "    def prepare_data(self, sentences: List[List[Tuple[str, str]]]) -> Tuple[List[Dict], List[str]]:\n",
    "        \"\"\"Prepare features and labels from annotated sentences\"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            tokens = [token for token, _ in sentence]\n",
    "            tags = [tag for _, tag in sentence]\n",
    "            \n",
    "            # Get POS tags\n",
    "            pos_tags = [pos for _, pos in nltk.pos_tag(tokens)]\n",
    "            \n",
    "            for i in range(len(tokens)):\n",
    "                feat = self.extract_features(tokens, pos_tags, i)\n",
    "                features.append(feat)\n",
    "                labels.append(tags[i])\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def train(self, train_sentences: List[List[Tuple[str, str]]]):\n",
    "        \"\"\"Train the NER model\"\"\"\n",
    "        print(\"Preparing training data...\")\n",
    "        features, labels = self.prepare_data(train_sentences)\n",
    "        \n",
    "        # Create label mappings\n",
    "        unique_labels = list(set(labels))\n",
    "        self.label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "        self.id_to_label = {i: label for label, i in self.label_to_id.items()}\n",
    "        \n",
    "        # Vectorize features and encode labels\n",
    "        X = self.vectorizer.fit_transform(features)\n",
    "        y = [self.label_to_id[label] for label in labels]\n",
    "        \n",
    "        print(f\"Training on {len(features)} examples with {len(unique_labels)} labels...\")\n",
    "        self.model.fit(X, y)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "    def predict(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"Predict entity labels for a sentence\"\"\"\n",
    "        pos_tags = [pos for _, pos in nltk.pos_tag(sentence)]\n",
    "        features = []\n",
    "        \n",
    "        for i in range(len(sentence)):\n",
    "            feat = self.extract_features(sentence, pos_tags, i)\n",
    "            features.append(feat)\n",
    "        \n",
    "        X = self.vectorizer.transform(features)\n",
    "        predictions = self.model.predict(X)\n",
    "        \n",
    "        return [self.id_to_label[pred] for pred in predictions]\n",
    "\n",
    "# Create sample training data (in real scenarios, you'd use CoNLL format data)\n",
    "sample_training_data = [\n",
    "    [('John', 'B-PER'), ('Smith', 'I-PER'), ('works', 'O'), ('at', 'O'), ('Google', 'B-ORG'), ('in', 'O'), ('California', 'B-LOC')],\n",
    "    [('Apple', 'B-ORG'), ('Inc', 'I-ORG'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Cupertino', 'B-LOC')],\n",
    "    [('Barack', 'B-PER'), ('Obama', 'I-PER'), ('was', 'O'), ('born', 'O'), ('in', 'O'), ('Hawaii', 'B-LOC')],\n",
    "    [('Microsoft', 'B-ORG'), ('Corporation', 'I-ORG'), ('headquarters', 'O'), ('in', 'O'), ('Seattle', 'B-LOC')],\n",
    "    [('The', 'O'), ('meeting', 'O'), ('is', 'O'), ('scheduled', 'O'), ('for', 'O'), ('Monday', 'O')]\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "ml_ner = FeatureBasedNER()\n",
    "ml_ner.train(sample_training_data)\n",
    "\n",
    "# Test prediction\n",
    "test_sentence = ['Elon', 'Musk', 'founded', 'Tesla', 'Motors']\n",
    "predictions = ml_ner.predict(test_sentence)\n",
    "\n",
    "print(\"\\nTest sentence with predictions:\")\n",
    "for word, pred in zip(test_sentence, predictions):\n",
    "    print(f\"{word}: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conditional Random Fields (CRF) for NER\n",
    "\n",
    "CRF is particularly effective for sequence labeling tasks like NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRFBasedNER:\n",
    "    def __init__(self):\n",
    "        self.crf = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "    \n",
    "    def word_features(self, sentence: List[str], i: int) -> Dict[str, Any]:\n",
    "        \"\"\"Extract features for word at position i in sentence\"\"\"\n",
    "        word = sentence[i]\n",
    "        pos_tags = [pos for _, pos in nltk.pos_tag(sentence)]\n",
    "        \n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word.lower()': word.lower(),\n",
    "            'word[-3:]': word[-3:],\n",
    "            'word[-2:]': word[-2:],\n",
    "            'word.isupper()': word.isupper(),\n",
    "            'word.istitle()': word.istitle(),\n",
    "            'word.isdigit()': word.isdigit(),\n",
    "            'postag': pos_tags[i],\n",
    "            'postag[:2]': pos_tags[i][:2],\n",
    "        }\n",
    "        \n",
    "        if i > 0:\n",
    "            word1 = sentence[i-1]\n",
    "            postag1 = pos_tags[i-1]\n",
    "            features.update({\n",
    "                '-1:word.lower()': word1.lower(),\n",
    "                '-1:word.istitle()': word1.istitle(),\n",
    "                '-1:word.isupper()': word1.isupper(),\n",
    "                '-1:postag': postag1,\n",
    "                '-1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = True\n",
    "        \n",
    "        if i < len(sentence) - 1:\n",
    "            word1 = sentence[i+1]\n",
    "            postag1 = pos_tags[i+1]\n",
    "            features.update({\n",
    "                '+1:word.lower()': word1.lower(),\n",
    "                '+1:word.istitle()': word1.istitle(),\n",
    "                '+1:word.isupper()': word1.isupper(),\n",
    "                '+1:postag': postag1,\n",
    "                '+1:postag[:2]': postag1[:2],\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = True\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def sentence_features(self, sentence: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Extract features for entire sentence\"\"\"\n",
    "        return [self.word_features(sentence, i) for i in range(len(sentence))]\n",
    "    \n",
    "    def prepare_data(self, sentences: List[List[Tuple[str, str]]]) -> Tuple[List[List[Dict]], List[List[str]]]:\n",
    "        \"\"\"Prepare CRF training data\"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            words = [word for word, _ in sentence]\n",
    "            labels = [label for _, label in sentence]\n",
    "            \n",
    "            X.append(self.sentence_features(words))\n",
    "            y.append(labels)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, train_sentences: List[List[Tuple[str, str]]]):\n",
    "        \"\"\"Train CRF model\"\"\"\n",
    "        print(\"Preparing CRF training data...\")\n",
    "        X_train, y_train = self.prepare_data(train_sentences)\n",
    "        \n",
    "        print(f\"Training CRF on {len(X_train)} sentences...\")\n",
    "        self.crf.fit(X_train, y_train)\n",
    "        print(\"CRF training completed!\")\n",
    "    \n",
    "    def predict(self, sentence: List[str]) -> List[str]:\n",
    "        \"\"\"Predict labels for sentence\"\"\"\n",
    "        features = self.sentence_features(sentence)\n",
    "        return self.crf.predict([features])[0]\n",
    "    \n",
    "    def evaluate(self, test_sentences: List[List[Tuple[str, str]]]) -> str:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        X_test, y_test = self.prepare_data(test_sentences)\n",
    "        y_pred = self.crf.predict(X_test)\n",
    "        \n",
    "        return flat_classification_report(y_test, y_pred)\n",
    "\n",
    "# Extended training data for CRF\n",
    "extended_training_data = sample_training_data + [\n",
    "    [('Amazon', 'B-ORG'), ('Web', 'I-ORG'), ('Services', 'I-ORG'), ('hosts', 'O'), ('in', 'O'), ('Virginia', 'B-LOC')],\n",
    "    [('Netflix', 'B-ORG'), ('streams', 'O'), ('globally', 'O'), ('from', 'O'), ('Los', 'B-LOC'), ('Angeles', 'I-LOC')],\n",
    "    [('Tim', 'B-PER'), ('Cook', 'I-PER'), ('leads', 'O'), ('Apple', 'B-ORG'), ('today', 'O')],\n",
    "    [('Facebook', 'B-ORG'), ('changed', 'O'), ('to', 'O'), ('Meta', 'B-ORG'), ('recently', 'O')],\n",
    "    [('London', 'B-LOC'), ('is', 'O'), ('the', 'O'), ('capital', 'O'), ('of', 'O'), ('England', 'B-LOC')]\n",
    "]\n",
    "\n",
    "# Train CRF model\n",
    "crf_ner = CRFBasedNER()\n",
    "crf_ner.train(extended_training_data)\n",
    "\n",
    "# Test CRF model\n",
    "test_sentences_crf = [\n",
    "    ['Mark', 'Zuckerberg', 'founded', 'Facebook', 'in', 'California'],\n",
    "    ['IBM', 'has', 'offices', 'in', 'New', 'York'],\n",
    "    ['Toyota', 'manufactures', 'cars', 'in', 'Japan']\n",
    "]\n",
    "\n",
    "print(\"\\nCRF Model Predictions:\")\n",
    "for sentence in test_sentences_crf:\n",
    "    predictions = crf_ner.predict(sentence)\n",
    "    print(f\"Sentence: {' '.join(sentence)}\")\n",
    "    for word, pred in zip(sentence, predictions):\n",
    "        print(f\"  {word}: {pred}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Pre-trained NER Models\n",
    "\n",
    "We'll use spaCy's pre-trained models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This requires spaCy model installation\n",
    "# Run: python -m spacy download en_core_web_sm\n",
    "\n",
    "def compare_ner_models(text: str):\n",
    "    \"\"\"Compare different NER approaches on the same text\"\"\"\n",
    "    print(f\"Text: {text}\\n\")\n",
    "    \n",
    "    # Rule-based NER\n",
    "    print(\"1. Rule-based NER:\")\n",
    "    rule_entities = ner.extract_entities(text)\n",
    "    for ent_type, entities in rule_entities.items():\n",
    "        print(f\"   {ent_type}: {entities}\")\n",
    "    \n",
    "    # NLTK NER\n",
    "    print(\"\\n2. NLTK NER:\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    chunks = nltk.ne_chunk(pos_tags)\n",
    "    \n",
    "    nltk_entities = []\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "            entity_type = chunk.label()\n",
    "            nltk_entities.append((entity_name, entity_type))\n",
    "    \n",
    "    for entity_name, entity_type in nltk_entities:\n",
    "        print(f\"   {entity_type}: {entity_name}\")\n",
    "    \n",
    "    # SpaCy NER (if available)\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc = nlp(text)\n",
    "        print(\"\\n3. SpaCy NER:\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"   {ent.label_}: {ent.text}\")\n",
    "    except OSError:\n",
    "        print(\"\\n3. SpaCy NER: (Model not installed)\")\n",
    "        print(\"   Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# Test comparison\n",
    "comparison_text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California on April 1, 1976.\"\n",
    "compare_ner_models(comparison_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "comparison_text2 = \"Elon Musk, CEO of Tesla and SpaceX, was born in South Africa and now lives in Texas.\"\n",
    "compare_ner_models(comparison_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NER Evaluation Metrics\n",
    "\n",
    "Understanding how to evaluate NER systems properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERMetrics:\n",
    "    def __init__(self):\n",
    "        self.tp = defaultdict(int)  # True positives\n",
    "        self.fp = defaultdict(int)  # False positives\n",
    "        self.fn = defaultdict(int)  # False negatives\n",
    "    \n",
    "    def extract_entities_from_bio(self, tokens: List[str], bio_tags: List[str]) -> List[Tuple[str, int, int, str]]:\n",
    "        \"\"\"Extract entities from BIO-tagged sequence\"\"\"\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for i, (token, tag) in enumerate(zip(tokens, bio_tags)):\n",
    "            if tag.startswith('B-'):\n",
    "                # Begin new entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = (tag[2:], i, i, token)\n",
    "            elif tag.startswith('I-') and current_entity and tag[2:] == current_entity[0]:\n",
    "                # Continue current entity\n",
    "                current_entity = (current_entity[0], current_entity[1], i, \n",
    "                                current_entity[3] + ' ' + token)\n",
    "            else:\n",
    "                # End current entity\n",
    "                if current_entity:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def evaluate_sequence(self, tokens: List[str], true_tags: List[str], pred_tags: List[str]):\n",
    "        \"\"\"Evaluate a single sequence\"\"\"\n",
    "        true_entities = set(self.extract_entities_from_bio(tokens, true_tags))\n",
    "        pred_entities = set(self.extract_entities_from_bio(tokens, pred_tags))\n",
    "        \n",
    "        # Count TP, FP, FN for each entity type\n",
    "        for entity in true_entities:\n",
    "            entity_type = entity[0]\n",
    "            if entity in pred_entities:\n",
    "                self.tp[entity_type] += 1\n",
    "            else:\n",
    "                self.fn[entity_type] += 1\n",
    "        \n",
    "        for entity in pred_entities:\n",
    "            entity_type = entity[0]\n",
    "            if entity not in true_entities:\n",
    "                self.fp[entity_type] += 1\n",
    "    \n",
    "    def compute_metrics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Compute precision, recall, and F1 for each entity type\"\"\"\n",
    "        metrics = {}\n",
    "        all_types = set(self.tp.keys()) | set(self.fp.keys()) | set(self.fn.keys())\n",
    "        \n",
    "        total_tp = total_fp = total_fn = 0\n",
    "        \n",
    "        for entity_type in all_types:\n",
    "            tp = self.tp[entity_type]\n",
    "            fp = self.fp[entity_type]\n",
    "            fn = self.fn[entity_type]\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics[entity_type] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'support': tp + fn\n",
    "            }\n",
    "            \n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "        \n",
    "        metrics['overall'] = {\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall,\n",
    "            'f1': overall_f1,\n",
    "            'support': total_tp + total_fn\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print evaluation report\"\"\"\n",
    "        metrics = self.compute_metrics()\n",
    "        \n",
    "        print(f\"{'Entity Type':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        for entity_type, scores in metrics.items():\n",
    "            if entity_type != 'overall':\n",
    "                print(f\"{entity_type:<15} {scores['precision']:<10.3f} {scores['recall']:<10.3f} {scores['f1']:<10.3f} {scores['support']:<10}\")\n",
    "        \n",
    "        print(\"-\" * 65)\n",
    "        overall = metrics['overall']\n",
    "        print(f\"{'Overall':<15} {overall['precision']:<10.3f} {overall['recall']:<10.3f} {overall['f1']:<10.3f} {overall['support']:<10}\")\n",
    "\n",
    "# Example evaluation\n",
    "evaluator = NERMetrics()\n",
    "\n",
    "# Sample data for evaluation\n",
    "test_cases = [\n",
    "    (\n",
    "        ['John', 'Smith', 'works', 'at', 'Google', 'in', 'California'],\n",
    "        ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC'],\n",
    "        ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O', 'B-LOC']  # Perfect prediction\n",
    "    ),\n",
    "    (\n",
    "        ['Apple', 'Inc', 'was', 'founded', 'by', 'Steve', 'Jobs'],\n",
    "        ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'B-PER', 'I-PER'],\n",
    "        ['B-ORG', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER']  # Missed I-ORG\n",
    "    ),\n",
    "    (\n",
    "        ['Microsoft', 'is', 'in', 'Seattle', 'Washington'],\n",
    "        ['B-ORG', 'O', 'O', 'B-LOC', 'I-LOC'],\n",
    "        ['B-ORG', 'O', 'O', 'B-LOC', 'B-LOC']  # Wrong tag for Washington\n",
    "    )\n",
    "]\n",
    "\n",
    "for tokens, true_tags, pred_tags in test_cases:\n",
    "    evaluator.evaluate_sequence(tokens, true_tags, pred_tags)\n",
    "\n",
    "print(\"NER Evaluation Report:\")\n",
    "evaluator.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Entity Types and Domain Adaptation\n",
    "\n",
    "Creating NER systems for specific domains with custom entity types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomedicalNER:\n",
    "    \"\"\"NER system specifically for biomedical domain\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Biomedical entity patterns\n",
    "        self.patterns = {\n",
    "            'GENE': r'\\b[A-Z][A-Z0-9]+\\b',  # Simple gene pattern\n",
    "            'PROTEIN': r'\\b[A-Z][a-z]+[0-9]*\\b',  # Simple protein pattern\n",
    "            'DISEASE': r'\\b(?:cancer|diabetes|hypertension|asthma|pneumonia)\\b',\n",
    "            'DRUG': r'\\b(?:aspirin|ibuprofen|penicillin|insulin|morphine)\\b',\n",
    "            'DOSAGE': r'\\b\\d+\\s*(?:mg|g|ml|cc|units?)\\b'\n",
    "        }\n",
    "        \n",
    "        # Domain-specific dictionaries\n",
    "        self.gene_dict = {'BRCA1', 'BRCA2', 'TP53', 'EGFR', 'KRAS'}\n",
    "        self.protein_dict = {'insulin', 'hemoglobin', 'collagen', 'keratin'}\n",
    "        self.disease_dict = {'alzheimer', 'parkinson', 'huntington', 'diabetes'}\n",
    "        self.drug_dict = {'aspirin', 'metformin', 'lisinopril', 'atorvastatin'}\n",
    "    \n",
    "    def extract_biomedical_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract biomedical entities from text\"\"\"\n",
    "        entities = {}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Pattern-based extraction\n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                entities[entity_type] = list(set(matches))  # Remove duplicates\n",
    "        \n",
    "        # Dictionary-based extraction\n",
    "        words = text_lower.split()\n",
    "        \n",
    "        genes = [word for word in words if word.upper() in self.gene_dict]\n",
    "        if genes:\n",
    "            entities['GENE'] = entities.get('GENE', []) + genes\n",
    "        \n",
    "        proteins = [word for word in words if word in self.protein_dict]\n",
    "        if proteins:\n",
    "            entities['PROTEIN'] = entities.get('PROTEIN', []) + proteins\n",
    "        \n",
    "        diseases = [word for word in words if word in self.disease_dict]\n",
    "        if diseases:\n",
    "            entities['DISEASE'] = entities.get('DISEASE', []) + diseases\n",
    "        \n",
    "        drugs = [word for word in words if word in self.drug_dict]\n",
    "        if drugs:\n",
    "            entities['DRUG'] = entities.get('DRUG', []) + drugs\n",
    "        \n",
    "        return entities\n",
    "\n",
    "# Financial NER example\n",
    "class FinancialNER:\n",
    "    \"\"\"NER system for financial domain\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'STOCK_SYMBOL': r'\\b[A-Z]{2,5}\\b',\n",
    "            'CURRENCY': r'\\$[0-9,]+(?:\\.[0-9]{2})?|USD|EUR|GBP',\n",
    "            'PERCENTAGE': r'\\b\\d+(?:\\.\\d+)?%\\b',\n",
    "            'FINANCIAL_TERM': r'\\b(?:IPO|merger|acquisition|dividend|earnings|revenue)\\b'\n",
    "        }\n",
    "        \n",
    "        self.companies = {'apple', 'google', 'microsoft', 'amazon', 'tesla'}\n",
    "        self.financial_instruments = {'stock', 'bond', 'option', 'future', 'etf'}\n",
    "    \n",
    "    def extract_financial_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract financial entities from text\"\"\"\n",
    "        entities = {}\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for entity_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                entities[entity_type] = list(set(matches))\n",
    "        \n",
    "        words = text_lower.split()\n",
    "        \n",
    "        companies = [word for word in words if word in self.companies]\n",
    "        if companies:\n",
    "            entities['COMPANY'] = companies\n",
    "        \n",
    "        instruments = [word for word in words if word in self.financial_instruments]\n",
    "        if instruments:\n",
    "            entities['INSTRUMENT'] = instruments\n",
    "        \n",
    "        return entities\n",
    "\n",
    "# Test domain-specific NER\n",
    "bio_ner = BiomedicalNER()\n",
    "fin_ner = FinancialNER()\n",
    "\n",
    "bio_text = \"The patient was diagnosed with diabetes and prescribed 500mg metformin. BRCA1 gene mutation increases cancer risk.\"\n",
    "fin_text = \"Apple stock (AAPL) rose 5.2% to $150.00 after strong earnings report. The company announced a dividend increase.\"\n",
    "\n",
    "print(\"Biomedical NER Results:\")\n",
    "bio_entities = bio_ner.extract_biomedical_entities(bio_text)\n",
    "for ent_type, entities in bio_entities.items():\n",
    "    print(f\"  {ent_type}: {entities}\")\n",
    "\n",
    "print(\"\\nFinancial NER Results:\")\n",
    "fin_entities = fin_ner.extract_financial_entities(fin_text)\n",
    "for ent_type, entities in fin_entities.items():\n",
    "    print(f\"  {ent_type}: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# NER Challenges\n",
    "\n",
    "Test your understanding with these progressive challenges!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Challenge 1: Pattern Enhancement\n",
    "Enhance the `RuleBasedNER` class with better patterns for:\n",
    "- Social Security Numbers (XXX-XX-XXXX)\n",
    "- IP Addresses (XXX.XXX.XXX.XXX)\n",
    "- Credit Card Numbers (XXXX-XXXX-XXXX-XXXX)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Add at least 3 new entity patterns\n",
    "- Test with sample text containing these entities\n",
    "- Achieve 90%+ precision on test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 1\n",
    "class EnhancedRuleBasedNER(RuleBasedNER):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: Add new patterns for SSN, IP addresses, and credit card numbers\n",
    "        pass\n",
    "\n",
    "# Test your enhanced NER\n",
    "test_text = \"\"\"\n",
    "Contact info: SSN 123-45-6789, IP address 192.168.1.1, \n",
    "Credit card 1234-5678-9012-3456 expires 12/25\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Entity Linking\n",
    "Create a simple entity linking system that maps recognized entities to knowledge base entries.\n",
    "\n",
    "**Requirements:**\n",
    "- Create a knowledge base with entity information\n",
    "- Link recognized entities to KB entries\n",
    "- Handle entity disambiguation\n",
    "\n",
    "**Success Criteria:**\n",
    "- Successfully link at least 80% of entities\n",
    "- Handle ambiguous entities (e.g., \"Apple\" company vs fruit)\n",
    "- Provide confidence scores for links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 2\n",
    "class EntityLinker:\n",
    "    def __init__(self):\n",
    "        # TODO: Create knowledge base\n",
    "        self.knowledge_base = {}\n",
    "    \n",
    "    def link_entities(self, entities, context):\n",
    "        # TODO: Implement entity linking logic\n",
    "        pass\n",
    "\n",
    "# Test entity linking\n",
    "test_entities = [('Apple', 'ORG'), ('New York', 'LOC'), ('Smith', 'PER')]\n",
    "context = \"Apple Inc. announced new products in New York where John Smith presented.\"\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 3: Multi-language NER\n",
    "Extend the NER system to handle multiple languages (English, Spanish, French).\n",
    "\n",
    "**Requirements:**\n",
    "- Detect language of input text\n",
    "- Use language-specific patterns and dictionaries\n",
    "- Handle code-switching (mixed languages)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Support at least 3 languages\n",
    "- Achieve 75%+ F1 score on multilingual test set\n",
    "- Handle mixed-language sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 3\n",
    "class MultilingualNER:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize language-specific resources\n",
    "        self.language_patterns = {}\n",
    "        self.language_dictionaries = {}\n",
    "    \n",
    "    def detect_language(self, text):\n",
    "        # TODO: Implement language detection\n",
    "        pass\n",
    "    \n",
    "    def extract_entities_multilingual(self, text):\n",
    "        # TODO: Extract entities considering language\n",
    "        pass\n",
    "\n",
    "# Test multilingual NER\n",
    "test_texts = [\n",
    "    \"Apple Inc. is located in California.\",  # English\n",
    "    \"Juan García trabaja en Madrid, España.\",  # Spanish\n",
    "    \"Marie Dupont vit à Paris, France.\",  # French\n",
    "]\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Active Learning for NER\n",
    "Implement an active learning system that selects the most informative examples for annotation.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement uncertainty sampling\n",
    "- Create annotation interface simulation\n",
    "- Update model with new annotations\n",
    "\n",
    "**Success Criteria:**\n",
    "- Reduce annotation effort by 40% compared to random sampling\n",
    "- Achieve target performance with fewer labeled examples\n",
    "- Implement at least 2 query strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 4\n",
    "class ActiveLearningNER:\n",
    "    def __init__(self, base_model):\n",
    "        self.model = base_model\n",
    "        self.labeled_data = []\n",
    "        self.unlabeled_data = []\n",
    "    \n",
    "    def uncertainty_sampling(self, candidates, n_samples=5):\n",
    "        # TODO: Select most uncertain examples\n",
    "        pass\n",
    "    \n",
    "    def diversity_sampling(self, candidates, n_samples=5):\n",
    "        # TODO: Select diverse examples\n",
    "        pass\n",
    "    \n",
    "    def update_model(self, new_annotations):\n",
    "        # TODO: Retrain model with new data\n",
    "        pass\n",
    "\n",
    "# TODO: Implement and test active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Challenge 5: Nested Entity Recognition\n",
    "Build a system that can recognize nested entities (entities within entities).\n",
    "\n",
    "**Example:**\n",
    "- \"University of California, Berkeley\" contains:\n",
    "  - ORGANIZATION: \"University of California, Berkeley\"\n",
    "  - LOCATION: \"California\"\n",
    "  - LOCATION: \"Berkeley\"\n",
    "\n",
    "**Requirements:**\n",
    "- Handle overlapping entity spans\n",
    "- Maintain entity hierarchy\n",
    "- Resolve entity boundaries conflicts\n",
    "\n",
    "**Success Criteria:**\n",
    "- Successfully identify nested entities in 85% of cases\n",
    "- Maintain hierarchy relationships\n",
    "- Handle at least 3 levels of nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 5\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "class NestedNER:\n",
    "    def __init__(self):\n",
    "        # TODO: Initialize nested entity recognition system\n",
    "        pass\n",
    "    \n",
    "    def find_nested_entities(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return format: [\n",
    "            {\n",
    "                'text': 'entity text',\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'type': 'ENTITY_TYPE',\n",
    "                'children': [nested_entities],\n",
    "                'parent': parent_entity_id\n",
    "            }\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # TODO: Implement nested entity extraction\n",
    "        pass\n",
    "    \n",
    "    def resolve_conflicts(self, entities: List[Dict]) -> List[Dict]:\n",
    "        # TODO: Resolve overlapping entity conflicts\n",
    "        pass\n",
    "\n",
    "# Test nested NER\n",
    "test_cases = [\n",
    "    \"University of California, Berkeley\",\n",
    "    \"New York City, New York, United States\",\n",
    "    \"Apple Inc. CEO Tim Cook from California\"\n",
    "]\n",
    "\n",
    "# TODO: Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 6: Few-Shot NER with Meta-Learning\n",
    "Implement a few-shot learning system that can quickly adapt to new entity types with minimal examples.\n",
    "\n",
    "**Requirements:**\n",
    "- Implement meta-learning algorithm (MAML or similar)\n",
    "- Support for new entity types with 5-10 examples\n",
    "- Fast adaptation mechanism\n",
    "\n",
    "**Success Criteria:**\n",
    "- Achieve 70%+ F1 on new entity types with 5 examples\n",
    "- Adaptation time < 1 minute\n",
    "- Support at least 5 different domain adaptations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution for Challenge 6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class FewShotNER:\n",
    "    def __init__(self, embedding_dim=128, hidden_dim=256):\n",
    "        # TODO: Initialize meta-learning NER model\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.model = None  # TODO: Define model architecture\n",
    "    \n",
    "    def meta_train(self, support_sets, query_sets, n_epochs=100):\n",
    "        # TODO: Implement MAML training\n",
    "        pass\n",
    "    \n",
    "    def fast_adapt(self, support_examples, n_steps=5):\n",
    "        # TODO: Fast adaptation to new entity type\n",
    "        pass\n",
    "    \n",
    "    def predict_new_domain(self, text, entity_type):\n",
    "        # TODO: Predict entities of new type\n",
    "        pass\n",
    "\n",
    "# TODO: Implement and test few-shot NER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}