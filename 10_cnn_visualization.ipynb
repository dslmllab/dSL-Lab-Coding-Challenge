{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Visualization: Understanding Deep Neural Networks\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Visualize CNN filters and weights** at different layers\n",
    "2. **Apply Zeiler & Fergus approach** to understand what CNNs learn\n",
    "3. **Apply Mahendran & Vedaldi approach** for feature inversion\n",
    "4. **Analyze layer-wise representations** in pre-trained networks\n",
    "5. **Understand the hierarchical feature learning** in deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Deep neural networks, particularly CNNs, are often considered \"black boxes\" due to their complexity. This notebook explores various techniques to visualize and understand what these networks learn at different layers.\n",
    "\n",
    "### Key Visualization Approaches:\n",
    "\n",
    "1. **Filter Visualization**: Direct visualization of learned weights\n",
    "2. **Feature Maps**: Visualizing activations at different layers\n",
    "3. **Feature Inversion**: Reconstructing inputs that maximize specific activations\n",
    "4. **Gradient-based Methods**: Understanding feature importance\n",
    "\n",
    "### The VGG16 Architecture\n",
    "\n",
    "We'll use VGG16, a well-known CNN architecture that won the ImageNet challenge in 2014. It consists of 16 layers with learnable parameters and has a very uniform architecture that makes it ideal for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages",
    "!pip install numpy pandas matplotlib seaborn torch torchvision tqdm scikit-learn pillow",
    "",
    "# Enable ipywidgets for Jupyter (for interactive dashboard)",
    "!jupyter nbextension enable --py widgetsnbextension",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Import our utility functions\n",
    "import cnn_utils\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"VGG16 Feature Layers:\")\n",
    "print(model.features)\n",
    "print(\"\\nVGG16 Classifier:\")\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature layers for easier access\n",
    "modules = list(model.features.modules())[1:]  # Remove the Sequential wrapper\n",
    "\n",
    "print(f\"Total number of feature layers: {len(modules)}\")\n",
    "print(\"\\nFirst few layers:\")\n",
    "for i, module in enumerate(modules[:5]):\n",
    "    print(f\"Layer {i}: {module}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sample_image():\n",
    "    \"\"\"Download a sample image for visualization\"\"\"\n",
    "    url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        img.save('sample_cat.jpg')\n",
    "        return img\n",
    "    except:\n",
    "        print(\"Could not download image. Please provide your own image.\")\n",
    "        return None\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"Normalize image for VGG16 input\"\"\"\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Download and load sample image\n",
    "img_raw = download_sample_image()\n",
    "if img_raw:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(img_raw)\n",
    "    plt.title(\"Original Image\", fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Preprocess for model input\n",
    "    img_tensor = normalize_image(img_raw)\n",
    "    print(f\"Input tensor shape: {img_tensor.shape}\")\n",
    "else:\n",
    "    print(\"Please add your own image file and update the path below\")\n",
    "    # img_raw = Image.open('your_image.jpg')\n",
    "    # img_tensor = normalize_image(img_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model prediction\n",
    "if 'img_tensor' in locals():\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
    "    \n",
    "    # Load ImageNet class labels\n",
    "    try:\n",
    "        # Download ImageNet class labels\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve(\n",
    "            \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\",\n",
    "            \"imagenet_classes.txt\"\n",
    "        )\n",
    "        \n",
    "        with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "            classes = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        print(\"Top 5 Predictions:\")\n",
    "        for i in range(5):\n",
    "            class_idx = top5_indices[0][i].item()\n",
    "            prob = top5_prob[0][i].item()\n",
    "            print(f\"{i+1}. {classes[class_idx]}: {prob:.4f}\")\n",
    "    except:\n",
    "        print(\"Could not load class labels. Showing raw predictions:\")\n",
    "        print(f\"Top prediction index: {top5_indices[0][0].item()}\")\n",
    "        print(f\"Confidence: {top5_prob[0][0].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Filter Weights\n",
    "\n",
    "Let's start by visualizing the actual learned weights in the convolutional filters. These filters are what the network uses to detect features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_weights(model, layer_idx=0, max_filters=64):\n",
    "    \"\"\"Visualize convolutional filter weights\"\"\"\n",
    "    \n",
    "    # Collect all convolutional layers\n",
    "    conv_layers = []\n",
    "    for module in model.features.children():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_layers.append(module)\n",
    "    \n",
    "    if layer_idx >= len(conv_layers):\n",
    "        print(f\"Layer index {layer_idx} is out of range. Max index: {len(conv_layers)-1}\")\n",
    "        return\n",
    "    \n",
    "    # Get weights from specified layer\n",
    "    layer = conv_layers[layer_idx]\n",
    "    weights = layer.weight.data.cpu()\n",
    "    \n",
    "    print(f\"Layer {layer_idx} weights shape: {weights.shape}\")\n",
    "    print(f\"(num_filters={weights.shape[0]}, input_channels={weights.shape[1]}, height={weights.shape[2]}, width={weights.shape[3]})\")\n",
    "    \n",
    "    # Limit number of filters to visualize\n",
    "    num_filters = min(max_filters, weights.shape[0])\n",
    "    grid_size = int(np.ceil(np.sqrt(num_filters)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(12, 12))\n",
    "    fig.suptitle(f'Conv Layer {layer_idx} Filter Weights\\n{layer}', fontsize=14)\n",
    "    \n",
    "    for i in range(grid_size * grid_size):\n",
    "        row, col = i // grid_size, i % grid_size\n",
    "        \n",
    "        if i < num_filters:\n",
    "            # Average across input channels for visualization\n",
    "            filter_img = weights[i].mean(dim=0)\n",
    "            \n",
    "            # Normalize for better visualization\n",
    "            filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min())\n",
    "            \n",
    "            axes[row, col].imshow(filter_img, cmap='viridis')\n",
    "            axes[row, col].set_title(f'Filter {i}', fontsize=8)\n",
    "        \n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first layer filters\n",
    "visualize_conv_weights(model, layer_idx=0, max_filters=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Feature Maps at Different Layers\n",
    "\n",
    "Now let's see how the input image is transformed as it passes through different layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_outputs(model, input_tensor):\n",
    "    \"\"\"Get outputs from all layers in the feature extractor\"\"\"\n",
    "    outputs = []\n",
    "    layer_names = []\n",
    "    \n",
    "    x = input_tensor\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        outputs.append(x.clone())\n",
    "        layer_names.append(f\"Layer {i}: {layer.__class__.__name__}\")\n",
    "    \n",
    "    return outputs, layer_names\n",
    "\n",
    "def visualize_layer_progression(model, input_tensor):\n",
    "    \"\"\"Visualize how image changes through network layers\"\"\"\n",
    "    \n",
    "    outputs, layer_names = get_layer_outputs(model, input_tensor)\n",
    "    \n",
    "    # Select subset of layers to visualize (skip some for clarity)\n",
    "    indices_to_show = [0, 2, 5, 7, 10, 12, 15, 17, 20, 22, 25, 27, 29, 30]  # Key layers\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "    fig.suptitle('Feature Maps Through Network Layers', fontsize=16)\n",
    "    \n",
    "    for idx, layer_idx in enumerate(indices_to_show):\n",
    "        if idx >= 16 or layer_idx >= len(outputs):\n",
    "            break\n",
    "            \n",
    "        row, col = idx // 4, idx % 4\n",
    "        \n",
    "        # Get output and convert to grayscale by averaging across channels\n",
    "        output = outputs[layer_idx][0].cpu().detach()  # Remove batch dimension\n",
    "        \n",
    "        if len(output.shape) == 3:  # Has channels\n",
    "            # Average across channels\n",
    "            grayscale = output.mean(dim=0)\n",
    "        else:\n",
    "            grayscale = output\n",
    "        \n",
    "        axes[row, col].imshow(grayscale, cmap='viridis')\n",
    "        axes[row, col].set_title(layer_names[layer_idx], fontsize=10)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(indices_to_show), 16):\n",
    "        row, col = idx // 4, idx % 4\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'img_tensor' in locals():\n",
    "    visualize_layer_progression(model, img_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Individual Filter Responses\n",
    "\n",
    "Let's examine how different filters in a specific layer respond to our input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filter_responses(model, input_tensor, layer_idx, max_filters=64):\n",
    "    \"\"\"Visualize individual filter responses at a specific layer\"\"\"\n",
    "    \n",
    "    # Get output at specified layer\n",
    "    x = input_tensor\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if i == layer_idx:\n",
    "            target_output = x\n",
    "            break\n",
    "    \n",
    "    # Get filter responses\n",
    "    filters = target_output[0].cpu().detach()  # Remove batch dimension\n",
    "    num_filters = min(max_filters, filters.shape[0])\n",
    "    \n",
    "    grid_size = int(np.ceil(np.sqrt(num_filters)))\n",
    "    \n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n",
    "    fig.suptitle(f'Layer {layer_idx} Filter Responses ({filters.shape[0]} total filters)', fontsize=16)\n",
    "    \n",
    "    for i in range(grid_size * grid_size):\n",
    "        row, col = i // grid_size, i % grid_size\n",
    "        \n",
    "        if i < num_filters:\n",
    "            filter_response = filters[i]\n",
    "            axes[row, col].imshow(filter_response, cmap='viridis')\n",
    "            axes[row, col].set_title(f'Filter {i}', fontsize=8)\n",
    "        \n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Layer {layer_idx} output shape: {target_output.shape}\")\n",
    "    print(f\"Showing first {num_filters} out of {filters.shape[0]} filters\")\n",
    "\n",
    "# Visualize responses from different layers\n",
    "if 'img_tensor' in locals():\n",
    "    # First conv layer (low-level features)\n",
    "    print(\"=== First Convolutional Layer (Low-level features) ===\")\n",
    "    visualize_filter_responses(model, img_tensor, layer_idx=0, max_filters=64)\n",
    "    \n",
    "    # Middle conv layer (mid-level features)\n",
    "    print(\"\\n=== Middle Convolutional Layer (Mid-level features) ===\")\n",
    "    visualize_filter_responses(model, img_tensor, layer_idx=10, max_filters=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Inversion: Mahendran & Vedaldi Approach\n",
    "\n",
    "This technique reconstructs an image that produces similar activations to our input image at a specific layer. This helps us understand what information is preserved at different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output(model, input_tensor, target_layer):\n",
    "    \"\"\"Get output from a specific layer\"\"\"\n",
    "    x = input_tensor\n",
    "    for i, layer in enumerate(model.features):\n",
    "        x = layer(x)\n",
    "        if i == target_layer:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "def feature_inversion(model, input_tensor, target_layer, num_iterations=200, lr=1e3):\n",
    "    \"\"\"Perform feature inversion using gradient descent\"\"\"\n",
    "    \n",
    "    # Get target representation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        target_features = get_layer_output(model, input_tensor, target_layer)\n",
    "    \n",
    "    # Initialize random noise image\n",
    "    noise_image = torch.randn_like(input_tensor, requires_grad=True, device=device)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = SGD([noise_image], lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Regularization parameters\n",
    "    alpha_reg_lambda = 1e-7\n",
    "    alpha_reg_alpha = 6\n",
    "    tv_reg_lambda = 1e-8\n",
    "    tv_reg_beta = 2\n",
    "    \n",
    "    reconstructed_images = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"Starting feature inversion for layer {target_layer}...\")\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get current features\n",
    "        current_features = get_layer_output(model, noise_image, target_layer)\n",
    "        \n",
    "        # Feature reconstruction loss\n",
    "        feature_loss = cnn_utils.euclidian_loss(target_features, current_features)\n",
    "        \n",
    "        # Regularization terms\n",
    "        alpha_reg = alpha_reg_lambda * cnn_utils.alpha_norm(noise_image, alpha_reg_alpha)\n",
    "        tv_reg = tv_reg_lambda * cnn_utils.total_variation_norm(noise_image, tv_reg_beta)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = feature_loss + alpha_reg + tv_reg\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(total_loss.item())\n",
    "        \n",
    "        # Save intermediate results\n",
    "        if i % (num_iterations // 10) == 0:\n",
    "            with torch.no_grad():\n",
    "                img_array = cnn_utils.recreate_image(noise_image)\n",
    "                reconstructed_images.append(img_array)\n",
    "            print(f'Iteration {i}: Loss = {total_loss.item():.6f}')\n",
    "    \n",
    "    return reconstructed_images, losses\n",
    "\n",
    "def visualize_inversion_process(reconstructed_images, target_layer):\n",
    "    \"\"\"Visualize the feature inversion process\"\"\"\n",
    "    \n",
    "    num_images = len(reconstructed_images)\n",
    "    cols = min(5, num_images)\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3*rows))\n",
    "    fig.suptitle(f'Feature Inversion Process - Layer {target_layer}', fontsize=16)\n",
    "    \n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, img in enumerate(reconstructed_images):\n",
    "        row, col = i // cols, i % cols\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(f'Iteration {i * 20}', fontsize=12)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_images, rows * cols):\n",
    "        row, col = i // cols, i % cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Perform feature inversion for different layers\n",
    "if 'img_tensor' in locals():\n",
    "    # Low-level features (early layer)\n",
    "    print(\"=== Feature Inversion: Early Layer (Low-level features) ===\")\n",
    "    early_reconstructions, early_losses = feature_inversion(model, img_tensor, target_layer=5, num_iterations=100)\n",
    "    visualize_inversion_process(early_reconstructions, target_layer=5)\n",
    "    \n",
    "    # High-level features (later layer)\n",
    "    print(\"\\n=== Feature Inversion: Later Layer (High-level features) ===\")\n",
    "    late_reconstructions, late_losses = feature_inversion(model, img_tensor, target_layer=18, num_iterations=100)\n",
    "    visualize_inversion_process(late_reconstructions, target_layer=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Different Layers\n",
    "\n",
    "Let's compare the final reconstructions from different layers to understand the information preserved at each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_layer_reconstructions(model, input_tensor, layers_to_compare, num_iterations=150):\n",
    "    \"\"\"Compare reconstructions from multiple layers\"\"\"\n",
    "    \n",
    "    reconstructions = {}\n",
    "    \n",
    "    for layer in layers_to_compare:\n",
    "        print(f\"Processing layer {layer}...\")\n",
    "        images, losses = feature_inversion(model, input_tensor, layer, num_iterations)\n",
    "        reconstructions[layer] = images[-1]  # Take final reconstruction\n",
    "    \n",
    "    # Visualize comparisons\n",
    "    fig, axes = plt.subplots(2, len(layers_to_compare) + 1, figsize=(4*(len(layers_to_compare)+1), 8))\n",
    "    \n",
    "    # Original image\n",
    "    original = cnn_utils.recreate_image(input_tensor)\n",
    "    axes[0, 0].imshow(original)\n",
    "    axes[0, 0].set_title('Original Image', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Reconstructions\n",
    "    for i, layer in enumerate(layers_to_compare):\n",
    "        axes[0, i+1].imshow(reconstructions[layer])\n",
    "        axes[0, i+1].set_title(f'Layer {layer}\\nReconstruction', fontsize=12)\n",
    "        axes[0, i+1].axis('off')\n",
    "        \n",
    "        # Show layer type\n",
    "        layer_info = str(list(model.features)[layer])\n",
    "        axes[1, i+1].text(0.5, 0.5, layer_info, ha='center', va='center', \n",
    "                         fontsize=8, transform=axes[1, i+1].transAxes, wrap=True)\n",
    "        axes[1, i+1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Feature Inversion Comparison Across Layers', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'img_tensor' in locals():\n",
    "    # Compare different layers\n",
    "    layers_to_compare = [2, 7, 14, 21, 28]  # Different depth levels\n",
    "    compare_layer_reconstructions(model, img_tensor, layers_to_compare, num_iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gradient-based Saliency Maps\n",
    "\n",
    "Let's create saliency maps to see which parts of the input image are most important for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map(model, input_tensor, target_class=None):\n",
    "    \"\"\"\n",
    "    Generate saliency map using gradients\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_tensor.requires_grad_()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    if target_class is None:\n",
    "        # Use the predicted class\n",
    "        target_class = output.argmax(dim=1)\n",
    "    \n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "    \n",
    "    # Get gradients\n",
    "    gradients = input_tensor.grad.data.abs()\n",
    "    \n",
    "    # Take maximum across color channels\n",
    "    saliency_map = gradients.max(dim=1)[0]\n",
    "    \n",
    "    return saliency_map[0].cpu().numpy()\n",
    "\n",
    "def visualize_saliency(model, input_tensor, original_image):\n",
    "    \"\"\"Visualize saliency map\"\"\"\n",
    "    \n",
    "    saliency = generate_saliency_map(model, input_tensor)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(original_image)\n",
    "    axes[0].set_title('Original Image', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Saliency map\n",
    "    im1 = axes[1].imshow(saliency, cmap='hot')\n",
    "    axes[1].set_title('Saliency Map', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Overlay\n",
    "    axes[2].imshow(original_image)\n",
    "    axes[2].imshow(saliency, cmap='hot', alpha=0.4)\n",
    "    axes[2].set_title('Overlay', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'img_tensor' in locals() and 'img_raw' in locals():\n",
    "    print(\"=== Saliency Map Analysis ===\")\n",
    "    visualize_saliency(model, img_tensor.clone(), img_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#Challenge Section\n",
    "\n",
    "Now it's your turn! Complete these challenges to deepen your understanding of CNN visualization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Multi-Image Analysis\n",
    "\n",
    "Compare how different types of images (animals, objects, scenes) activate different filters in the same layer.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load 3-4 different images from different categories\n",
    "2. Visualize filter responses for the same layer across all images\n",
    "3. Identify which filters are consistently active vs. image-specific\n",
    "4. Analyze the patterns you observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement multi-image analysis\n",
    "def analyze_multiple_images(model, image_paths, layer_idx):\n",
    "    \"\"\"\n",
    "    Analyze filter responses across multiple images\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# Test with different image categories\n",
    "# image_paths = ['cat.jpg', 'car.jpg', 'building.jpg', 'flower.jpg']\n",
    "# analyze_multiple_images(model, image_paths, layer_idx=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: Filter Specialization Analysis\n",
    "\n",
    "Investigate what specific features individual filters have learned to detect.\n",
    "\n",
    "**Tasks:**\n",
    "1. For a specific layer, identify the top 10 most active filters for your input image\n",
    "2. Create synthetic images that maximally activate each of these filters\n",
    "3. Analyze what patterns these filters detect\n",
    "4. Compare early vs. late layer filter specializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement filter specialization analysis\n",
    "def analyze_filter_specialization(model, input_tensor, layer_idx, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze what specific filters have learned\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    # Hint: Use feature inversion to create images that maximally activate specific filters\n",
    "    pass\n",
    "\n",
    "def maximize_filter_activation(model, layer_idx, filter_idx, num_iterations=200):\n",
    "    \"\"\"\n",
    "    Create an image that maximally activates a specific filter\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: Layer-wise Information Analysis\n",
    "\n",
    "Quantify how much information about the original image is preserved at different layers.\n",
    "\n",
    "**Tasks:**\n",
    "1. Perform feature inversion for layers 1, 5, 10, 15, 20, 25, 30\n",
    "2. Calculate similarity metrics between original and reconstructed images\n",
    "3. Plot information preservation vs. layer depth\n",
    "4. Identify the \"information bottleneck\" in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement information preservation analysis\n",
    "def calculate_image_similarity(img1, img2):\n",
    "    \"\"\"\n",
    "    Calculate similarity between two images using multiple metrics\n",
    "    \"\"\"\n",
    "    # Implement SSIM, PSNR, or other similarity metrics\n",
    "    pass\n",
    "\n",
    "def analyze_information_preservation(model, input_tensor, layers_to_test):\n",
    "    \"\"\"\n",
    "    Analyze how much information is preserved at different layers\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "# layers_to_test = [1, 5, 10, 15, 20, 25, 30]\n",
    "# analyze_information_preservation(model, img_tensor, layers_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Adversarial Visualization\n",
    "\n",
    "Create visualizations that highlight the network's vulnerabilities.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate adversarial examples that fool the network\n",
    "2. Visualize the difference between original and adversarial images\n",
    "3. Show how saliency maps change for adversarial examples\n",
    "4. Analyze which layers are most affected by adversarial perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement adversarial visualization\n",
    "def generate_adversarial_example(model, input_tensor, target_class, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Generate adversarial example using FGSM attack\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def visualize_adversarial_effects(model, original_tensor, adversarial_tensor):\n",
    "    \"\"\"\n",
    "    Visualize how adversarial examples affect network activations\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Custom Architecture Visualization\n",
    "\n",
    "Apply these visualization techniques to a different architecture.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load a different pre-trained model (ResNet, DenseNet, etc.)\n",
    "2. Adapt the visualization functions for the new architecture\n",
    "3. Compare the learned representations between VGG16 and your chosen model\n",
    "4. Analyze architectural differences in terms of feature learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement custom architecture visualization\n",
    "def load_alternative_model(model_name='resnet50'):\n",
    "    \"\"\"\n",
    "    Load and prepare an alternative architecture for visualization\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def compare_architectures(model1, model2, input_tensor):\n",
    "    \"\"\"\n",
    "    Compare learned representations between different architectures\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: Interactive Visualization Dashboard\n",
    "\n",
    "Create an interactive tool for exploring CNN visualizations.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create functions that allow users to:\n",
    "   - Select different layers to visualize\n",
    "   - Choose specific filters to analyze\n",
    "   - Upload custom images for analysis\n",
    "2. Implement real-time feature inversion\n",
    "3. Add comparison tools for multiple images/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement interactive visualization dashboard\n",
    "def create_interactive_dashboard():\n",
    "    \"\"\"\n",
    "    Create an interactive dashboard for CNN visualization\n",
    "    \"\"\"\n",
    "    # You can use ipywidgets for interactive elements\n",
    "    # from ipywidgets import interact, IntSlider, Dropdown\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 7: Analysis Questions\n",
    "\n",
    "Answer these questions based on your experiments:\n",
    "\n",
    "1. **Hierarchical Learning**: How do the features learned by early layers differ from those learned by later layers?\n",
    "\n",
    "2. **Information Flow**: At which layer does the network lose the most spatial information? Why might this be beneficial?\n",
    "\n",
    "3. **Filter Redundancy**: Do you observe redundant filters? How might this affect network efficiency?\n",
    "\n",
    "4. **Feature Inversion Quality**: Why do reconstructions from deeper layers look more abstract?\n",
    "\n",
    "5. **Practical Applications**: How could these visualization techniques be used in real-world applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers:\n",
    "\n",
    "**1. Hierarchical Learning:**\n",
    "<!-- Your answer here -->\n",
    "\n",
    "**2. Information Flow:**\n",
    "<!-- Your answer here -->\n",
    "\n",
    "**3. Filter Redundancy:**\n",
    "<!-- Your answer here -->\n",
    "\n",
    "**4. Feature Inversion Quality:**\n",
    "<!-- Your answer here -->\n",
    "\n",
    "**5. Practical Applications:**\n",
    "<!-- Your answer here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Research Paper Implementation\n",
    "\n",
    "Implement a more advanced visualization technique from recent research:\n",
    "\n",
    "**Options:**\n",
    "1. **Grad-CAM**: Implement Gradient-weighted Class Activation Mapping\n",
    "2. **Integrated Gradients**: Implement the attribution method\n",
    "3. **LIME**: Implement Local Interpretable Model-agnostic Explanations\n",
    "4. **Deep Dream**: Implement the Google Deep Dream algorithm\n",
    "\n",
    "Choose one and implement it from scratch, then compare it with the basic methods we've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement advanced visualization technique\n",
    "def implement_gradcam(model, input_tensor, target_layer, target_class):\n",
    "    \"\"\"\n",
    "    Implement Grad-CAM visualization\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def implement_integrated_gradients(model, input_tensor, target_class, steps=50):\n",
    "    \"\"\"\n",
    "    Implement Integrated Gradients\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}